标题：基于深度学习的条件式视觉内容生成研究及应用

摘要
随着深度学习的快速发展和计算能力的不断提高，条件式视觉内容生成技术蓬勃发展并取得了许多瞩目的科研成果，在火热的泛娱乐、元宇宙和虚拟人等领域具有极大的应用价值。然而，受限于当前技术与高标准应用需求之间的差距，生成模型在效率和易用性等方面有待进一步提高，且如何通过给定的条件输入（如图像，音频，运动信息等）生成高质量且合理的视觉内容仍是当前面对的棘手问题。
本文聚焦基于深度学习的条件式视觉内容生成技术，按照对信息理解程度逐渐升高的原则，由浅及深地对不同层次的问题进行分类总结并展开具体研究。一方面，低层次的图像像素级任务核心关注局部结构感知及分布预测问题，面临的最大挑战是如何生成合理且高质量的图像，以及设计高效的模型来支持各种终端应用。另一方面，高层次的图像语义级生成任务不仅需要理解图像中的语义信息，同时对高分辨率、高精度生成模型研究及训练优化提出了极大挑战。尤其过渡到视频生成任务中这些难点愈发凸显，且需要额外地对时序合理性及生成多样性进行建模。针对上述挑战，本文研究典型的图像上色和超分辨率任务，以期从低层次的感知中生成更高质量的视觉内容；以及语义层面的人脸换脸、人脸驱动和图像动态化任务，以期从高层次的理解中可控地生成更丰富的图像和视频。本文的研究内容与主要创新点如下：
1. 从局部结构感知的图像增强出发，本文针对图像上色和超分辨率问题展开研究，提出了一种高效的联合图像上色和超分辨率端到端框架，设计了金字塔阀控交叉注意力模块以支持自动和参考两种上色模式，不仅能更好地理解并聚合参考图像的颜色信息，同时具有较强的解释性。此外，本文针对任意倍率图像放大应用需求提出了连续像素映射模块，使用更少计算量的同时提升了模型的预测精度。
2. 从多条件受限的图像纹理语义迁移出发，本文针对人脸换脸问题展开研究，提出了一种基于区域注意力感知的换脸方法以对人脸进行更精细的建模，其包含新颖的面部区域感知的局部分支和源特征适应的全局分支：前者通过引入全局注意力机制来有效地建模不重合的多尺度面部语义交互，而后者补充全局身份相关的线索来进一步保证生成图像的身份一致性。此外，本研究提出了一种无监督人脸软掩膜预测模块，进一步提升了模型的准确性与实用性。
3. 从多条件受限的图像几何语义编辑出发，本文针对人脸驱动问题展开研究，提出了III摘要浙江大学博士学位论文一种基于人脸几何和纹理信息解耦思想设计的多人脸驱动模型，其包含一个精心设计的人脸关键点转换器分支以在几何空间上进行不同身份的面部运动迁移，以及一个几何感知生成器分支生成人脸驱动图像，在保证图像生成质量的基础上实现了多人脸驱动任务目标。同时本文将该框架扩展到了音频多人脸驱动任务，设计了音频特征融合器和几何控制器模块分别进行音频特征提取及高效注入，并提出了一个高质量的AnnVI 数据集以支持高分辨率的音频人脸驱动研究。
4. 从运动约束下的图像序列生成出发，本文针对图像动态化问题展开研究，基于运动和纹理解耦的思想设计了端到端的动态视频生成框架，其包含光流编码器模块和双分支动态视频生成器：前者将表示视频运动的光流信息编码为归一化向量，同时可通过随机运动向量采样的推理方式实现多样化的视频生成；后者在运动向量的控制下基于单帧输入图像生成合理的目标动态视频。此外，针对当前延时视频数据集质量较差的问题，本研究提出了大规模的高分辨率QST 数据集来支持该任务的持续研究。
针对以上研究内容和成果，本文在多个主流数据集上进行了大量的实验评估，证明了所提方法的有效性和优越性，在基于深度学习的条件式视觉内容生成领域取得了出色的研究成果，同时提出的部分算法模型已用于商业产品中，具有较大的应用价值。
关键词：计算机视觉；深度学习；条件式视觉内容生成；图像上色；图像超分辨率；人脸换脸；人脸驱动；图像动态化IV

Abstract
With the rapid development of deep learning and the continuous improvement of comput-ing power, the technology of conditional visual content generation is booming and has achieved alot of remarkable achievements, which has significant application values in the hot fields of pan-entertainment, meta-universe, and virtual human. However, limited by the gap between the currenttechnology and the requirements of high-standard applications, the eﬀiciency and ease-of-use ofthe generation model need to be further improved. Also, how to generate high-quality and reason-able visual content through the given conditional inputs (i.e., image, audio, motion information,etc.) is still a challenging problem to be solved at present.
This thesis focuses on the research of deep learning-based conditional visual content gen-eration, and specific problems at different levels are summarized and studied from shallow todeep, according to the principle of increasing understanding of information. On the one hand, lowpixel-level tasks focus on local structure perception and distribution prediction, and the biggestchallenge is how to generate reasonable and high-quality images while designing eﬀicient modelsto support various terminal applications. On the other hand, high semantic-level tasks need to un-derstand the semantic information in images, which also put forward significant challenges aboutthe high-resolution image generation, high-precision model design, and the optimization proce-dure. Especially in the transition to video generation tasks, these diﬀiculties become increasinglyprominent, and additional modelings of temporal rationality and diversity generation need to beconsidered. Given the above challenges, this thesis studies typical image colorization and super-resolution tasks that generate higher-quality visual content from low-level perception; as well assemantic face swapping, face animation, and image dynamic tasks to generate richer images andvideos from high-level understanding controllably. The research contents and main contributionsof this thesis are as follows:
1. Starting from the local structure perception of image translation, this thesis studies the im-age colorization and super-resolution problems, and we propose an end-to-end framework to solvesimultaneously image colorization and super-resolution eﬀiciently. Specifically, a novel pyramidvalve cross attention module is designed to support both automatic and referential colorization,VABSTRACT浙江大学博士学位论文which can not only understand and aggregate the color information of reference images better butalso has a strong interpretation. Also, a continuous pixel mapping module is proposed to meet theapplication requirements of arbitrary image magnification, improving the model accuracy withless computation.
2. Starting from multi-condition constrained texture transfer, this thesis studies the image-level face swapping problem and proposes a novel region-aware face swapping method for moredelicate modeling, which includes a facial region-aware branch and a source feature-adaptivebranch. The former effectively models non-overlapping multi-scale facial semantic interactionsby introducing a global attention mechanism, while the latter complements global identity-relatedcues to ensure identity consistency for the generated image further. In addition, this study proposesan unsupervised facial mask prediction module further to improve the accuracy and practicabilityof the model.
3. Starting from multi-condition constrained geometry editing, this thesis studies the image-level face animation problem and proposes a multi-identity face animation model, which followsthe decoupling idea of face geometry and texture information. This model consists of a well-designed face landmark converter branch for facial movement migration of different identities ingeometric space, as well as a geometry-aware generator branch to generate animated face images,realizing the multi-identity face animation task on the basis of ensuring the generation quality.
Simultaneously, this framework is extended to the audio-guided multi-identity face animation task,and we design an audio feature fuser module as well as a geometry controller module for eﬀicientaudio feature extraction and injection, respectively. Also, a high-quality AnnVI dataset is proposedto support high-resolution audio-guided multi-identity face animation research.
4. Starting from the motion constrained image sequence generation, this thesis studies theimage dynamic problem and designs an end-to-end dynamic video generation framework based onthe idea of decoupling motion and texture information. In detail, our approach consists of an opticalflow encoder and a dynamic video generator. The former encodes the optical flow informationrepresenting video motion into a normalized vector, and it provides an easy inference mannerto generate various videos by randomly sampling motion vector; The latter generates reasonabletarget dynamic video based on a single input image under the control of the motion vector. Inaddition, given the poor quality of the current time-lapse video dataset, this study proposes alarge-scale high-resolution QST dataset to support the ongoing research on this task.
VI浙江大学博士学位论文ABSTRACTOn the above research contents and achievements, this thesis conducts massive experimentalevaluations on several mainstream datasets, and the results prove the effectiveness and superiorityof the proposed methods. In this thesis, excellent research results have been achieved in conditionalvisual content generation based on deep learning, and some studied models have been used incommercial products with great application values.
Keywords: Computer Vision; Deep Learning; Conditional Visual Content Generation; ImageColorization; Image Super-Resolution; Face Swapping; Face Animation; Image DynamicVII

目录：
第一章 绪论
1.1 研究背景和意义
1.2 研究问题与挑战
1.3 研究内容与主要贡献
1.4 本文组织结构
第二章 相关文献综述
2.1 条件式深度图像生成模型研究现状
2.1.1 生成对抗网络理论
2.1.2 任务数据集介绍
2.1.3 非条件式生成模型代表性工作
2.1.4 典型条件式视觉内容生成架构
2.2 像素级图像生成研究现状
2.2.1 图像上色研究现状
2.2.2 图像超分辨率研究现状
2.3 语义级图像视频生成研究现状
2.3.1 人脸换脸研究现状
2.3.2 人脸驱动研究现状
2.3.3 视频生成研究现状
2.4 本章小结
第三章 局部结构感知的图像增强
3.1 引言
第三章 方法概述
3.2 金字塔阀控交叉注意力模块
3.3.1 阀控交叉注意力模块
3.3.2 多尺度结构扩展
3.4 连续像素映射模块
3.5 总体目标函数
3.6 实验结果
3.6.1 数据集介绍
3.6.2 实验设置
3.6.3 评测指标
3.6.4 与主流方法对比
3.6.5 人工评估实验
3.6.6 消融实验
3.6.7 解释性实验
3.7 本章小结
第四章 多条件受限的图像纹理语义迁移
4.1 引言
4.2 方法概述
4.3 面部区域感知分支
4.3.1 区域感知的身份编码器
4.3.2 基于注意力机制的特征交互模块
4.3.3 局部感知的身份投影器
4.4 源特征适应分支
4.5 人脸掩膜预测模块
4.6 总体目标函数
4.7 实验结果
4.7.1 数据集介绍
4.7.2 实验设置
4.7.3 评测指标
4.7.4 与主流方法对比
4.7.5 人工评估实验
第5章 多条件受限的图像几何语义编辑
5.1 引言
5.2 方法概述
5.3 基于人脸图像控制的多人脸驱动
5.3.1 人脸关键点转换器
5.3.2 几何感知生成器
5.3.3 三元组感知损失
5.3.4 总体目标函数
5.4 基于音频信号控制的多人脸驱动
5.4.1 音频特征融合器
5.4.2 几何感知生成器
5.4.3 总体目标函数
5.5 AnnVI 数据集
5.6 实验结果
5.6.1 数据集介绍
5.6.2 实验设置
5.6.3 评测指标
5.6.4 与主流方法对比
5.6.5 消融实验
5.6.6 解释性实验
5.7 本章小结
第6章 运动约束下的图像序列生成
6.1 引言
6.2 方法概述
6.3 光流编码器
6.4 动态视频生成器
6.5 总体目标函数
第6章 实验结果
6.6 QST 数据集
6.6.1 数据集介绍
6.6.2 图像生成测试
6.7 实验结果
6.7.1 数据集介绍
6.7.2 实验设置
6.7.3 评测指标
6.7.4 与主流方法对比
6.7.5 人工评估实验
6.7.6 消融实验
6.7.7 多样化生成测试
6.8 本章小结
第7章 总结与展望
7.1 本文工作总结
7.2 未来工作展望
参考文献
申请人简历
攻读博士学位期间的主要学术成果

1绪论1.1研究背景和意义自从AlexNet 获得2012 年ImageNet LSVRC 比赛冠军之后，基于深度学习的计算机视觉技术飞速发展，同时伴随着GPU 硬件算力的提高和5G 通信的进步，计算机视觉技术已在多个领域上取得了瞩目成绩，甚至在一些如图像分类的应用场景中已超越人类表现。近年来，随着人民生活水平的提高和对美好生活需求的日益增长，泛娱乐和元宇宙等行业开始进入广众视野并逐渐融入人们的日常生活，从图像修复与增强到图像编辑生成，从虚拟人技术到短视频中的特效生成，其中的人工智能支撑技术俨然已从前期的分类、检测、分割等判别类任务过渡到了挑战性更高的生成类任务。
第48 次《中国互联网络发展状况统计报告》 指出，截止2021 年6 月，我国互联网普及率71.6%，手机网民规模达10.07 亿，巨大的应用市场进一步催生了研究机构和企业对图像生成技术的重视。腾讯研究院与腾讯优图实验室在《AI 生成内容发展报告2020》 中指出深度生成在技术成熟度和社会影响力两方面都在迅速发展，逐渐在影视制作、娱乐交互、医疗教育、政治安全、艺术创作等领域产生不菲价值。
《中国AI 数字商业产业展望2021-2025》 一文预测2025年中国生成式AI 技术栈应用规模突破2,000 亿，《量子位·十大前沿科技报告》、《虚拟数字人深度产业报告》 和民生证券发布的《2030 年的元宇宙产业》 则指出以人脸生成为中心的深度生成技术在综合性虚拟人和元宇宙生成引擎中扮演着中坚力量。
随着时间推移，图像生成技术已经并可预见地将在更多行业中发挥重要作用，而应用需求的增长也对图像生成质量和可控性提出了更高要求。然而，受限于图像生成技术发展与高标准应用需求之间的差距，如何在给定的条件输入下（如图像、音频和运动信息等）生成高质量且合理的视觉内容仍是当前要面对的棘手问题。在神经科学和仿生学领域，研究者发现灵长类视觉系统包含着相互交织的分层视觉通路[10–12]，从视网膜将光信号转化为电信号开始，视觉皮层中低层结构先提取如边缘和斑点的早期局部视觉特征，而后过渡到中级的局部纹理结构与颜色描述，最后经下颞叶皮质层处理后获得高级语义级别理解。
类似地，在条件式视觉内容生成研究中同样存在着由浅及深的处理次序，本文对主流的条件式视觉内容生成任务进行总结分析，按照(1) 单/多条件输入；(2) 像素/语义级理解；(3) 是否时序建模三个标准归纳出不同应用任务之间的所属类别差异与渐进层次。
如图1.1 所示，11绪论浙江大学博士学位论文图1.1条件式视觉内容生成相关任务分类及本文研究路线x最基本的图像自动上色[13–18]、图像超分辨率[19–24]、图像去噪[25–27]、图像去雾[28–30] 和图像去雨[31–33] 等条件式生成任务归类为基于单条件输入的局部结构感知的图像增强，这些任务一般只需要在像素级层面进行分布预测学习且输入的条件仅包含一张输入图像。y图像参考上色[14,34–38]、图像风格化[39–41] 和图像和谐化[42–44] 等任务可归类为基于多条件输入的局部结构感知的图像增强，其需要两个及以上的条件作为输入且一般不涉及语义信息的理解，核心关注局部结构感知及分部预测问题。z信息理解程度更深的图像补全[45–47]、图像语义转换[48–50] 和正面人脸预测[51–53] 等任务归类为单条件受限的图像编辑，{而人脸换脸[54–59]、人脸驱动[60–67] 和妆容迁移[68–70] 等任务则归类为多条件受限的图像语义编辑，这些任务致力于解决语义目标精细理解及高清生成问题，因需要考虑语义层面的理解与生成而具有很大的挑战性。受限于应用需求和算力支撑，目前大多数条件式生成工作集中于图像建模而很少在视频领域上进行研究，而需求较少的视频生成应用一般也可由图像生成方法在时序上逐帧扩展而来，比如视频上色、视频人脸换脸和视频帧补全等应用均可由相应的图像应用加上时序运动约束实现。然而，这种直观的逐帧生成方式会面临时序不稳定和串行速度慢的问题，在保证空间图像生成质量的同时也需要研究生成视频的时序合理性及多样性问题，这在实际应用中仍存在许多挑战。
众所周知，条件式视觉内容生成本身是创造的过程，受限于技术发展的不完备，实用中生成的图像仍存在较多伪造缺陷，对于精度需求较高的场景还需要人工后期处理，无疑是对时间和金钱的浪费，故如何提高生成图像的真实性与模型的易用性仍是非常有持续研2浙江大学博士学位论文1绪论究价值的课题。一方面，对于低层次生成任务，其目标是对局部特征进行建模或预测局部像素分布，如何设计高效且有效的模型架构进行局部感知与建模则显得尤为重要。另一方面，高层次生成任务需提取并理解图像中的语义信息，这对高精度生成模型研究及训练优化提出了极大挑战，尤其是过渡到发展不充分的视频生成任务这些难点则愈发凸显。总而言之，基于深度学习的条件式视觉内容生成技术仍处于研究发展阶段，不能很好地满足近些年相关行业应用的快速发展对相关技术的需求。为了缓解上述问题，本文从泛娱乐和元宇宙等行业中以人为中心的图像生成和内容创作应用需求出发，对基于深度学习的条件式视觉内容生成展开深入研究，从信息理解程度角度对多个维度的典型研究问题提出创新解决方案。针对低层次的局部结构感知的图像增强，本文对典型的图像上色和超分任务进行研究，提出了高效的联合图像上色超分框架，支持在连续空间上任意倍率的图像放大，具有极高的实用价值。针对高层次的多条件受限的图像语义编辑，本文以应用广泛的人脸换脸和人脸驱动为研究对象。对于前者，本文提出了面部纹理和几何解耦的多人脸驱动模型，支持图像和音频多种信息源控制，并设计了三元感知损失函数帮助训练，实现了几何可控的生成过程；对于后者，本文首次引入注意力机制对人脸五官进行精细建模，并提出了无监督人脸掩膜预测方法，能够有效降低生成图像的视觉缺陷。最终，本文进一步扩展了运动约束下的图像序列生成研究，针对新兴的图像动态化任务提出了解决方案与思考。
研究问题深度学习多条件语义理解时序建模应用任务名称局部结构感知的图像增强自动上色、超分辨率、图像去噪、图像去雾、图像去雨...
参考上色、图像风格化、图像和谐化...
单条件受限的图像编辑图像补全、图像语义转换、正面人脸预测...
多条件受限的图像语义编辑人脸换脸、人脸驱动、妆容迁移、人脸老化...
运动约束下的图像序列生成图像动态化、视频人脸换脸、视频翻译、视频编辑...
1.2研究问题与挑战本文聚焦于基于深度学习的条件式视觉内容生成研究及应用，从泛娱乐和元宇宙等行业对内容生成技术的应用需求出发，根据条件信息数量和对图像信息的理解程度将现有的研究任务分为局部结构感知的图像增强、单条件受限的图像编辑、多条件受限的图像语义编辑和运动约束下的图像序列生成四类问题，如表1.1 所示，其中黑色加粗的三项为本文研究问题，并按照图1.1 红色标注的研究路线对研究问题下的典型任务开展深入研31绪论浙江大学博士学位论文图1.2本文研究对象层次划分究。具体地，本文依次针对局部结构感知的图像增强中的图像上色和超分辨率典型任务（图像像素级）、多条件受限的图像语义编辑中的人脸换脸和人脸驱动典型任务（图像语义级），以及运动约束下的图像序列生成中的图像动态化典型任务（视频语义级）开展研究，如图1.2 所示。
(1) 局部结构感知的图像增强关注图像像素级的局部结构感知及分布预测问题，本研究针对典型的图像上色和超分辨率任务进行研究。低层次的条件式旧照修复应用中，当前工作会将图像上色和超分辨率技术串接使用以获得视觉友好的增强图像，但这种离散的操作方式是程序繁琐且低效的，存在底层共享特征的重复提取问题，同时串接模型对图像建模存在分布差异，故设计高效的联合图像上色和超分辨率的端到端框架显得尤为重要。
对于该新问题，核心难点在于如何设计高效的上色和超分子模块并有效地对两者进行融合以实现统一模型表征。对于图像上色模块，设计时不仅要考虑模型效果，也要支持自动和参考两种上色模式以满足实际应用需求。与此同时，通常的图像超分方法仅能以固定倍率放大图像，应用时会执行缩小操作以匹配目标设备分辨率而造成额外的算力损耗，故连续倍率的超分算法研究对于应用尤为重要，但如何在保证模型轻量化的前提下实现细节清晰可鉴的图像超分仍是一个非常具有挑战性的问题。
(2) 多条件受限的图像语义编辑关注图像语义级的语义精细理解及高清生成问题，本研究从图像语义编辑出发对典型的人脸换脸任务进行研究，进一步针对图像几何编辑探究了典型的人脸驱动任务。对于人脸换脸任务，其旨在将源图像中人脸身份迁移到目标图像中的人脸上，同时保持目标图像中与身份无关的属性不变。主流工作均对基于编码器的逆映射方案进行改进，将换脸视为一种特殊的风格迁移任务，并基于StyleGAN 生成高分辨4浙江大学博士学位论文1绪论率换脸图像。然而，这些方法仅在全局特征表示层面进行源人脸和目标人脸的信息交互，而忽略了局部区域的身份相关性建模，这限制了模型身份一致性的表达能力。此外，当前基于条件向量输入的渐进式生成模型依赖于参数共享机制，这会导致换脸图像中不该发生变化的区域不可避免地存在像素差异，一些工作使用预提取的人脸掩膜对换脸图像和目标人脸混合来缓解这一问题，或采用有监督方式训练额外的人脸掩膜预测网络，但这些方案会导致最终图像边缘产生伪影且需要额外较大的计算量。因此，在不依赖外部数据或网络的基础上，如何基于现有框架设计新的无监督软掩码学习范式是一项艰巨但有重要价值的研究课题。对于人脸驱动任务，其旨在利用图像/音频等驱动信号对给定的源人脸图像进行面部表情或头部姿态控制。早期工作引入参数化3D 模型表征人脸，这类方法具有高质量和高分辨率的人脸驱动生成能力，但在3D 模型成本和运行速度上具有较高的代价，因而限制了其广泛应用的可能性。受益于大规模数据集的发布，基于深度学习的端到端生成模型发展迅速，能够在保证效果的基础上具有更快的运行速度。然而，当前方法大多针对单人进行建模，即模型一旦训练完毕后不具有很强的泛化性，有些方法支持多人脸驱动但生成的图像质量较差，这极大地限制了模型的应用性。因此，如何在保证生成高质量的人脸驱动图像的前提下实现多人脸驱动是一项有巨大意义的挑战。此外，当前方法大多以图像作为控制信号进行研究，基于音频的人脸驱动因音频领域发展较缓尚未有健全的研究标准，而近一年泛娱乐和元宇宙等领域又促生了相关技术的需求，故完善基于音频控制的人脸驱动研究体系并实现高质量音频多人脸驱动技术是一项极有挑战性和意义的问题。
此外，当前基于音频的人脸驱动数据集存在图像质量较差且分辨率不高的问题，这极大地限制了该研究领域的发展速度，因此高质量音频人脸驱动数据集的构建是非常有必要的。
(3) 运动约束下的图像序列生成关注视频语义级的时序合理性及多样性生成问题，本研究针对典型的图像动态化任务进行研究。运动约束下的图像序列生成问题中，图像动态化任务旨在以单帧图像作为输入预测动态视频，核心需要关注生成视频的质量和运动的合理性。对于前者，许多工作通过引入对抗训练过程改善了生成视频的质量，但基于向量输入的模型生成的图像质量仍差强人意，因此，如何设计高质量的动态视频生成框架尤为重要。对于后者，一张图像可对应多个时序视频输出，该病态生成特性导致模型很难建模物体的运动特性。一些工作基于解耦的思路使用循环神经网络对时序运动进行显性建模，虽然缓解了运动预测合理性的问题，但需要额外标记的人体姿态信息以及时序预测模型，同时串行的预测过程在实际应用中速度相对较慢。因此，如何基于运动和纹理解耦的思路设计并行解码的高质量视频生成模型是一项极具挑战性和实用价值的研究课题。
51绪论浙江大学博士学位论文1.3研究内容与主要贡献图1.3本文研究内容与思路本文从基于深度学习的计算机视觉在图像生成领域的研究及应用背景出发，对多种具体应用任务进行了分析和总结，结合实际应用需求阐述了条件式视觉内容生成研究的必要性和紧迫性，并对研究现状与挑战进行了探讨。本文研究内容按照对信息理解程度逐渐升高的原则分为局部结构感知的图像增强（图像像素级）、多条件受限的图像纹理语义迁移（图像语义级）、多条件受限的图像几何语义编辑（图像语义级）和运动约束下的图像序列生成（视频语义级）四个层面，并以多个典型任务作为对象展开研究。具体包括像素级的基础图像增强技术，如图像上色和图像超分辨率，以及语义级的高级图像生成任务，如人脸换脸、基于多信息源的人脸驱动和视频生成，并以提高模型的准确性、高效性和泛化性为目标，针对多种生成任务提出了一系列针对性的有效方法。本文的研究内容与思路框架如图1.3 所示，主要包含如下四个部分：
局部结构感知的图像增强本文第一部分从局部结构感知的图像增强出发，针对具体的联合图像上色和超分辨率问题进行研究。作为常用的图像增强技术，在老照片修复应用中图像上色和图像超分算法一般被串行使用以获得视觉友好的增强图像，然而这种方式低效且效果很难保证。基于此，本章首次提出联合图像上色和超分应用任务并设计了一个高效的端到端SCSNet 去实现该任务目标。此外，本文针对上色子任务设计了即插即用的特征融合模块以更好地进行颜色特征匹配，针对超分子任务研究了如何在保证局部细节清晰可鉴的基础上实现连续分辨率空间上的的图像放大方法。具体而言，总结该研究内容6浙江大学博士学位论文1绪论的贡献如下：
1) 本研究提出了一种高效的联合图像上色和超分辨率端到端框架，支持自动上色和参考上色两种模式，大量定性和定量实验证明提出的方法具有明显更优越的生成效果和运行效率优势，且生成的图像相比于主流方法具有更高的真实性。2) 本研究设计了一个即插即用的金字塔阀控交叉注意力模块，不仅能更好地理、解并聚合参考图像的颜色信息，同时具有较强的解释性。3) 本研究精心设计了连续像素映射模块，在连续空间中实现任意倍率图像放大，友好的计算量带来更大的实用价值。
多条件受限的图像纹理语义迁移本文第二部分从多条件受限的图像纹理语义迁移出发，针对具体的区域注意力感知的人脸换脸问题进行研究。
主流方法均对基于StyleGAN的高分辨率逆映射方案进行改进，人脸的身份信息一般仅在全局特征表示层面通过AdaIN操作方法进行注入，这忽略了局部身份信息的交互过程，导致换脸图像中像素值不应变化的区域不可避免地会因共享权重机制存在生成误差。基于此，本章提出了新颖的局部加全局双分支结构对人脸进行更精细的建模，同时利用StyleGAN 天然的面部语义表征能力设计了轻量的无监督软掩膜预测模块。具体而言，总结该研究内容的贡献如下：1) 本研究提出了一种基于区域注意力感知的换脸方法RAFSwap，其包含一个新颖的面部区域感知的局部分支和源特征适应的全局分支：前者通过引入Transformer 来有效地建模不重合的多尺度面部语义交互，后者通过补充全局身份相关的线索来进一步保证生成图像的身份一致性。
2) 本研究针对局部分支设计了区域感知的身份编码器、基于注意力机制的特征交互模块和局部感知的身份投影器三个子模块，并在多尺度上进行了扩展，该细粒度建模方式能够极大地提升身份迁移效果。3) 为了解决当前的逆映射框架会受到物体遮挡和背景扭曲的影响，本研究提出了一种无监督人脸软掩膜预测模块，提升模型在实际应用时的准确性与泛化性。4) 大量的定性和定量实验证明所提方法相较于最先进对比方法的优越性，生成的图像在保证高保真度的基础上获得了更好的身份一致性表现，同时在人工评估实验中也更受青睐。
多条件受限的图像几何语义编辑本文第三部分从多条件受限的图像几何语义编辑出发，针对具体的多信息源控制的多人脸驱动问题进行研究。得益于计算机视觉的飞速发展，主流人脸驱动方法几乎都基于深度学习中的生成对抗网络进行构建，在保证驱动效果的基础上相较于传统方法具有更快的运行速度。然而，大多数方法仅支持单人建模以生成高质量的驱动图像，而当前的多人脸驱动模型在图像生成质量上却差强人意。此外，受限于音频的缓慢发展，基于音频控制的人脸驱动更是鲜有人问津。基于此，本文以实际应用71绪论浙江大学博士学位论文需求出发，针对基于图像/音频控制的多人脸驱动算法进行了研究，致力于在模型的图像生成质量和可控性层面进行深入思考与改进。具体而言，总结该研究内容的贡献如下：1)本研究提出了一种基于人脸几何和纹理信息解耦思想设计的多人脸驱动模型FReeNet，其包含一个精心设计的人脸关键点转换器分支以在几何空间上进行面部运动迁移，以及几何感知生成器分支生成人脸驱动图像。2) 本研究基于三元组损失和感知损失设计了三元组感知损失函数，训练时加强了模型的纹理/几何信息的解耦学习能力。3) 本研究设计了音频特征融合器和几何控制器模块，基于FReeNet 框架实现了基于音频输入的多人脸驱动模型，同时提出了一个高质量的AnnVI 数据集以支持高分辨率的图像生成研究。4) 广泛的定性和定量实验表明提出的方法很好地实现了多信息源控制的多人脸驱动任务，相较于主流方法在多个公开数据集上获得了更高的指标，人工评估实验也证明了所提方法生成的图像具有更高的质量和真实性。
运动约束下的图像序列生成本文第四部分从运动约束下的图像序列生成出发，针对具体的运动纹理解耦的图像动态化问题进行研究。图像动态化任务旨在以单帧图像作为输入预测动态视频，其时序运动的合理性和多样性以及生成的图像序列质量是评估算法优劣的主要标准。然而，当前工作主要使用循环神经网络对时序运动进行显性建模，虽然缓解了上述问题，但需要额外的标注或算力成本，同时时序串行预测在实际应用中速度也相对较慢。基于此，本文针对并行视频生成模型进行研究以提高模型的实用性，同时基于运动和纹理解耦的思路设计高效的运动特征表示及注入方式，以保证模型的有效性和多样性生成特性。具体而言，总结该研究内容的贡献如下：1) 本研究基于运动和纹理解耦的思想设计了端到端的动态视频生成框架，在内容损失、运动损失和对抗损失的约束下实现了高质量和多样化的动态视频生成。2) 本研究针对运动解耦空间设计了光流编码器模块，将表示视频运动的光流信息编码为归一化向量，同时提供了随机运动向量采样的推理方式实现多样化的视频生成。3) 本研究基于解耦思想设计了双分支动态视频生成器，其包含一个运动分支来引入运动控制信号，以及一个内容分支来提取纹理信息，在两者作用下生成目标动态视频。4) 针对当前延时视频数据集质量较差的问题，本研究提出了一个大规模的高分辨率QST 数据集来支撑该任务的研究，同时该数据集也可作为高分辨率图像和视频生成等任务的新基准，为多种任务的持续发展带来新鲜血液。5) 大量的实验证明了本文方法相对于主流方法的优越性，在人脸数据集和多个延时视频数据集上获得了更高的指标和更好的视觉效果，同时能够以一张初始图像作为输入生成多样化的动态视频，具有非常高的应用潜力。
8浙江大学博士学位论文1绪论1.4本文组织结构本文组织结构如图1.4 所示，共分为七个章节对条件式视觉内容生成问题展开深入研究，以信息理解程度为准则递进呈现不同层次的研究内容。各章节的具体内容安排如下：
图1.4本文组织结构第一章从深度学习在图像生成领域的研究及应用背景出发，根据信息理解程度将多种条件式视觉图像生成任务进行了划分及归类，总结出局部结构感知的图像增强、多条件受限的图像纹理语义迁移、多条件受限的图像几何语义编辑和运动约束下的图像序列生成四个层面的研究问题，从典型任务和应用价值层面讨论了不同层次问题下具体的条件式视觉内容生成任务的研究意义，并分析了相应的技术挑战与潜在改进思路，最后简要概括了本文的主要研究内容和贡献。
第二章从条件式深度生成模型、像素级图像生成和语义级图像视频生成研究三个方面进行总结，系统性地介绍了基于深度学习的条件式视觉内容生成的国内外研究现状。该章节首先回顾了深度生成模型算法的研究时间线，从向量输入和图像输入两方面对相关代表性工作进行了整理与论述。其次，从应用角度出发对像素级/语义级图像生成任务进行了分类梳理，针对本文研究的图像上色、图像超分、人脸换脸、人脸驱动和视频生成五个典型任务归纳了主流研究路线，并对相关工作的优点及局限性进行了客观的分析与评价，阐述了本文研究工作的动机与必要性。
第三章从联合图像上色和超分辨率框架研究切入对局部结构感知的图像增强开展研究。该章节从旧照修复的实际应用需求出发，提出了一个高效的联合图像上色超分框架，91绪论浙江大学博士学位论文能够在保证模型效果的前提下实现端到端的模型训练和推理。此外，本章提出了即插即用的金字塔阀控交叉注意力模块以更好地聚合颜色信息，同时设计了连续像素映射模块以实现连续倍率图像放大的应用需求。最后，充分的定性和定量实验证明了所提框架的有效性和高效性，剥离实验也证明了模型中各个组件的有效性和必要性。
第四章从基于区域注意力感知的人脸换脸研究切入对多条件受限的图像纹理语义迁移开展研究。该章节针对基于StyleGAN 的逆映射方案只通过全局特征建模而不能很好地保持身份一致性的问题，提出局部加全局双分支结构对人脸进行更精细的建模，引入了基于注意力机制的局部感知模块以进行人脸五官之间的信息交互。此外，为了缓解因共享参数导致的换脸图像背景不一致问题，本章节基于StyleGAN 强大的面部语义表征能力设计了无监督软掩膜预测模块，通过对生成的换脸图像与目标图像进行混合得到属性一致性更强的输出结果。最后，该章节设计了大量的实验对所提出的基于区域注意力感知的人脸换脸模型进行了验证与分析，证明了其在生成图像质量和身份一致性保持上具有显著的优越性。
第五章从基于多信息源控制的多人脸驱动研究切入对多条件受限的图像几何语义编辑开展研究。该章节针对当前的人脸驱动方法仅支持单人脸高质量建模或少数驱动模型在生成图像质量上较差的问题，首次提出了高质量多人脸驱动模型。得益于运动纹理解耦的设计，所提方法在保证高质量图像生成的基础上具有很好的可控性。此外，本章将该框架扩展到了音频人脸驱动任务，并发布了一个高质量AnnVI 数据集以支持算法的持续研究。最后，该章节针对提出的多信息源控制的多人脸驱动方法进行了定量和定性实验，证明了其相较于主流方法具有更好的定量视觉效果和定性评估指标结果，同时具有显著的效率提升。
第六章从运动纹理解耦的图像动态化研究切入对运动约束下的图像序列生成开展研究。针对当前的单帧图像到视频生成方法存在生成视频质量较差或运动不合理的问题，本章基于运动和纹理解耦的思想设计了全新端到端的动态视频生成框架，支持快速的并行视频解码和多样化测试生成以支撑实际的应用需求。此外，本章提出了一个大规模的高分辨率QST 数据集以支持图像动态化任务的持续研究，同时该数据集也可作为高分辨率图像和视频生成等任务的新基准。最后，多个数据集上的大量对比实验证明了所提方法的优越性，同时消融实验结果也说明了每一个模块的有效性。
第七章对本文的工作内容进行了总结，同时对未来的研究方向进行了展望，在基于深度学习的条件式视觉内容生成问题上列举了一些有改进潜力和应用前景的研究思路。
102相关文献综述自本世纪一十年代以来，以卷积神经网络为主的深度学习技术蓬勃发展，在多个领域取得了瞩目的进步，尤其是2014 年Goodfellow 等人 首次提出生成对抗网络（GenerativeAdversarial Networks，GAN）概念后，基于深度学习的非条件式视觉内容生成研究发展火热[72,73]，生成图像的质量也越来越高，但由于该技术的不可控性导致在很多场景下的应用受限。此后，随着条件式生成对抗网络（Conditional GAN）的提出以及应用需求的增加，基于深度学习的条件式视觉内容生成技术受到广泛研究，逐渐实现在条件信息约束下可控地生成目标图像，如目标类别、图像、音频和运动信息等，在泛娱乐、元宇宙、影视制作和安防等领域具有重要应用价值，如图2.1 左侧所示。本文研究基于深度学习的条件式视觉内容生成技术，从具体的像素级图像生成和语义级图像生成任务着手，按照对输入信息的理解程度由浅及深展开研究。首先，本章将介绍条件式深度图像生成模型研究现状，从生成对抗网络理论出发，回顾基于深度学习的图像生成模型和数据集的发展进程，并对条件式视觉内容生成领域的基础框架进行介绍。其次，围绕条件式视觉内容生成研究目标和实际的生成应用需求，本章按照对图像信息的理解程度依次对局部结构感知的图像增强中的图像上色和超分辨率问题、多条件受限的图像语义编辑中的人脸换脸和人脸驱动问题以及运动约束下的图像序列生成中的视频生成五个典型任务的文献进行了梳理，归纳其发展脉络以及当前面临的挑战。图2.1 右侧直观展示了本文研究的典型任务名称及目标。
2.1条件式深度图像生成模型研究现状生成模型是一种经典的无监督学习模型，其目的是生成与真实分布尽可能接近的真实图像，有自编码器（AutoEncoder，AE）、变分自编码器（Variational AutoEncoder，VAE）、生成对抗网络、流模型Glow 和量化变分自编码器（Vector Quantized VariationalAutoEncoder，VQ-VAE）等若干种类型，考虑到当前研究的深入程度和模型自身，本文针对效果较好且发展潜力较大的生成对抗网络进行相关研究工作。条件式深度图像生成研究建立于非条件式深度图像生成之上，聚焦于如何通过输入条件可控地生成合理的高质量图像，在多种实际应用中发挥着重要的作用，如图像上色和图像超分辨率等低层次像素级生成任务，以及人脸换脸、人脸驱动和视频生成等高层次语义级生成任务。本小节从生112相关文献综述浙江大学博士学位论文图2.1本文具体研究任务总结成对抗网络理论出发对其训练过程做了介绍，并对非条件式生成对抗网络框架的发展及国内外研究现状进行了总结，最后介绍了当前典型的两种主流条件式视觉内容生成框架。
2.1.1生成对抗网络理论图2.2生成对抗网络训练过程生成对抗网络概念最早于2014 年由Goodfellow 等人 发布，其核心思想受启发于博弈论中的零和博弈，将生成问题视作生成器和鉴别器两个网络的对抗和博弈。具体地，生成器以均匀分布或者正态分布中采样的随机噪声作为输入，试图产生更接近真实分布的合成数据，而鉴别器用来辨别生成器的输出和真实数据的差异，可以视为一个二分类器，采用交叉熵表示该过程如下：
minG maxD V (D, G) = Ex∼pdata (x)[log D(x)] + Ez∼pz(z)[log(1 −D(G(z)))],(2.1)其中z 为随机噪声，x 为真实数据，G 和D 分别表示生成器和鉴别器网络，第一项log D(x)表示鉴别器对真实数据的判断，第二项log(1 −D(G(z))) 则表示对合成数据的损失项。训练时通过如此的极大极小博弈过程，循环交替地优化G 和D 来训练相应的模型权重，直12浙江大学博士学位论文2相关文献综述到达到Nash 均衡点代表训练结束。图2.2 定性展示了训练过程中数据分布的动态变化特性，以黑色点线、绿色实线和蓝色虚线分别表示真实数据分布、生成数据分布和鉴别器判别结果，两条水平线之间的单向箭头表示生成器将采样到的噪声映射到真实数据过程。随着过程(a)→(d) 的训练进行，生成的数据G(z) 与真实数据分布逐渐接近，D 也逐渐不能鉴别出输入数据的真假性，即输入数据的真假概率判断结果收敛至0.5。训练过程到达(d)时达到Nash 均衡点，生成数据与真实数据分布重叠，即G 生成的虚假数据与真实数据分布极为接近。
图2.3基于生成对抗思想的图像生成框架得益于上述生成对抗网络理论的提出，高质量图像生成问题进一步飞速发展。图2.3展示了基于该生成对抗训练思想的图像生成框架，生成器从采样的随机噪声中解码对应的生成人脸图像，与真值图像池中随机采样的真实人脸图像交替送入鉴别器进行对抗训练，这种方式能够极大地提高生成图像的质量和纹理细节。最近几年，许多基于该思想的优秀图像生成方法[79–87] 被相继提出，显著地提高了生成图像的视觉效果和定量评估指标结果。
2.1.2任务数据集介绍数据是各种计算机视觉任务的基石，尤其是在生成任务中数据的规模、分辨率和质量好坏等因素对对算法的研究和落地至关重要。表2.1 列举了为基础图像生成模型研究提供支撑的一些常用数据集信息，可以发现在2012 年之前，数据集的规模普遍较小且分辨率较低，这与当时计算机视觉研究热度较低且算力有限有很强的关系。自从AlexNet2012年获得ImageNet LSVRC 比赛冠军之后，基于深度学习的计算机视觉技术飞速发展，直到132相关文献综述浙江大学博士学位论文数据集名称提出年份数据描述分辨率数据量MNIST 1998小型分类数据集28×2870,000 张图像CIFAR10 2009小型分类数据集32×3260,000 张图像CIFAR100 2009小型分类数据集32×3260,000 张图像STL10 2011小型分类数据集96×9613,000 张图像ImageNet-1k 2012通用分类数据集256×2561,331,167 张图像LSUN 2015大型分类数据集256×256每类12 万到300 万张图像CelebA 2015人脸属性数据集178×218202,599 张图像VoxCeleb1 2017音视频人脸数据256×256超过10 万个片段VoxCeleb2 2018音视频人脸数据256×256超过100 万个片段CelebA-HQ 2018高质量人脸数据512×51230,000 张图像FFHQ 2018高质量人脸数据1024×102470,000 张图像AFHQ 2020高质量动物数据512×51215,000 张图像2014 年Goodfellow 等人 提出生成对抗网络概念，图像生成领域再一次受到研究者的广泛关注，有关的高质量数据集也相继被提出以支持算法的进一步研究。通过时间线可以看到，从2009 年的CIFAR10 到2015 年的LSUN，再到最近两年的CelebA-HQ 和FFHQ，图像数据的分辨率越来越高且规模也逐渐增大，这繁荣的发展之路也得益于日渐增长的算力支持和高清图像的应用需求。
2.1.3非条件式生成模型代表性工作自生成对抗网络概念从2014 年提出以来，许多优秀的结构设计工作相继被提出以提高生成图像的质量并改善多样性生成问题[49,74,81,96–99,99–112]。图2.4 显示了一些历年代表性工作，原始的GAN 模型 使用全连接神经网络作为生成器和鉴别器，这种最简单的直观架构适用于一些简单的图像数据集，比如MNIST 和CIFAR10，但对于更复杂的图像生成则显得力不从心。针对生成图像分辨率较低且泛化性不足的问题，Denton 等人提出在拉普拉斯金字塔框架内应用级联CNN 提取不同分辨率下的特征，设计的LAPGAN实现了从低分辨率输入得到高分辨率图像的目标，而同时期的DCGAN 首次将反卷积神经网络体系结构应用于生成器的设计中，实现了更高分辨率图像生成的同时显著提升了生成图像的质量。Zhao 等人 引入能量概念到GAN 设计当中并将鉴别器视为一个能量函数，其认为低能量对应于数据流形附近的区域而高能量属于其它区域，同时使用自编码器架构设计了一个EBGAN 示例，在实现高分辨率图像生成的前提下具有更好的稳定性。
截止2017 年，基于深度学习的图像生成模型尽管在图像质量和稳定性层面取得了很大的进步，但生成的图像分辨率很难突破256×256 且很容易产生伪造缺陷。基于上述问题，采用渐进式生成思想 的PROGAN 横空出世，从4×4 分辨率的输入图像开始，生成器14浙江大学博士学位论文2相关文献综述图2.4非条件式生成模型历年代表性工作和鉴别器的模型深度和大小随着训练过程的进行逐渐增大，直到最终实现1024×1024 分辨率的图像生成，如图2.5 所示。如此的分辨率适应生成过程不仅会使两个网络的学习更加稳定，同时由于当前分辨率会通过横向连接利用前置分辨率先验知识而不会导致遗忘问题。后续的同时期工作基本沿用该训练策略[83,85]，以获得高分辨率和高可信度的生成图像。然而，这些工作均基于标准的卷积层进行构建，考虑到卷积操作基于静态建模机制且只能建模局部特征，导致这类生成模型在学习多类别图像生成时存在较大困难，或者生成图像中的关键语义区域会发生偏移，比如鼻子在生成的人脸中没有出现在正确的位置。
受到基于全局动态建模的自注意力机制（Self-Attention） 启发，SAGAN 在生成器和鉴别器架构设计时引入了该模块，使得在全局学习和长期依赖建模上具有更大的优势，首次在ImageNet 多类别图像生成上取得了良好的性能。随后的BigGAN 基于SAGAN 改进，证明了更复杂的模型和更大的批处理大小能够显著提高GAN 的性能，再一次刷新了ImageNet 数据集上的生成结果。2019 年StyleGAN 的发布可谓轰动了整个图像生成领域，极大地提高了图像分布度量结果，且生成图像的质量真实到人眼都很难分辨。
具体地，该方法借鉴图像风格迁移领域的信息注入和控制方式，实现了自动学习、无监督地分离高152相关文献综述浙江大学博士学位论文图2.5渐进式训练过程级属性和多样性生成的目标，同时还支持在特定尺度上进行生成控制。最近两年，Karras团队基于该框架又进一步做了改进，针对少量生成图像会出现明显的水珠瑕疵问题改进了StyleGAN2，针对像素粘连导致的时序不和谐问题提出了StyleGAN3。
除了上述针对基础模型架构的研究工作，还有一些研究者从损失函数设计和稳定性训练角度提出了相关的改进措施[109,116–119]。自从GAN 被提出来之后就存在着训练困难、生成器和鉴别器的损失无法指示训练进程、生成样本缺乏多样性等问题。由此，Gulrajani 等人于2017 年提出了Wasserstein GAN，从理论上分析了原始GAN 的问题所在并针对性地给出了改进要点，不仅极大地改善了训练不稳定和模式坍塌问题，同时提供了类似准确率的概念用来描述训练进程。然而，该方法在某些情况也比较难收敛且会生成较差的样本，原因在于WGAN 采用了权重修剪策略来近似满足鉴别器的Lipschitz 约束，这会潜在地导致训练过程产生一些不可控的行为，比如参数分布不连续和梯度消失或爆炸。于是乎Gulrajani 等人改进WGAN 提出了WGAN-GP，使用额外的梯度惩罚项来进行自适应的权重修正，很好地解决了上述问题。同样针对Lipschitz 连续性问题，Miyato 等人 提出光谱归一化操作来进行约束，每一层参数的权重更新之后均除以最大奇异值以限制拉伸系数不会超过1。至此，GAN 的稳定训练问题基本得到了解决，后续的相关改进工作主要针对图像生成质量的进一步改善进行研究[122–125]。
2.1.4典型条件式视觉内容生成架构条件式生成模型最早由Mirza 等人于2014 年在CGAN 工作中提出，目的是希望能够可控地进行图像生成而不是单纯的随机生成过程，这是该技术在实际应用中落地的关键，其核心思想是在生成器和判别器中引入额外的条件信息y 来进行定向可控生成，表16浙江大学博士学位论文2相关文献综述示为：
minG maxD V (D, G) = Ex∼pdata (x)[log D(x | y)] + Ez∼pz(z)[log(1 −D(G(z | y)))],(2.2)研究时需要以任务定义为导向将条件信息y 具体化，比如语义类别、目标人脸属性、运动姿态等条件信息。最近几年，许多改进的条件式视觉内容生成模型被相继提出[48–50,95,126–131]，不仅生成图像质量越来越好且分辨率也越来越高，作为基础支撑模型在许多条件式视觉内容生成任务上发挥了重要作用，比如图像翻译[132–139]、图像上色[15,35,140–150]、图像超分辨率[19,21,23,24,30,151–154]、人脸换脸[54–58,155,156] 和人脸驱动[63,64,67,157–164] 等。视频生成可视为图像生成框架在时序上的扩展，使用多通道输出同时预测多帧图像[165,166]，也有些研究者引入3D 卷积或LSTM 进行时序图像生成建模[166–170]。
图2.6UNet 和pSp 架构示意图主流的条件式视觉内容生成工作一般基于UNet 或pSp 框架进行实现，前者因较早提出而受到研究者的广泛关注，而基于StyleGAN 框架的后者具备高质量图像生成能力，本文研究工作也借鉴了上述两个架构的设计思路。图2.6 左半部分展示UNet 架构的结构图，可以看到模型整体为U 型结构设计，编码器部分逐步提取深层特征，经跳接操作与上采样特征拼接后由解码器得到输出图像，该框架的特点是参数量较小且速度较快。
图2.6 右半部分为pSp 架构示意图，输入图像条件经编码器提取不同分辨率的层级特征，经由map2style 模块映射到StyleGAN 中不同深度的风格编码中，最后利用StyleGAN 的高分辨率生成能力得到1,024×1,024 输出图像。该框架相比于UNet 具有更高质量和更高分辨率的图像生成能力，但速度和参数量不占优势。实际应用时应根据任务的性质和指标要求灵活选择合适的框架进行研究。
2.2像素级图像生成研究现状低层级的像素级图像生成旨在对图像中局部区域进行建模而不需要对高层语义信息进行理解，在图像上色、图像超分辨率、图像去噪、图像去雾和图像去模糊等领域均有广172相关文献综述浙江大学博士学位论文泛应用。考虑到最近火热的老照片着色及图像增强应用需求，本文针对图像上色和图像超分辨率进行深入研究，并阐述相关任务的研究现状。
2.2.1图像上色研究现状图2.7图像上色任务示意图图像上色任务（Image Colorization）旨在从给定的灰度图像中预测两个缺失的通道信息，即在LAB 空间下使用L 通道预测AB 通道，在旧照修复和影音传播增强等领域中发挥着重要作用。如图2.7 所示，图像上色任务根据是否需要参考图像信息可分为自动上色和参考上色两种模式，前者只需要黑白图像输入，由预训练的网络决定输出图像的颜色结果，故模型测试时的上色倾好受到训练数据的色彩分布影响；后者则需要额外的参考图像来提供可控的语义颜色信息，其研究难点是如何将参考图像中的颜色信息合理地注入到黑白图像中的不同语义内容上。
自动上色模式基于深度学习的上色方法出现之前，Deshpande 等人 提出了一个LEARCH 框架训练色度映射中的二次目标函数，其系数由图像特征通过随机森林算法进行控制。该目标函数允许长尺度空间相关性建模，通过约束空间像素误差实现图像上色。
然而，这种传统的方式效果比较有限，经常会出现上色错误或边界不分的情况，随后基于深度学习的方法几乎主导了自动图像上色领域[16,17,143,147,171]。Cheng 等人 提出使用多级特征描述子来回归像素值，为了进一步降低生成图像中的伪影，该工作提出了一种基于联合双边滤波的后处理步骤以及一种自适应图像聚类技术来整合全局图像信息。随后，Zhang 等人 将色度空间量化为有限的像素值区间，把图像上色问题视为一种特殊的分18浙江大学博士学位论文2相关文献综述类问题来缓解潜在的不确定病态问题，并在训练时使用类平衡策略来增加生成图像的颜色多样性。后来一些基于VAE 的工作[18,148] 学习颜色场的低维嵌入，而一些研究[172,173] 引入对抗性训练思想来生成多样化且真实可信的图像。此外，Lei 等人 设计了一个包含上色和细化的两阶段模型，而I2C 使用了一个现成的目标检测器获取额外的目标级特征，额外的先验知识不仅能提升对上色目标的定位能力，也能提升上色的合理性。最近，Wan等人 引入了Transformer 结构到上色任务中，实现了很好上色结果的同时具有不错的时序颜色稳定性，在多个数据集上获得了最先进的结果。尽管上述的上色方法取得了不错的效果，但在一些特殊样例下仍会存在上色不合理的问题，且自动上色设计使得网络训练完毕后针对同一张灰度图像只能生成一种预测结果，导致模型丧失了可控性和多样性而限制了实用范围。
参考上色模式为了提高上色任务的可控性，一些基于参考图像的上色方法应运而生，其除了黑白图像外还需额外的一张彩色图像或颜色条件输入来提供色彩控制信息[37,149]。
Zou 等人 提出了高质量彩色图像生成模型SECat，其在颜色标签信息的控制下对灰度线稿图像进行上色，而工作[34,35,149] 则使用参考图像作为控制信息对灰度素描图着色。为了进一步提高上色的合理性和生成图像的真实性，一些工作[35,174,175] 引入额外的鉴别器构建对抗网络来辅助训练过程。DeepRemaster 提出了一个相似度子模块来计算源图像和参考图像之间的双向相似度映射，同时在二维图像上色基础上扩展了时间维度，使用基于注意力机制的3D 卷积构建上色模型，具有很好的时序一致性。考虑到单阶段模型的局限性，一些方法[36,37] 采用coarse-to-fine 思想设计了级联上色模型，后一阶段以前一阶段的上色输出作为输入对上色结果进行精调，以提供更精细合理的上色结果。尽管上述方法使用不同的思路实现了更好的上色结果，但如何合理地将参考特征聚合到源图像中仍然是一个非常大的挑战[36,176]。最近一些工作在特征融合方面做了尝试，Gray2ColorNet 提供了一种基于门控注意力机制的颜色融合网络，一些研究者[38,150] 也将研究火热且建模能力很强的Transformer 结构引入到图像上色任务中。尽管如此，当前的方法在很多场景中，尤其是数据集外包含较多目标的测试案例中仍容易产生伪影，比如颜色偏移和边界不明显的问题，仍有待进一步研究。
2.2.2图像超分辨率研究现状图像超分辨率（Super Resolution）是从给定的低分辨率图像中恢复高分辨率图像的过程，是计算机视觉的一个经典应用，在监控设备、卫星图像遥感、移动端数字高清、视频复原和医学影像等领域都有重要的应用价值。
自从Dong 等人 于2015 年提出了基于深192相关文献综述浙江大学博士学位论文度卷积神经网络（Convolutional Neural Network，CNN）的SRCNN 之后，许多基于CNN的方法[19–24] 被相继提出并取得了远超传统方法的结果。EDSR 移除传统残差网络中不必要的批量归一化操作，并设计了一种新的多尺度深度超分辨率框架，显著提高了超分网络性能。RCAN 和RDN 对基础模型中的残差模块进行了改进，前者提出了残差中的残差（Residual in Residual）结构来形成非常深的网络，其包含了几个具有长跳跃连接的残差组，每个残差组包含一些具有短跳跃连接的残差块；后者设计了残差密集块（ResidualDense Block，RDB），其通过密集连接的卷积层提取丰富的局部特征，同时该方法允许从前一个RDB 的状态直接连接到当前RDB 的所有层，然后使用RDB 中的局部特征融合操作从先前和当前的局部特征中自适应地学习更有效的特征。一些研究者[22,23] 在训练阶段进一步引入对抗损失，大大提高了模型的性能。为了解决真实世界的图像匹配问题，工作[178,179] 贡献了新的基于真实世界的数据集，其包含了若干同一真实场景中成对的低分辨率和高分辨率图像。最近，Guo 等人 提出了一种针对成对和非成对数据的双向回归方法，其形成了一个闭环以提供额外的监督信息。随着硬件计算能力的增强，在大规模数据集上基于Transformer 的预训练模型已显示出优于传统方法的有效性，Chen 等人 设计了一种新的预训练模型IPT，能够最大限度地挖掘Transformer 的潜力并将其下沉到图像超分辨率任务中。上述方法均取得了不错的效果，但网络一旦训练完毕后只能进行固定倍率的图像放大，应用时一般会再一次执行缩放操作以匹配目标设备分辨率而造成额外的算力损耗，故连续倍率的超分算法研究对于应用显得尤为必要。Meta-SR 针对该问题提出了解决方案，其通过预测每个像素的卷积权重来进行图像的连续放大，但整体来看该方法的效果和效率还有待进一步提高。
2.3语义级图像视频生成研究现状高层次的语义级图像视频生成旨在对图像中高层语义信息进行理解，并根据任务目标和相应的输入条件生成合理的输出图像。考虑到泛娱乐、元宇宙行业对以人为中心的内容生成和创造的需求，本文针对人脸换脸、人脸驱动和视频生成三个具体应用进行深入研究，并阐述相关任务的研究现状。
2.3.1人脸换脸研究现状人像换脸也称人脸融合，旨在将源图像中的人脸身份信息迁移到目标图像中的人脸上，同时保持目标图像中背景、头发等与身份无关的属性不变，在以短视频为主的泛娱乐行业应用、影视制作、人脸安全、政治武器等领域都有极大的应用需求与价值。早期的工20浙江大学博士学位论文2相关文献综述作[54,180,181] 主要关注基于3D 模型的换脸方法，这种方式需要有经验的操作者通过人工交互进行，不仅成本较高且步骤繁琐效率较低，不能广泛适用于实际应用当中。为了解决这一问题，Thies 等人 于2016 年提出了新颖的面部表情捕获与驱动方法Face2Face，其首先利用基于非刚性模型的捆绑方法解决单目视频人脸身份恢复的欠约束问题，并使用稠密光度一致性测量来跟踪源/目标视频的面部表情，最后通过源人脸与目标人脸之间的形变传递实现实时的高质量表情驱动。Nirkin 引入全卷机神经网络实现的人脸分割模型到3DMM 换脸架构中，更精确的语义信息带来了更优质的换脸结果。基于传统三维人脸模型的换脸方法一般速度较慢，且对应用场景的软硬件环境要求较高，因此该类方法一般只在影视制作等对速度指标不是那么敏感的场景中应用较多。随着深度学习的发展和生成对抗模型最近几年的火热，人脸换脸领域的最新研究几乎均为基于深度学习的方法。DeepFaceLab 基于自编码器架构对两个特定身份进行换脸，虽然实现了自动化流程且能得到很好的换脸结果，但该方法一旦训练完毕后仅能针对特定身份进行换脸而不具有泛化性。IPGAN 利用可学习对抗网络将源人脸的身份和目标人脸的属性分别进行分离，经重组后恢复交换身份的人脸合成图像，同时该方法还可利用大量未标记的训练人脸图像提高合成人脸图像的保真度。上述方法主要针对人脸身份一致性进行研究，模型结构设计时均未考虑图像中的遮挡问题，而这在实际应用中是极常见的一种挑战。Li 等人针对该挑战提出了一种新颖的两阶段框架FaceShifter，第一阶段设计了一种用于提取多级目标人脸属性的编码器，以及以一个包含自适应注意力反规范化层（Adaptive AttentionalDenormalization）的生成器以自适应地利用和集成目标属性以生成高保真度换脸图像，而第二阶段针对面部遮挡问题提出了自训练机制的调优网络以恢复异常区域。
为了实现更高质量且高分辨率的换脸结果，目前几乎所有的方法均基于GAN 逆映射框架[131,184–188]，即在隐空间中隐式地进行身份迁移，然后通过StyleGAN 解码出换脸人脸。FaceInpainter 同样采用两阶段架构设计思路，第一阶段以3D 先验知识、目标人脸的纹理编码和源人脸的身份因子信息作为输入，结合目标人脸的前景和背景信息通过SFI（Styled Face Inpainting）模型自适应地将人脸拟合到固定的背景中，第二阶段基于AdaIN 设计了一个联合细化网络（Joint Refinement Network），结合面部遮挡信息对前一阶段的输出进一步进行细化。最近的MegaFS 基于StyleGAN2 获得了非常好的高清换脸结果，但其在处理遮挡问题上直接使用硬掩膜对输出图像进行加权，导致人脸边缘会出现生硬的伪造痕迹，且因需要额外的掩膜真值而限制了应用范围。目前几乎所有的主流方法[58,59] 均在全局特征表示层面使用AdaIN 进行源人脸和目标人脸的信息交互，而忽略了局部区域的身份相关性建模，例如嘴唇、鼻子、眉毛和眼睛，这将限制模型对身份一致性的表达能力。此外，如何设计端212相关文献综述浙江大学博士学位论文到端的模型去解决遮挡问题对于实际应用有非常大的价值，仍需要进一步进行研究。
2.3.2人脸驱动研究现状人脸驱动旨在利用驱动信号对给定的源人脸图像进行面部表情或头部姿态控制，生成的输出图像在保持源人脸身份与背景一致性的前提下，面部表情或头部姿态符合驱动信号的语义信息，在泛娱乐和元宇宙等行业具有极大的应用需求和潜力。根据驱动信号可分为基于图像控制的人脸驱动和基于音频控制的人脸驱动两种类型，前者以人脸图像作为条件控制源人脸的头部姿态和表情，而后者以音频作为条件控制信号。
基于图像控制人脸驱动研究现状得益于大规模人脸数据集的发布[190–193] 以及高精度人脸检测算法的提出[194–197]，人脸驱动研究取得了极大的进步并有许多优秀的方法被提出，按照实现方式主要分为基于三维模型的传统合成方法[158,159,198–201] 和基于深度学习的生成方法[60–65,202,203]。对于前者，人脸由一个手工制作的预定义参数化三维模型进行表征，首先通过学习或者特定设备捕获的方式获得驱动图像/视频/真实人脸的面部运动，并将其拟合到预定义模型上的参数空间中，然后通过形变渲染等计算机图形学的标准流程得到目标图像/视频。
这类方法由计算机图形中的角色动画技术发展而来，因驱动生成的人脸质量和分辨率较高而在游戏和影视制作等领域中受到广泛应用。然而，基于3D 模型的方法都需要有经验的3D 建模师进行预定义模型制作，成本较高的同时周期也较长，串行的步骤不仅繁琐且对计算量需求也较高。对于后者，受益于最近几年算力的提高和深度学习模型训练稳定性的深入研究，基于生成对抗网络的模型取得了显著的进步，该类方法一般采用端到端的设计方案而具有较快的运行速度，且在利用大规模数据集学习分布模式方面具有天然优势，非常适用于对算力要求较低的场景，具有很高的实际应用价值。这类方法一般采用编码器对输入图像和控制信号分别进行特征提取，融合后经解码器生成输出图像，并辅以对抗训练的思想以保证生成高质量的人脸驱动图像。进一步的工作[49,60,203,204] 通过引入循环一致损失实现了非成对人脸的训练，不仅极大地降低了对训练数据的要求，同时能够获得更好的生成结果。上述方法一旦训练完毕后只能对特定身份进行驱动，这极大地限制了该技术的应用价值。随后ReenactGAN 提出了多对一人脸驱动来解决上述问题，引入了转换器模块在潜在的人脸边界空间中将不同目标人员的人脸运动适应到源人脸上，然后经由特定的源人脸解码器生成人脸图像。然而，该方法在实际应用中仍然是低效的，因为每个源人脸身份都需要独有的转换器和解码器，这不仅对训练数据提出了更高的要求，同时针对每个人单独训练需要大量的计算资源和时间成本。
基于上述问题，X2Face设计了嵌入网络模型对目标人脸进行运动编码，然后利用驱动网络对源人脸进行控制生22浙江大学博士学位论文2相关文献综述成输出人脸，成功地实现一个模型进行多对多人脸驱动，即源人脸和驱动信息均支持多人输入。同时，该工作提出了一种使用大量视频数据集完全自监督训练网络的方法，这对人脸驱动技术的落地推动具有重要意义，但生成的图像在质量和面部细节方面仍有不足。随后，Siarohin 等人 提出了效果更好且解释性很强的FOMM 模型，其使用自监督训练方案对人脸的纹理和运动信息进行解耦，同时设计了由可学习的关键点和局部仿射变换组成的表示以支持复杂的面部运动。最近，Ren 等人结合传统的3DMM 方法提出了人像图像神经渲染器PIRenderer，其使用3DMM 参数控制人脸运动，即可以通过直观的参数修改生成具有准确运动的人脸图像。然而，当前方法仍很难做到既高效又有效，如何设计高效的统一模型实现多人脸面部驱动，同时保证输出人脸与源人脸的身份一致性仍需持续的研究。
基于音频控制人脸驱动研究现状随着动画制作、虚拟人和游戏等领域的发展，催生了基于音频控制的人脸驱动应用需求。一些最近的音频人脸驱动工作通过预测参数对预定义的人脸模型进行控制[161,205–211]，比如Karras 等人 提出从神经网络中直接学习输入音频波形到人脸模型的3D 顶点坐标的映射，并使用一个紧凑的隐码来消除仅靠音频无法解释的面部表情变化的歧异，推理时可直接使用隐码对面部进行情感控制。然而，上述方法因人脸模型顶点过多而需要大量的参数，Tian 等人 结合双向LSTM 模型与注意力机制建模时变语境信息在语音中的潜在表征，直接通过音频信号预测每一时刻的blendshape参数，用于控制预定义的三维模型运动。音频驱动的3D 人脸动画已经被广泛探索，但如何实现逼真的、类人的表现仍然是一个未解决的问题。针对这一问题，Cudeiro 等人提出了一个独特的4D 面部数据集，设计的VOCA 模型支持任何语音信号作为输入来制作出逼真地面部动画。然而，上述基于预定义3D 模型的传统方法同样面临着步骤繁琐且不能实时运行的问题，因此基于深度学习的工作相继被提出以解决上述问题，一些研究者[200,213] 提出生成嘴唇区域像素而不是整个人脸，这虽然降低了模型的训练难度但需要额外的人脸融合后期操作，不适合实际应用。随后，端到端模型被研究者所青睐[63,161,164,200]，因为这些方法能够以非常快的速度生成像素级别的全人脸图像。X2Face 使用无监督方案预测人脸运动的潜在表征，同时可扩展学习音频到该表征空间的映射，其经由解码器来控制生成具有符合相应运动的人脸驱动图像。Choi 等人 则提出了一个连接推理阶段和生成阶段的多模态学习框架，结合生成对抗训练技术直接从语音波形以端到端的形式直接生成人脸。最近，Zhou 等人 提出的MakeItTalk 与之前的大多方法需要后处理步骤不同，该工作学习从音频直接到图像原始像素的直接映射，并解耦输入音频信号中的内容和232相关文献综述浙江大学博士学位论文说话人身份信息，前者控制嘴唇及附近面部区域的运动，而后者决定面部表情的细节和说话者头部的姿态，这种解耦的方式具有不错的人脸驱动效果，同时也具有更好的解释性。
尽管如此，如何更好地建模音频与面部运动仍是一个值得探究的问题，因为不同的人对于相同的语音信号可能会呈现出各异的驱动结果，且如何保证高质量的语音驱图像生成仍有待进一步研究。
2.3.3视频生成研究现状视频生成是一项从噪声、图像或掩码等特殊输入条件下生成图像序列的任务[169,214–217]，在多种领域中具有非常高的应用价值和潜力，比如影视制作、短视频应用、视频数据集增广和运动预测等。该研究因相比于二维图像级别的人脸换脸和人脸驱动因扩展了时间维度而更具有挑战性，除了对图像内容进行语义理解外，还要对不同语义间的相互关系和运动信息进行建模。Michael 等人在2016 年的工作 中首次采用生成对抗训练思想缓解由标准均方误差损失函数导致的固有模糊预测，并将基线模型结构扩展到了多尺度架构上以捕获更丰富的不同细粒度特征。随后，VGAN 利用大量未标记视频学习用于视频生成任务的场景动力学模型，提出了一个基于生成对抗网络的VGAN，其包含时空卷积结构来解耦场景前景和背景，实验结果表明该方法相较于基线成功实现了一秒小视频的生成效果，证明了其通过静态图像预测未来运动的有效性。Saito 等人 则基于时空解耦的思想设计了时间层面的时序生成器和空间层面的图像生成器，前者单个隐变量作为输入输出一组时序上的隐变量，对应于生成视频中的每一帧图像，而后者将每一个时间节点的隐变量转换成对应图像。随后的MoCoGAN 采用类似的运动和内容分解的设计思路，每一帧对应的随机向量包含运动和内容两部分，经图像解码器将随机向量序列映射到视频帧序列。上述方法仅能生成较低分辨率的视频帧，为了高分辨率和高质量的视频生成，FutureGAN 引入渐进增长GAN 的概念进行图像解码，并采用时空三维卷积来捕捉视频序列的空间和时间成分。
然而，这些基于随机噪声输入和渐进式增长的方法普遍存在生成图像质量较低或训练过程较复杂的问题，给实际应用带来了极大的挑战，因此，一些研究者以单帧图像作为输入条件进行了研究。MDGAN 采用了两阶段网络设计方案，在保证生成质量的前提下能够生成较长时间范围的未来帧。该方法第一阶段以单帧图像作为输入生成一段真实内容的视频，第二阶段通过在运动学方面做监督损失来细化第一阶段生成的视频，降低与真实视频的误差。
Nam 等人 提出了一种多帧联合条件生成框架来有效地学习场景光照变化与时间之间的相关性，并进一步提出了多域训练方案，可以从两个不同分布且缺少时24浙江大学博士学位论文2相关文献综述间戳标签的数据集训练鲁棒的生成模型。与其他视频合成算法相比，该方法使用显性的时间戳作为控制变量而没有采用类似LSTM 或3D 卷积的建模方式。此外，还有一些研究工作采用额外的几何信息对特定场景下的视频生成进行建模。Yang 等人 和Cai 等人提出了一种姿态引导的人体视频合成方法，其通过对抗训练方式先预测位姿序列，然后结合提供外观信息的输入图像生成时序视频，另外一些研究[222,223] 则以语义标签作为输入合成真实感很强的视频。最近，一些基于流模型的方法也取得了较大的进步，Liang 等人设计了一个双向运动GAN 模型，同时解决了原始的未来帧预测和未来流预测任务，Li 等人 将多帧图像预测任务定义为多时间步长流预测和流到帧图像合成两阶段，前者采用3D 卷积建模时空关系而后者采用生成对抗方式预测真实视频。当前方法大多针对如何更好地建模时空关系进行研究，也有一些模型针对结构进行改进，但整体来看，生成的视频在高保真性、合理性和多样性等方面仍有较大的改进空间。
2.4本章小结本章首先对条件式深度图像生成模型研究现状进行了总结，从生成对抗网络理论、任务数据集、非条件式视觉内容生成历年代表性工作和典型条件式视觉内容生成架构四方面进行了详细介绍。然后以低层次的像素级图像生成和语义级图像视频生成两个层面作为研究内容，按照对输入信息的理解程度，由浅及深系统性地对条件式视觉内容生成中的图像上色、图像超分辨率、人脸换脸、人脸驱动和视频生成五个具体任务进行了详细梳理，对相关工作的主要贡献和局限性进行了客观的评价，并阐述了本文研究的动机与必要性。
253局部结构感知的图像增强局部结构感知的图像增强关注图像像素级的局部结构感知及分布预测问题，本章节针对联合图像上色和超分辨率框架展开具体研究，其作为低层次的条件式视觉内容生成任务，在旧照修复和影音传播增强等领域中发挥着重要作用。在恢复低分辨率灰度图象的实际应用中，使用者往往会将图像上色和超分两个模块串接使用以获得视觉友好的目标图像，但这种方式是独立且低效的。本章以实际应用需求为出发点，针对同时图像上色超分问题进行研究，提出了一个高效的联合图像上色和超分端到端框架，支持自动和参考两种上色模式。此外，本研究设计了金字塔阀控交叉注意力模块来更好地建模颜色特征交互，并提出了连续像素映射模块实现任意倍率的图像放大。多个公开测试集上的结果表明，本方法相比于当前最优方法取得了明显的效果提升，同时拥有更少的参数量和更快的运行速度，具有很高的实用价值。
3.1引言局部结构感知的图像增强研究致力于解决基础的条件式视觉内容生成问题，一般意义而言，其仅使用局部建模方式即可满足图像分布预测任务需求，不涉及较复杂的图像语义理解，包含图像上色[13–15]、图像超分辨等[19,23,24]、图像去噪[25–27]、图像风格化[39–41] 和图像和谐化[42–44] 等具体任务，在影视制作和图像增强等领域具有非常大的应用潜力。本章以典型的图像上色和图像超分辨率任务为例展开相关研究，前者在色彩分布空间上建模灰度图到彩色图的翻译过程，后者针对局部纹理分布建模低分辨率图像到高分辨率图像的翻译过程。
得益于深度学习的发展，图像上色和图像超分辨率研究最近几年取得了显著的进步，相较于传统方法具有更好的视觉效果和定量指标[16,18,19,22,151,225]。前者对灰度图进行RGB颜色估计，以提高图像视觉和美学感知质量[13,15,17,140,172]，可应用于老照片修复和艺术创作等领域；后者从给定的低分辨率（Low Resolution，LR）图像中恢复高分辨率（HighResolution，HR）图像[20,23,24]，在数字高清、显微成像和医学影像等领域都有重要的应用价值。在一些如黑白老照片修复和灰度线稿艺术创作的实际场景中，使用者希望能够将仅能获取的低分辨率灰度图像生成更有吸引力的高分辨率彩色图像。如图3.1 上半部分所示，目前基于深度学习的自动化解决流程主要包含三个级联过程：1）使用自动或参考上273局部结构感知的图像增强浙江大学博士学位论文图3.1主流图像上色超分流程与本文方法对比色模型对灰度图像进行着色以获得视觉上吸引人的RGB 图像。2）利用图像超分辨率方法学习从LR 图像到HR 图像的非线性映射。3）下采样HR 图像到适合目标设备展示的分辨率。本研究灵感来源于对上述应用流程的观察，从算法效果看，图像上色和图像超分任务由于输出不唯一而存在病态特性，当前的算法输出效果往往会出现肉眼可察觉的不和谐内容，故需要针对每一个子模型特性进行改进，因每一个环节的精度都会对结果产生影响。
从框架效率看，上述级联过程在实际使用中繁琐且效率低，根本原因在于独立的图像上色和超分辨率过程本可以共享一部分底层特征，串接的处理流程造成了特征的重复提取。此外，超分算法一般以固定的倍率放大图像，故最后阶段的设备分辨率适应的下采样操作意味着冗余计算的存在。针对这一问题，本章设计了联合图像上色和超分的端到端框架，如图3.1 下半部分所示，通过统一的网络实现同时上色和超分辨率两个目标，同时支持任意分辨率的图像放大。具体地，该框架包含图像上色和超分辨率两个分支分别对局部颜色和纹理分布进行建模。
对于上色分支，其学习如何从给定的灰度图像中预测两个缺失的通道信息，即在LAB空间下使用L 通道预测AB 通道。根据是否使用参考图像作为指引，图像上色可分为自动上色和参考上色两种模式。前者只需要低分辨率黑白图像，流程直观但生成的图像往往色度较差，原因是病态问题下的每个语义对象可以有多种颜色，如何生成自然的图像是一件很难判断的事，加之如果使用不适当的训练策略，网络往往会输出平均色彩。后者上色模式则需要额外的参考图像来提供更可控的语义颜色信息，其中一个关键点是如何将参考28浙江大学博士学位论文3局部结构感知的图像增强图像中的颜色信息合理地注入到黑白图像中的不同语义内容上。一些研究者提出计算相关矩阵来表征黑白图像和参考图像之间的双向相似性[37,227]，也有一些工作[36,38,176] 使用拼接、AdaIN 或转换器模块 来聚合信息。
尽管如此，当前的信息交互方法易导致生成图像中存在视觉缺陷，且缺乏视觉可解释性。
受Self-Attention 工作的启发，本章节设计了一个即插即用的金字塔阀控交叉注意力模块（Pyramid Valve Cross Attention, PVCAttn），其不仅实现了更好的上色效果，同时多尺度的阀控注意力机制对信息交互过程具有更好的可解释性。此外，得益于PVCAttn 模块灵活的开关设计，提出的框架支持两种上色模式的自由切换。
对于超分辨率分支，其学习如何从LR 图像重建HR 图像，目的是为了获得感官更好的输出图像。通常来说，图像超分方法一般采用固定倍率放大的建模方式[23,24]，这与自然世界中连续的视觉表示与感知方式背道而驰，同时实际应用时不匹配的放大倍率也会造成额外的算力损耗，如用训练好的4x 放大率模型部署于仅需2.8x 放大率的移动端设备。最近，一些研究者开始关注连续倍率超分模型，为实际应用提供了一个新思路，如Meta-SR 通过预测每个像素的卷积权重来尝试图像的连续放大。为了进一步提高模型效率的同时降低模型参数量，本章提出了一个全新的连续像素映射模块（Continuous PixelMapping，CPM）在连续空间中实现要求的目标分辨率输出，无需额外的下采样操作。
凭借两个模块的设计以及端到端的模型训练，本章提出的联合图像上色和超分辨率框架在多个公开数据集上取得了多项指标的最优结果，较主流方法参数量下降了至少2 倍之上的同时运行速度提升3 倍之上，有非常明显的实用优势。
总结本章研究内容的贡献如下：
1. 本研究提出了一种高效的联合图像上色和超分辨率端到端框架，支持自动上色和参考上色两种模式，大量定性和定量实验证明提出的方法具有明显更好的生成效果和运行效率优势，且生成的图像相比于主流方法具有更高的真实性。
2. 本研究设计了一个即插即用的金字塔阀控交叉注意力模块，不仅能更好地理解并聚合参考图像的颜色信息，同时具有较强的解释性。
3. 本研究设计了连续像素映射模块，在连续空间中实现任意倍率图像放大，友好的计算量带来更大的实用价值。
3.2方法概述本章针对图像上色和图像超分辨率串接的实际应用需求为研究目标，设计了一种高效的联合图像上色和超分辨率框架，其通过端到端的训练和推理方式极大地提高了模型效率，同时相比主流方法具有更好的效果。如图3.2 所示，本章提出的同时图像上色和超分293局部结构感知的图像增强浙江大学博士学位论文图3.2联合图像上色和超分辨率框架示意图方法（Simultaneously Image Colorization and Super-Resolution，SCSNet）由一个用于预测图像颜色信息的上色分支，以及一个用于生成高分辨率图像的超分辨率分支组成。对于输入低分辨率灰度图Ils（以下称为源图像），首先用初始卷积层将其扩充维度得到丰富的特征表达，即：Ils ∈R1×Hs×Ws →F inits∈R64×Hs×Ws.
对于上色分支，本章设计了全新的编码器ϕsE 和ϕrE 来提取灰度源图像和参考图像的深度特征Fs 和Fr，表示如下：
Fs =ϕsE(F inits) ∈R256×Hs/4×Ws/4,Fr =ϕrE(Ilabr ) ∈R256×Hs/4×Ws/4.
(3.1)特别地，该上色分支支持两种上色模式：直接将源图像特征映射到输出的自动模式，即Fs →Fint，以及采用即插即用的PVCAttn 模块聚合参考图像特征的参考模式：
Fint = PVCAttn(Fs, Fr) ∈R256×Hs/4×Ws/4.
(3.2)随后包含一个自注意力层和若干卷积层的解码器ϕD 将颜色信息Fint 恢复到原始分辨率Fcolor ∈R64×Hs×Ws，以用于后续的图像超分辨率环节。
对于超分辨率分支，本章以带跳接层的两个卷积层为基本块，使用由若干基本块串接的编码器ψE 从初始特征图F inits提取残差纹理特征Ftex ∈R64×Hs×Ws。随后采用3×3 卷积聚合纹理特征Ftex 和颜色特征Fcolor 得到Fcs ∈R256×Hs×Ws。最后，提出的连续像素映射模块通过映射函数F(·) 回归目标HR 图像ˆIlabt∈R3×Hs∗p×Ws∗p，其中p 代表任意的放大倍率，而F cs ∈R2×Hs∗p×Ws∗p 经由Fcs 插值得到。
30浙江大学博士学位论文3局部结构感知的图像增强图3.3阀控交叉注意力模块示意图3.3金字塔阀控交叉注意力模块3.3.1阀控交叉注意力模块为了更有效地建模源图像和参考图像之间的语义关联以提高上色的合理性，同时提高模型的可解释性，本章提出了一个新颖的阀控交叉注意力模块VCAttn。图3.3 为模块具体的结构示意图，其输入为源图像特征Fs ∈RCs×Hs×Ws 和参考图像特征Fr ∈RCr×Hr×Wr，输出目标特征图Ft ∈RCs×Hs×Ws。受自注意力机制在图像生成任务应用中的启发，本章使用交叉注意力方式建模源图像和参考图像之间的信息交互。具体地，使用三个卷积层操作从Fs 和Fr 分别提取Query 特征Qs ∈RC×Hs×Ws、Key 特征Kr ∈RC×Hr×Wr 和Value特征Vr ∈RCs×Hr×Wr。随后，Qs 和Kr 通过矩阵相乘和归一化指数函数计算得到相关矩阵CMat，其每一行代表Fs 每一个位置与Fr 的归一化相关性，与Vr 计算后得到基于参考图像的更新特征Fr→s。进一步，本章设计了阀控机制对特征进行过滤和解释，Fs 和Fr→s 拼接后预测阀通过量V1 和V2，用来控制Fs 和Fr→s 的信息流通量。图中Identity 代表源特征图Fs 是共享的。
3.3.2多尺度结构扩展考虑到单一分辨率不足以建模精细的特征交互表达，本章进一步提出在多尺度上使用阀控交叉注意力模块进行建模，设计了金字塔状的VCAttn 模块。如图3.4 所示，源图像特征Fs 和参考图像特征Fr 分别经过下采样得到多尺度特征，每个尺度特征经由不共享的阀控交叉注意力模块获得对应尺度输出，最后不同尺度特征经上采样、拼接和后处理卷积得到多尺度特征输出。
313局部结构感知的图像增强浙江大学博士学位论文图3.4金字塔阀控交叉注意力模块扩展示意图3.4连续像素映射模块主流图像超分方法一般采用固定倍率的建模方法，而真实世界中的视觉表示与感知方式确是连续的，这启发了本章对连续放大倍率输出的思考。进一步，本工作发现实际应用时不匹配的放大倍率也会造成额外的算力损耗，也催生了直接生成目标分辨率图像的需求。基于此，本工作研究在连续像素空间中对离散特征映射进行建模，并提出了一个高效的连续像素映射模块作为超分辨率输出头。示意图如图3.5 所示，图像中每个位置x, y 对应的特征F x,y 包含两部分内容：基于邻域四点进行双线性插值得到的主要特征Fx,ycs ，以及描述连续局部空间信息的局部坐标特征Zx,yrel 。
由于坐标是连续的且可以无限插值，因此输出图像的分辨率可以灵活调节，即F x,y 可以在连续空间中建模。对于每个点的局部坐标特征Zx,yrel ，本研究使用其与原始分辨率图像中最近锚点的局部相对坐标进行建模，以便为每个位置提供细粒度的空间信息指导。具体地，本研究在原始特征Fcs ∈R256×Hs×Ws 中寻找其对应的锚点，计算局部坐标特征Zx,yrel 公式如下：
Zxrel =mod(x, xunit)/xunit ∗2 −1,Zyrel =mod(y, yunit)/yunit ∗2 −1,(3.3)其中xunit = 1/Ws, yunit = 1/Hs, mod 代表余数运算，Zx/yrel 在取值范围为-1 到+1，即坐标(-1,-1) 对应于左上角，而(+1,+1) 对应于右下角。最终，由四个线性层构成的连续像素映射函数F(·) 将特征图映射到目标图像ˆIlabt 。值得注意的是，每个点的主要特征在获取时需要对齐角点。
32浙江大学博士学位论文3局部结构感知的图像增强图3.5连续像素映射模块示意图3.5总体目标函数给定真实彩色图像Ilabt与生成彩色图像ˆIlabt ，本章提出的SCSNet 在训练阶段采用三种训练损失函数：内容损失LC、感知损失LP 和对抗训练损失LAdv。
内容损失采用像素级别的内容损失函数来评估图像质量，其计算生成的目标图像和真实图像之间的ℓ1 误差：
LC = || ˆIlabt−Ilabt ||1.
(3.4)感知损失此项损失函数在特征图层面计算生成的目标图像和真实图像之间的语义差异，以保证生成的上色图像在语义细节上尽可能与真实图像一致：
LP = E"5Xl=1wl · ||ϕl( ˆIlabt ) −ϕl(Ilabt )||1#,(3.5)其中ϕl(·) 表示使用预训练的VGG16 网络 提取图像在l1 卷积层的激活特征图，wl 是l层损失量的权重。
333局部结构感知的图像增强浙江大学博士学位论文对抗训练损失采用标准的相对鉴别器D  进行对抗训练以提高图像质量和真实性，结果表明该损失函数能够极大地提高模型性能。
LGAdv =Ex∼px(D(x) −E˜x∼p˜x[D(˜x)] −1)2+E˜x∼p˜x(D(˜x) −Ex∼px[D(x)])2,LDAdv =E˜x∼p˜x(D(˜x) −Ex∼px[D(x)] −1)2,(3.6)其中px 和p˜x 表示真实的图像分布和生成的图像分布。
总的训练损失Lall 为上述损失函数的加权平均：
Lall = λCLC + λPLP + λAdvLAdv,(3.7)其中λC = 10, λP = 5 和λAdv = 1 用来平衡不同损失函数的权重。
3.6实验结果3.6.1数据集介绍本章基于广泛使用的ImageNet 数据集构建了一个高质量的ImageNet-C 子集对本研究提出的联合图像上色和超分方法进行训练和评估，并进一步在CelebA-HQ、Flow-ers、Bird 和COCO 数据集上进行不同方法的泛化性测试。评测之前，本章先对几个数据集内容及构建方法进行简单介绍：
ImageNet-C图像上色一般使用公开的ImageNet 进行训练，但经统计发现其中有一些数据色彩不明晰或图像分辨率较低，考虑到本章还对图像超分辨率进行研究且其对训练数据质量要求较高，因此构建一个基于ImageNet 的高质量图像上色超分子数据集。具体地，本章从ImageNet 中过滤掉了一些文件大小小于80K 且颜色不丰富的低质量图像，构建了一个色彩度高的高分辨率数据集ImageNet-C，其中C 代表Color，最终得到407,041张训练图像和16,216 张验证图像。
其它测试数据集为了验证模型的泛化能力，本研究使用类似的数据集构建方式对几个主流的数据集进行处理，最终得到的数据集规模为：CelebA-HQ 包含30,000 张图像，Flowers 包含8,189 张图像，Bird 包含479,548 张图像，COCO 包含98,246 张图像。
3.6.2实验设置本研究采用LAB 色彩空间进行图像上色和超分辨率任务，默认采用1∼4 上采样倍率进行实验，网络输入的源图像和参考图像分辨率为128×128。对于超分辨率任务数据集的34浙江大学博士学位论文3局部结构感知的图像增强构建，本工作沿用DRN 的双三次下采样核对高分辨率图像进行处理。训练时自动和参考两种模式交替进行，对源图像使用随机水平翻转数据增强，对参考图像使用随机水平翻转和弹性变形 数据增强。所有代码基于PyTorch 框架实现，使用Adam 优化器（参数β1 = 0.9, β2 = 0.999）对模型参数进行训练，权重衰减设置为1e−4，学习率设置为1e−4，使用8 块V100 GPU 训练模型，每块GPU 训练批大小为4，在ImageNet-C 数据集上共迭代训练50 次。
为了增加初期训练稳定性，在第一轮数据训练时采用预热学习率[235,236]策略。感知损失函数（公式3.5）的权重w1−5 设置为1.0/32, 1.0/16, 1.0/8, 1.0/4 和1.0。
3.6.3评测指标为了更全面地与主流方法进行对比，本研究采用图像上色领域使用的FID（FréchetInception Distance） 和CN（Image ColorfulNess） 在语义层面评估图像的分布和色彩度，同时采用图像超分辨率领域使用的PSNR（Peak Signal-to-Noise Ratio）和SSIM（Struc-tural Similarity） 在像素级层面评估图像预测的偏差。进一步为了充分评估各种方法的优劣，本研究聘请真人对生成图像的真实性进行评判。为了公平比较，提出的方法仅在ImageNet-C 训练而不使用额外的数据集，对比方法使用相应原论文开源的预训练模型，注意这些模型可能会使用额外的数据集进行训练。
3.6.4与主流方法对比为了充分证明所提方法的效果和效率，本研究在几个数据集上做了大量定性和定量实验，并对实验结果进行讨论。由于目前除了ColTran 尚未有其它端到端的同时图像上色和超分辨率研究工作，因此本研究选择一些最先进的上色方法(AutoColor、DRemas-ter、InstColor 和DEVC) 以及超分辨率方法(ESRGAN 和DRN) 串接作为本文研究的对比方法。
具体地，本章将上述上色方法分为自动上色和参考上色两类进行对比。
定性对比结果本小节在ImageNet-C 和COCO 两个数据集上进行了视觉感知的定性实验，以直观地展示不同方法在同时图像上色超分问题的生成效果，在评判时主要考虑生成图像的色彩丰富度和真实性。图3.6 展示了与三种主流方法的对比结果，第一列表示输入的灰度图像，其它列展示了提出方法和对比方法在不同数据集的测试结果，左上角的数字代表对应方法在V100 GPU 上的推理速度（FPS）。结果显示AutoColor 和DRemaster 在色彩度还原上效果较差，InstColor 在一些细节上会存在色偏（如第二行案例中的右手呈黄色），提出的方法在色彩和细节方面比其他方法更好。
图3.7 显示了以128 分辨率弹性形变参考图像作为上色条件下的不同上色方法结果，353局部结构感知的图像增强浙江大学博士学位论文图3.6自动上色模式可视化定性对比图3.7参考上色模式可视化定性对比除了ColTran 方法外，其它方法都能很好地传递参考颜色。但本章提出的方法在保持颜色于参考图像一致性的同时，可以生成更清晰且真实的图像，且运行速度相较于最新方法具有至少数倍的提升，证明了提出方法的先进性。值得注意的是，ColTran 输入了额外的HR灰度图像以获得更好的清晰度，这对图像同时上色超分任务来说是不公平的。因此本小节将其输入的灰度图像减少到与其他方法相同的分辨率进行测试以充分地评估不同的方法，36浙江大学博士学位论文3局部结构感知的图像增强方法ImageNet-CCelebA-HQFlowersBirdCOCO平均值AutomaticAutoColor + ESRGAN31.72525.63764.10742.95638.90440.666DRemaster + ESRGAN33.90834.77574.60145.50537.24945.208InstColor + ESRGAN26.35331.10948.42536.21224.95433.411AutoColor + DRN31.66626.08363.89143.03439.20840.776DRemaster + DRN33.99334.84274.35645.74237.39745.266InstColor + DRN26.50131.38948.47536.37825.15333.579Ours-Auto25.99227.80946.60734.40124.04731.771ReferentialDRemaster + DRN25.49829.70241.50631.84335.26732.763DEVC + DRN26.05049.12639.42642.44436.70238.750ColTran15.86010.40521.59518.58018.39116.966Ours-Ref9.63212.7719.77613.52613.81211.903方法ImageNet-CCelebA-HQFlowersBirdCOCO平均值参数量(M)↓速度(FPS)↑AutomaticAutoColor + ESRGAN3.3903.4883.2103.1903.4173.33925.43216.910DRemaster + ESRGAN2.9983.0542.6782.8333.0782.92865.5169.217InstColor + ESRGAN3.5883.3063.5103.4353.6353.49566.9905.541AutoColor + DRN3.3893.4893.2103.1893.4173.33918.56010.781DRemaster + DRN2.9963.0542.6782.8323.0762.92758.6447.037InstColor + DRN3.5883.3073.5113.4363.6353.49560.1184.671Ours-Auto4.6883.8924.7244.3344.5734.4429.95433.293ReferentialDRemaster + DRN3.9903.4264.3113.5053.7223.79173.9876.311DEVC + DRN4.2883.5704.5163.7003.9384.00269.5707.435ColTran4.6924.2155.0034.5294.6794.62470.6970.209Ours-Ref5.2884.3885.7434.8224.9745.04315.35827.466图中表示为ColTran-LR，结果表明在这种设置下该模型输出图像会变得模糊。
定量对比结果本小节在ImageNet-C、CelebA-HQ、Flowers、Bird 和COCO 五个数据集上选用图像级别和像素级别度量准则对不同方法优劣进行比较。特别地，采用图像级别的FID 评估生成图像和真实图像之间的分布差异、图像级别的CN 评判图像的色彩度、像素级别的PSNR 和SSIM 评估生成图像和真实图像之间的像素差异性。具体地，每种方法随机生成5,000 张图像图像进行评估，其中2,500 张用于自动上色任务，2,500 张用于参考上色任务。表3.1 和表3.2 分别显示了不同方法对于几个数据集在自动和参考模式下的图像级定量结果，可以总结出如下几点结论：
1. 不同超分辨率方法（即ESRGAN 和DRN）对结果影响不大，为了后续试验方便及公373局部结构感知的图像增强浙江大学博士学位论文平性，接下来的实验默认选取最先进的DRN 超分方法进行研究。
2. 参考模式往往比自动模式得到更好的指标结果，说明参考图像对于生成图像的色彩信息具有较大贡献。
3. 不同数据集的CN 度量略有差异，对于模型的泛化性提出挑战。本章的方法获得了最高的CN 评分（自动模式为4.442，参考模式为5.043），比目前的最佳方法分别提高了+0.947↑和+0.419↑。这意味着提出的方法可以更好地捕捉颜色信息，生成视觉上更胜一筹的彩色图像。
4. 本章进一步对所有方法的参数量和运行速度进行计算和比较，提出的SCSNet 拥有最少的参数（自动模式下比InstColor+DRN 降低×6↓倍；参考模式下比ColTran 降低×4↓倍）和最快的运行速度（自动模式下比InstColor+DRN 快×8↑倍; 参考模式下比ColTran 快×130↑倍），图3.8 更直观地展示了不同方法间的效果与效率差异，表明本文研究具有更高的实用价值。
图3.8主流上色超分方法FID 指标与推理速度对比果。提出的方法相比于对比方法一致性地获得了更好的评估分数，意味着SCSNet 预测的图像与真实图像更加一致。这里有一个有趣的发现，前述的ColTran 无法集成参考图像颜色(见图3.7) 也可从定量结果得到反映，因为其获得了更差的像素级度量结果。
38浙江大学博士学位论文3局部结构感知的图像增强方法ImageNet-CCelebA-HQCOCOPSNR ↑SSIM ↑PSNR ↑SSIM ↑PSNR ↑SSIM ↑DRemaster + DRN19.3430.81125.5590.91521.0390.845InstColor + DRN22.1260.84226.5230.92322.9170.856Ours-Auto22.8070.85627.1600.91723.3410.872ColTran20.7340.84524.4950.91422.7870.857DRemaster + DRN24.6710.87128.5820.92826.6630.901Ours-Ref27.6940.92330.7410.95028.1970.9313.6.5人工评估实验所研究的联合图像上色超分问题本身存在病态特性，且每个度量准则都存在一定的缺点，即有时定量指标结果很好但给人直观的感觉却不是很好，反之亦然。基于这一问题，本章节进一步使用人工评估的方式对不同方法生成的图像进行对比。具体地，在COCO数据集随机选择500 张源图像作为测试输入，每种方法可获得500 张目标图像，其中每种模式图像占比一半。提出的方法与对比方法针对同一图像构成一个测试对，每个测试对给50 个测试者观看1 秒钟，测试者需选择哪一幅图像在视觉上更真实，最终统计比例。
结果，以及与基于参考图上色的ColTran 和DRemaster+DRN 对比结果，证明了提出的方法生成的图像更受人们喜爱，意味着生成的图像视觉友好性更强且真实性较高。
对比方法真实性(%)Oursv.s.
AutoColor + DRN77.3 v.s. 22.7Oursv.s.
DRemaster + DRN90.6 v.s. 9.4Oursv.s.
InstColor+ DRN59.4 v.s. 40.6Oursv.s.
DRemaster + DRN82.7 v.s. 17.3Oursv.s.
DEVC+ DRN95.1 v.s. 4.9Oursv.s.
ColTran68.6 v.s. 31.43.6.6消融实验损失函数对结果影响提出的方法训练时采用内容损失、感知损失和对抗训练损失，为了评估损失函数对训练结果的影响，本小节在参考上色模式下使用不同损失函数的组合进行模型训练，并测试对应的图像级和像素级指标。表3.5 显示了损失函数消融结果，通过分析可得出结论：每个损失函数都对模型的性能有贡献，当所有损失函数都被应用时模393局部结构感知的图像增强浙江大学博士学位论文型能够获得最佳度量分数。
LCLPLAdvFID ↓CN ↑PSNR ↑SSIM ↑16.2904.12826.1720.89815.0684.19627.4560.91811.6165.17126.5360.9069.6325.28827.6940.923网络模块对结果影响本章提出的方法针对上色任务和超分任务分别提出了金字塔阀控交叉注意力模块和连续像素映射模块，其中金字塔阀控交叉注意力模块由多尺度的阀控交叉注意力模块构成，本小节进行了定量实验来评估每个组成模块的有效性。具体地，为了与提出的PVCAttn 进行公平比较，本研究构建了一个基础交叉注意力模块（BasicCross-Attention，BCAttn），其删除了金字塔结构和阀控模块。 表3.6 显示了不同组件组合下的模型效果，当提出的两个组件都使用时提出的方法获得了最好的评估分数。
BaselineBCAttnPVCAttnCPMFID ↓CN ↑PSNR ↑SSIM ↑17.5414.76325.5170.88716.6354.76726.1730.89615.3344.86326.6190.9079.9785.05926.7960.9059.6325.28827.6940.9233.6.7解释性实验CPM 效率对比实验本研究进一步对CPM 模块性能进行分析，在广泛使用的Set14、BSD100 和DIV2K 数据集上与RDN 和Meta-SR 进行了对比实验。
类似Meta-SR 实验设置，本研究将RDN 的超分辨率输出头替换为CPM 模块，并在X2、X3 和X4设置上进行实验。表3.7 显示了定量实验结果，尽管CPM 模块是为连续放大倍率而设计的，但其仍然获得了相当具有竞争力的结果。值得注意的是，使用了CPM 模块的方法只需要存储一个统一的模型，而不是为每个放大倍率存储一个模型，能够大大提高深度模型的实用性。本研究也与当下唯一考虑连续倍率生成的方法Meta-SR 对比了效率，所提的CPM 具有更少的参数量（0.45M 降低到了0.35M）和更快的运行速度（92FPS 提高到了178FPS）。
40浙江大学博士学位论文3局部结构感知的图像增强方法指标Set14B100DIV2KX2X3X4X2X3X4X2X3X4BicubicPSNR ↑30.24027.55026.00029.56027.21025.96031.35028.49026.920SSIM ↑0.86880.77420.72270.84310.73850.66750.90760.83390.7774RDN PSNR ↑34.01030.57028.81032.34029.26027.72035.17031.39029.340SSIM ↑0.92120.84680.78710.90170.80930.74190.94830.89310.8446Meta-RDN PSNR ↑34.04030.55028.84032.35029.30027.75035.18031.42029.360SSIM ↑0.92130.84660.78720.90190.80960.74230.94840.89350.8448CPM-RDNPSNR ↑34.00030.61028.91032.35029.31027.77035.18031.45029.380(Ours)SSIM ↑0.92070.84720.78800.90200.81010.74230.94860.89300.8428PVCAttn 模块可视化解释本研究在设计PVCAttn 结构时考虑了可解释性的因素，采用的交叉注意力结构能够在一定程度上解释源图像与参考图像不同位置之间的相关性。
图3.9 可视化了该模块中的注意力映射图，考虑到展示空间的限制仅选取源图像中的几个图3.9PVCAttn 可视化解释图点进行定性说明，图中左侧显示了两个不同位置（远景天空和近景草地）的注意力图，右侧显示城堡区域的的金字塔层级注意力图。对结果分析可得到如下两点结论：
1. PVCAttn 可以很好地匹配原图中的区域到参考图中具有相似语义信息的区域，即可以很好地将参考图像中的信息转移到源图中，具有较强的可控性和解释性。
2. 低尺度特征图更关注参考图的平均区域，而不同尺度特征关注的差异性侧面表明了金字塔结构带来的互补性优势。
413局部结构感知的图像增强浙江大学博士学位论文连续放大率生成得益于连续像素映射模块的设计，SCSNet 可以生成连续放大倍率的目标图像。为了验证方法的有效性，本小节可视化了从1 倍到4 倍的连续生成过程。
如图3.10 所示，本研究方法生成的结果在不同分辨率中具有一致的颜色稳定性，且相邻分辨率图像过渡平滑，符合模块设计预期。
图3.10连续分辨率生成结果图3.7本章小结本章针对图像像素级研究中的局部结构感知的图像增强问题进行了讨论，从实际应用角度出发，对典型的联合图像上色超分任务进行了深入研究。当前的自动化解决方案包含图像上色、图像超分辨率和目标分辨率感知的下采样三个串接步骤，不仅流程繁琐效率低，而且输出的结果会受到每一个离散的子模块影响。针对这一具体问题，本章对高效的端到端模型设计进行了深入研究，针对图像上色和图像超分两个子任务当前的难点进行了分析并提出了解决方案，具有非常高的实用价值。具体地，本章节提出了一个先进的端到端SCSNet 模型执行联合图像上色和超分辨率任务，相比于主流方法不仅在模型精度上得到了较显著提升，同时拥有更少的参数量和更快的运行速度。为了实现这一最先进的模型，本章设计了一个即插即用的金字塔阀控交叉注意力模块，使得模型能够同时支持自动上色和参考上色两种模式。该模块不仅能够更好地聚合参考图像的颜色信息，获得更好的上色结果，同时具有很好的解释性。此外，本章针对超分任务设计了连续像素映射模块，支持在连续空间中实现任意倍率图像放大，相比于对比方法拥有极具竞争力的效果和更少的计算量，具有很大的实用价值。最后，大量定性和定量的实验验证了所提方法的先进42浙江大学博士学位论文3局部结构感知的图像增强性和实用性，人工实验说明了SCSNet 能够生成更符合人眼感知的真实图像，消融实验和解释性实验则进一步证明了每个子模块的有效性。
434多条件受限的图像纹理语义迁移多条件受限的图像纹理语义迁移关注语义级的图像纹理理解及高清生成问题，本章节从图像语义迁移出发对典型的人脸换脸任务进行研究，提出了基于区域注意力感知的人脸换脸方法。作为高层次的条件式视觉内容生成任务，其旨在将源图像中人脸身份迁移到目标图像中的人脸上，同时保持目标图像中与身份无关的属性不变，在影视制作和安全防御对抗等领域具有极其重要的应用价值。不同于着重关注局部特征的低层次图像生成任务，本章研究的换脸任务需要在语义层面对人脸身份进行理解，确保生成人脸的身份一致性，然而当前工作的效果仍差强人意。本章以换脸身份一致性和高质量人脸生成研究为出发点，针对人脸身份建模方法进行了深入研究，提出了全新的基于人脸区域感知的建模方式（Region-Aware Face Swapping，RAFSwap），合理引入的Transformer 结构更有效地实现了身份信息的提取和迁移。具体地，RAFSwap 在特征提取与交互阶段采用局部加全局的两分支建模方式，并设计了无监督人脸掩膜预测器以区别人脸前景与背景信息，保证更好身份一致性的前提下实现了和谐的高分辨率换脸生成。特别地，局部分支基于注意力机制和人脸解析模型进行构建，包含区域感知的身份编码器、基于注意力机制的特征交互模块和局部感知的身份投影器三个子模块。多个公开数据集上的定性和定量实验表明，提出的方法相较于最先进方法能够生成身份更一致的高分辨率换脸图像。
4.1引言多条件受限的图像纹理语义迁移研究致力于在更高级的语义层面对语义信息迁移进行建模，同时需要在任务相关的多个条件约束下进行受限的高清图像生成研究，相比像素级别的条件式视觉内容生成任务具有更高的挑战性，包含人脸换脸[54–59] 和妆容迁移[68–70]等具体任务，最近两年在泛娱乐和虚拟人等领域具有非常高的需求和应用价值。本章以典型的人脸换脸任务为研究对象，对人脸身份语义的迁移问题展开相关研究。
局部结构感知的图像增强研究致力于解决基础的条件式视觉内容生成问题，一般意义而言，其仅使用局部建模方式即可满足图像分布预测任务需求，不涉及较复杂的图像语义理解，包含图像上色[13–15]、图像超分辨等[19,23,24]、图像去噪[25–27]、图像风格化[39–41] 和图像和谐化[42–44] 等具体任务，在影视制作和图像增强等领域具有非常大的应用潜力。本章以典型的图像上色和图像超分辨率任务为例展开相关研究，前者在色彩分布空间上建模454多条件受限的图像纹理语义迁移浙江大学博士学位论文灰度图到彩色图的翻译过程，后者针对局部纹理分布建模低分辨率图像到高分辨率图像的翻译过程。
人像换脸也称人脸融合，旨在将源图像中的人脸身份信息迁移到目标图像中的人脸上，同时保持目标图像中背景、头发等与身份无关的属性不变，在以短视频为主的泛娱乐行业应用、影视制作、人脸安全、政治武器等领域都有极大的应用需求与潜在价值。得益于深度学习理论与应用的快速发展，以及数据集的丰富和硬件算力的提高，人像换脸技术最近几年发展迅速，许多研究者针对人像换脸任务进行了深入研究并取得了重大进展[57,58,243]，相较于传统方法在效率和效果上都具有显著的优势。
主流方法均采用包含编码器的逆映射方案[130,131]，即换脸模型包含一个用于提取图像特征的编码器，以及一个以StyleGAN 为核心的高分辨率图像生成器，训练的目标是在潜在向量空间找到合适的换脸人脸表征向量，经预训练的生成器得到换脸图像。最近一些研究者基于该框架采用多种思路对方法进行改进，能够获得视觉感官上身份一致性更强的目标图像，但对于一些精度要求较高的应用场景结果仍差强人意。本小节对当前换脸方法和研究难点进行深入分析，总结出影响模型效果的两个持续待解决的关键问题：
1. 如何保证生成人脸的身份与源人脸身份的一致性，包括局部和全局的面部属性和细节。目前几乎所有的主流方法[58,59] 仅在全局特征表示层面进行源人脸和目标人脸的信息交互，而忽略了局部区域的身份相关性建模，例如嘴唇、鼻子、眉毛和眼睛，这将限制模型对身份一致性的表达能力。
2. 如何在基于生成对抗网络（Generative Adversarial Network，GAN）的逆映射框架下生成高分辨率换脸图像，同时保持生成人脸图像与目标人脸图像中与身份无关的细节一致，比如遮挡和背景。
最近工作[57,243] 利用StyleGAN2 作为强大的解码器，虽然能够提升图像生成质量，但未能很好地保持目标面部与身份无关的属性的一致性。
MegaFS 采取一种折中的后处理解决方法，即利用预训练的分割模型得到人脸的硬掩码，然后将生成图像与目标图像以掩码为条件进行加权混合，但这种方法容易在生成图像的边缘产生伪影且需要额外的计算量。
本研究致力于解决上述两个问题：在特征编码交互阶段使用局部五官建模方式对人脸进行更细粒度的建模，同时在图像生成阶段设计了无监督人脸软掩膜预测方法以解决遮挡和背景不一致问题。
最近的一些工作[58,59,189,244,245] 将人脸换脸视为一种特殊的风格迁移任务，输入的源图像经编码器处理后得到向量表征，然后采用全局的AdaIN 操作将源人脸的身份信息传输到目标人脸。然而，一般由人脸识别网络产生的身份向量自然情况不能很好的解耦身份46浙江大学博士学位论文4多条件受限的图像纹理语义迁移图4.1与主流换脸方法在不同困难场景的效果对比信息，不可避免地会包含一些与源人脸身份无关的其他冗余信息，例如背景、发型和场景中光线分布等。这些干扰信息也会一并通过AdaIN 以全局方式注入到目标特征中，使得生成图像的非人脸区域会发生变化，导致人脸身份一致性受到挑战。图4.1 左半部分展示了一些相关主流方法在源人脸和参考人脸具有较大属性差异情况下的换脸结果，例如第一行的源人脸具有非常厚的刘海，第三行源人脸的白色头发与目标人脸的黑色头发差异较大。结果显示基于AdaIN 的主流方法不能很好地处理这一问题，生成的人脸图像在一些具有挑战性的测试样例中存在明显的伪影，即包含过多源图像中非人脸身份的信息。这一现象激发了本研究从特征交互层面的思考：既然全局特征本身存在信息干扰的问题，采用额外身份相关的局部特征是否能更全面地建模身份迁移？基于此，本研究提出了全新的双分支结构从细粒度和粗粒度两层面对特征交互进行更精细的建模，其中基于面部区域感知的局部分支（Facial Region-Aware branch，FRA）从人脸五官角度显性地对源人脸和目标人脸进行身份信息交互，即嘴唇、鼻子、眉毛和眼睛；而源特征适应的全局分支（Source Feature-Adaptive branch，SFA）则用来补充源人脸中非五官区域的全局线索，比如皮肤皱纹和五官相对位置信息。具体地，局部分支采用编码-特征提取-解码的设计思路，受启发于最近被证明建模能力很强的Transformer[115,248–251] 结构设计了3 个子模块，即区域感知的身份编码器（Region-Aware Identity Tokenizer，RAT）、基于注意力机制的特征交互模块和局部感知的身份投影器（Region-Aware Identity Projector，RAP），同时在多尺度上进行了扩展以进一步提升模型效果。经过上述精心的设计，提出的模型相较于主流方法能够更好地应对第一个挑战，在换脸图像质量和身份一致性保持上具有非常明显的优势。
如图4.1 第四列所示，提出的方法具有更少的伪影存在，明显优于最先进的FaceShifter和SimSwap。
主流换脸方法[57,243] 大多采用基于StyleGAN 的逆映射框架[130,131] 以满足高分辨率人474多条件受限的图像纹理语义迁移浙江大学博士学位论文脸生成的实际应用需求。然而，这些方法无法保证换脸图像中与身份无关的区域（如遮挡和背景）与目标图像的一致性，该致命问题产生的原因是基于向量的条件渐进式生成是一个共享参数生成的过程，一些本不该发生变化的区域不可避免地会因共享权重机制存在生成误差。为了缓解这一问题，最近的MegaFS 通过预先提取的人脸掩膜对生成的高分辨率换脸图像和目标人脸进行混合，而HifiFace 则通过有监督的方式训练一个额外的人脸掩膜预测网络，但这类方法由于采用了硬掩膜对换脸图像进行后处理，会导致最终图像边缘产生伪影且需要额外的计算量，同时也必须依赖于标注的人脸掩膜真值，这限制了上述方法的实际应用价值。本章针对这一挑战进行研究，发现预训练的StyleGAN2 本身包含了丰富的面部语义表征，受启发于此，本研究设计了一个轻量型的人脸掩膜预测模块（Face Mask Predictor，FMP）以无监督的方式预测人脸软掩膜。如图4.1 右侧所示，这样的设计在保持人脸身份一致性的前提下实现了更和谐的高分辨率换脸，相比于主流最先进方法具有更高的实用性。
凭借局部加全局的双分支模型架构设计以及无监督人脸掩膜预测模块，本文提出的换脸方法在多个公开数据集上取得了多项指标的最优结果，较主流方法具有非常明显的实用优势。总结本章研究内容的贡献如下：
1. 本研究提出了一种基于区域注意力感知的换脸方法RAFSwap，其包含一个新颖的面部区域感知的局部分支和源特征适应的全局分支：前者通过引入Transformer 来有效地建模不重合的多尺度面部语义交互，后者通过补充全局身份相关的线索来进一步保证生成图像的身份一致性。
2. 本研究针对局部分支设计了区域感知的身份编码器、基于注意力机制的特征交互模块和局部感知的身份投影器三个子模块，并在多尺度上进行了扩展，该细粒度建模方式能够极大地提升身份迁移效果。
3. 为了解决当前的逆映射框架会受到物体遮挡和背景扭曲的影响，本研究提出了一种无监督人脸软掩膜预测模块，提升模型在实际应用时的准确性与泛化性。
4. 大量的定性和定量实验证明所提方法相较于最先进对比方法的优越性，生成的图像在保证高保真度的基础上获得了更好的身份一致性表现，同时在人工评估实验中也更受青睐。
4.2方法概述本章以实际应用需求出发对换脸任务进行了深入研究，针对身份和细节一致性提出了有效的解决方案。具体地，本章基于StyleGAN 的逆映射框架提出了一种新颖的基于区域48浙江大学博士学位论文4多条件受限的图像纹理语义迁移图4.2基于区域注意力感知的换脸框架示意图注意力感知的换脸方法RAFSwap，其通过端到端的训练和推理方式可生成高保真换脸图像。图4.2 展示了研究方法的脉络，源人脸图像Is 和目标人脸图像It 经共享的分层图像编码器ϕE 抽取特征后得到源特征Fs = {F 0s , F 1s , F 2s } 和目标特征Ft = {F 0t , F 1t , F 2t }，上标0、1、2 分别代表小、中、大尺度。同时使用预训练的BiseNet 人脸解析模型提取输入人脸图像的五官语义标签（本文细分为嘴唇、鼻子、眉毛和眼睛共四类）得到源掩膜Ms 和目标掩膜Mt，它们在后续的局部语义建模中用来对特征区域进行选择。蓝色线表示源特征适应的全局分支流程，其通过全局平均池化（Global Average Pooling，GAP）和多层感知机（Multi-Layer Perceptions，MLPs）对源图像特征Fs 进行处理得到全局特征F g；橙色线表示面部区域感知的局部分支流程，其通过串接的区域感知的身份编码器（RAT）、基于注意力机制的特征交互模块（T ）和局部感知的身份投影器（RAP）三个子模块得到局部特征F l。F g 与F l 融合得到分层的换脸特征ˆFt，随后采用类似pSp 的特征映射方式从层次特征图中提取不同细粒度的样式，通过StyleGAN2 生成器控制生成原始的换脸图像ˆIs→t。同时，提出的人脸软掩膜预测模块（FMP）预测表征人脸身份区域的软掩膜M，用来混合目标图像和换脸图像得到最终的换脸输出图像ˆIout。特别地，ˆ表示生成的人脸而不是真实的人脸，且所有与向量有关的操作（包括特征图到向量的映射以及对向量的特征提取）均采用512 通道设置。
4.3面部区域感知分支主流方法人脸换脸视为一种特殊的风格迁移任务，一般采用全局的AdaIN 方式将源人脸的身份信息迁移到目标人脸。虽然这种全局建模方式能够取得不错的效果，但对于身份一致性要求较高的应用场景仍差强人意。
本小节针对这一问题提出了额外的面部区域感494多条件受限的图像纹理语义迁移浙江大学博士学位论文知局部分支，其采用编码-特征提取-解码的设计思路在细粒度身份表征层面进行建模，包含区域感知的身份编码器、基于注意力机制的特征交互模块和局部感知的身份投影器三部分。
图4.3区域感知的身份编码器和投影器结构示意图4.3.1区域感知的身份编码器为了在细粒度层面建模换脸图像的身份信息一致性，本研究提出了区域感知的身份编码器RAT 来显示地对面部局部区域进行空间压缩和编码。图4.3 左半部分显示了RAT 模块的具体结构，输入的源特征Fs 和源掩膜Ms 经区域感知的平均值化操作Φ 以及若干线性变换后得到紧凑的局部身份相关的词元Ts ∈RN×L×512，其中N 表示特征图尺度的数量（本文设置为3），L 表示局部区域的数量（本文为4），Φ 遵循SEAN 的操作方式。具体地，双线性插值被用来调整语义标签在不同尺度下的大小以匹配对应源特征图的大小，随后Φ 将每个尺度下每个区域的特征通过平均操作聚合为对应的词元。用公式表达该过程如下：
T ns = Linear(Φ(Fs, M ns )),(4.1)其中n 代表不同尺度下嘴唇、鼻子、眉毛和眼睛在Ms 中的索引。
4.3.2基于注意力机制的特征交互模块得益于前述的区域感知的身份编码器，不同尺度下的五官局部特征被压缩成了对应的向量词元Ts，为了进一步在向量空间进行特征融合和增强，本研究引入注意力机制来建模不同尺度下多语义词元之间的交互。
具体地，模块T 包含若干层Transformer 结构，每层Transformer 由多头自注意力机制（Multi-head Self-Attention，MSA）、前馈层（Feed-ForwardNetwork，FFN），层归一化（Layer Normalization，LN）和残差连接（Residual Connection）构成，具体结构介绍如下：
MSA 等价于几个自注意力操作的融合，这些操作共同关注来自不同表示子空间的信50浙江大学博士学位论文4多条件受限的图像纹理语义迁移息，表示为：
MultiHead(Q, K, V ) = Concat ( Head 1, Head 2, . . . , Head h) W O,here Head i = AttentionQW Qi , KW Ki , V W Vi= softmax"QW Qi KW KiT√dk#V W Vi= AV W Vi ,(4.2)其中Q，K 和V 代表输入的Query、Key 和Value 数据；W Qi ∈Rdm×dk, W Ki∈Rdm×dk, W Vi∈Rdm×dv, W O ∈Rhdv×dm 为参数矩阵；dm 为输入维度，而dk 和dv 为每个投影子空间的隐藏维度；h 为SA 并行分支的个数，A ∈Rl×l 为注意力矩阵，表示所有标记之间的关系；l为序列长度。为了使得该结构适配本研究，首先应用整形操作将Ts 的N 和L 维度融合得到标准序列格式，即Ts ∈RNL×512，同时将Q，K 和V 均替换为Ts，得到：
Head i = softmax"TsW Qi TsW KiT√dk#TsW Vi= ATsW Vi .
(4.3)FFN 由两个级联线性变换层组成，中间穿插一个ReLU 激活函数：
FFN(x) = max (0, xW1 + b1) W2 + b2,(4.4)其中x 是输入词元，W1 和W2 是两个线性层的权重，b1 和b2 是对应的偏差。局部身份相关的词元Ts 到特征丰富后的词元ˆTs 的转换过程可表示为：
ˆTs = Ts + [MSA|FFN](LN(Ts)).
(4.5)至此，每个词元经T 模块进行特征增强和交互后得到更富有多尺度和多语义的表征。
4.3.3局部感知的身份投影器前述两个模块分别对源图像的局部区域特征进行了词元编码和增强，进一步则需要考虑如何将与身份相关的源词元编码投射到目标特征的空间分布上，同时处理源人脸和目标人脸之间某些属性的非对齐情况，比如头部姿态和面部表情等不一致的因素。最近的SEAN 提出一种特征替换操作，直接将目标人脸待编辑区域简单地替换为源图像样式，这种方式虽然能够实现上述功能，但效果却不令人满意。为了更好地实现上述目标，本研究设计了一个与前述的局部感知的身份映射器对应的局部感知的身份投影器RAP，其通过相关性计算自适应地将身份信息更新到目标人脸特征中。
图4.3 右半部分显示了RAP 模块的具体结构，输入的目标特征Ft 和目标掩膜Mt 通过点乘计算得到人脸相关的掩码特514多条件受限的图像纹理语义迁移浙江大学博士学位论文图4.4人脸掩膜预测模块结构示意图征图，每一个位置的掩码特征与源图像的词元ˆTs 计算适应度并加权，得到的新特征通过点加的方式对目标特征进行细化更新，得到基于局部身份增强的输出特征F l。具体来说，F mt首先沿高度和宽度维度展平得到F mt∈RHW×512，以其每个尺度的每个位置特征作为Query，以ˆTs 作为Key 和Value，通过交叉注意力机制计算得到注意力矩阵A ∈RHW×NL（参考公式4.2 ），其中Aij 表示目标图像中第i 个位置与源图像中第j 个词元之间的关系。
该过程用公式表示如下：
F l = Ft + RS(A ˆTsW ),(4.6)其中W 为可学习的权重，RS 表示特征图重新排列操作。
4.4源特征适应分支面部区域感知分支可以将源人脸的身份相关特征通过局部注意力建模的方式注入到目标人脸中，而忽略了影响换脸身份一致性的全局面部表征，例如皮肤皱纹、五官的相对距离等。基于此，本小节设计了一个全局的源特征适应分支，其捕捉全局信息作为补充线索来区分不同的身份。如图4.2 蓝色线信息流所示，为了避免源人脸与目标人脸的空间不匹配，尺寸最小的源人脸特征图经串行的全局平均池化（GAP）和多层感知机操作（MLPs）进行特征提取和自适应重组，得到全局特征F g。随后，全局特征F g 广播后与局部特征F l 进行点加操作输出换脸特征ˆFt。公式表示如下：
F g = MLPs(GAP(F 0s )),ˆFt = F g + F l.
(4.7)52浙江大学博士学位论文4多条件受限的图像纹理语义迁移4.5人脸掩膜预测模块基于StyleGAN 的逆映射框架可以满足高分辨率人脸生成的实际应用需求，然而，由于基于向量的条件渐进式生成是一个共享参数生成的过程，导致生成图像中一些本该不发生变化的区域会存在误差，表现为模型不能很好地处理遮挡和背景扭曲问题。一个直观的解决办法是使用人脸掩膜对生成图像和目标图像进行混合，以排除错误生成的区域，而这会涉及到对掩膜获取方式和精度的研究。MegaFS 利用预训练的分割模型产生硬掩膜，其往往会在边缘产生伪影且需要额外较大的计算量。受Labels4Free 启发，本研究通过对中间特征图进行可视化发现StyleGAN2 不同层的特征已包含丰富的人脸语义先验，因此基于其多层特征图作为输入，设计了一个平行的无监督人脸掩膜预测模块（FMP）来得到换脸图像中人脸的软掩膜。图4.4 展示了FMP 的结构示意图，16-256 分辨率的特征图被选择作为输入，每层特征经不共享的BottleNeck 模块进行特征提取并将通道降低到32，不同层的输出特征缩放到256 分辨率并进行拼接，再经1 × 1 卷积层和Sigmoid 层得到目标单通道软掩膜M，其混合目标图像It 和换脸图像ˆIs→t 得到最终的换脸输出图像ˆIout：
ˆIout = M ⊙ˆIs→t + (1 −M) ⊙It.
(4.8)4.6总体目标函数给定源图像Is 和目标图像It，本文提出的RAFSwap 在训练阶段采用三种训练损失函数：身份损失Lid、重建损失Lrec 和感知损失Lp。
身份损失经过良好训练的人脸识别模型可以提供具有鉴别性的身份嵌入，为了保持人脸身份的一致，本小节使用余弦相似度来估计生成的人脸图像与源人脸图像的的身份嵌入表征之间的相似度:
Lid = 1 −cos(R(Is), R( ˆIout)),(4.9)其中R(·) 为预先训练的ArcFace 人脸识别网络。
重建损失如果源人脸和目标人脸来自相同的身份，则生成的人脸图像应该与目标人脸图像一致。此项损失函数在像素层面计算生成的换脸图像和目标图像之间的差异：
Lrec = ˆIout −It2if It = Is0其他.
(4.10)534多条件受限的图像纹理语义迁移浙江大学博士学位论文感知损失除了在像素级评估两张人脸图像之间的差异，训练时还利用LPIPS 度量 在特征图层面计算生成的换脸图像和目标图像之间的语义差异，以保证语义细节上的一致性。该损失函数表示为：
Lp =ϕp( ˆIout) −ϕp(It)2 ,(4.11)其中ϕp(·) 表示预训练的VGG16 网络。
总的训练损失Ltotal 为上述损失函数的加权平均：
Ltotal = λidLid + λrecLrec + λpLp,(4.12)其中λid = 0.15, λrec = 1.0 和λp = 0.8 用来平衡不同损失函数的权重。
4.7实验结果4.7.1数据集介绍本文基于广泛使用的CelebA-HQ 和FaceForensics++ 数据集进行换脸实验。评测之前，本章先对上述两个数据集进行简单介绍：
CelebA-HQCelebA 总计包含来自10,177 个名人身份的202,599 张人脸图片，每张图片都包含人脸位置标注框、5 点人脸关键点坐标以及40 个属性标记。
CelebA-HQ 是CelebA的一个高质量版本，其拥有3 万张1,024 分辨率图像，常用于高清人脸生成和换脸等与人脸相关的计算机视觉任务中。
FaceForensics++一个高质量的面部伪造数据集，目的是让研究人员能够以监督的方式训练基于深度学习的方法。作者构建数据集时为了模拟更真实的场景，广泛地收集了1,000 个主要来自YouTube 的源视频，共包含509,914 张原始图像。这些视频均经手动筛选以保证视频的高质量，且每个视频在选取时规避人脸角度过大和面部遮挡的情况。最终选择两种基于计算机图形学的方法（Face2Face 和FaceSwap）和两种基于学习的方法（DeepFakes 和NeuralTextures）生成伪造图像，这四种方法都需要源和目标演员视频对作为输入，最终输出是由生成图像组成的视频。除了伪造输出，作者还计算真值掩码来表明像素是否被修改，可以用来训练伪造定位模型。
4.7.2实验设置本研究采用CelebA-HQ 作为训练集训练高清换脸模型，使用CelebA-HQ 测试集进行高清换脸测试，使用FaceForensics++ 数据集进行换脸属性评估（由于高清换脸模型生成54浙江大学博士学位论文4多条件受限的图像纹理语义迁移的图像为1,024，需缩放到256 进行测试）。输入的源图像和目标图像分辨率为256，输出的换脸图像分辨率为1,024，训练时源图像和目标图像为同一张图像的概率为0.2。所有代码基于PyTorch 框架实现，训练时预训练的StyleGAN2 权重固定，其它参数使用Adam 优化器（参数β1 = 0.9, β2 = 0.999）进行优化，权重衰减设置为1e−4，学习率设置为1e−4，使用1 块V100 GPU 训练模型，训练批大小为8，共迭代5 万次。
4.7.3评测指标为了更全面地与主流方法进行对比，本研究默认采用身份检索准确性（ID Retrieval）、姿态误差（Pose Error）和表情误差(Expression Error) 三个指标来对模型效果进行评估，对于高分辨率图像评估使用身份相似性（ID Similarity）代替身份检索准确性以和对比方法保持一致。具体地，身份检索准确性和身份相似性使用CosFace 对身份嵌入进行提取，前者利用余弦相似度检索最接近的人脸，并使用标准的重识别检索框架进行结果评估，后者计算换脸图像和源图像之间的余弦相似性；姿态误差和表情误差使用位姿估计器和三维人脸模型 提取图像人脸的位姿和表情向量，并使用L2 距离计算换脸图像和目标人脸之间的指标误差。
三种评测准测试时均将输入图像大小缩放到256 以满足指标评测模型的输入分辨率要求。进一步为了充分评估各种方法的优劣，本研究聘请真人对生成图像的身份一致性感知和图像质量进行评判，同时也对不同方法的参数量和效率进行了定量对比。
4.7.4与主流方法对比为了充分证明所提方法的有效性，本研究在CelebA-HQ 和FaceForensics++ 数据集上做了大量定性和定量实验，并对实验结果进行分析和讨论。
定性对比结果本小节在FaceForensics++ 数据集上定性评估提出的RAFSwap 换脸模型与一些主流方法的对比结果。如图4.5 所示，前两列分别为源图像和目标图像，最后一列显示了本研究方法的结果，其它列为主流方法的结果，每一行代表一个具有挑战性的案例，主要表现为源人脸与目标人脸在一些属性上差异较大，比如人脸形状、发型和眉毛等。结果显示FaceShifter 和SimSwap 生成的换脸图像容易出现伪影导致视觉质量较低，主要表现为不能很好地建模人脸五官区域，导致生成的人脸一致性较差，而MegaFS 能够很好地处理上述挑战但面部肤色和妆容却容易受到改变。可以看到本研究提出的的方法相比于对比方法在人脸身份一致性保持方面有明显的改善，同时具有更好的视觉感官效果。以最难的眼睛语义迁移为例，提出的方法无论是从眼睛形态还是颜色方面都能很好地554多条件受限的图像纹理语义迁移浙江大学博士学位论文图4.5FaceForensics++ 定性对比可视化图4.6泛化性对比可视化保持源图像的属性。
进一步为了测试不同方法的泛化性，本小节在数据集外图像上与FaceShifter 和FaceIn-painter 进行了对比实验，如图4.6 所示。得益于精心设计的双分支结构和软掩模生成模块，提出的RAFSwap 能够更好地保持源身份信息的一致性(如嘴巴和眼睛) 和目标其他属性的一致性（如发色、肤色和背景），并能处理遮挡情况（如眼镜）和背景一致性问题。注意56浙江大学博士学位论文4多条件受限的图像纹理语义迁移该图中的源图像和目标图像来源于对比方法论文，目的是为了更公平地比较。
图4.7CelebA-HQ 定性对比可视化考虑到本文研究方法针对高分辨率人脸进行生成，同时考虑了人脸掩膜预测的设计，这与最先进的MegaFS 具有相同的研究目标，因此本小节将在CelebA-HQ 数据集上进一步与MegaFS 单独进行对比。图4.7 展示了4 组源图像和目标图像存在显著差异（性别、年龄、肤色和头部姿态）的生成结果，可以看到提出的RAFSwap 在源人脸眼睛颜色和皮肤纹理等属性保持上具有明显的优势，同时忠实地尊重目标人脸的属性。最后一列为FMP模块预测的软人脸掩膜，其保证了生成的融合换脸图像在边缘上更和谐。
定量对比结果为了更全面地证明本文提出方法的有效性，本小节在FaceForensics++数据集上与主流方法进行了定量对比，采用身份检索准确性、姿态误差和表情误差3 个指标评估，流程遵循MegaFS 中的实验设置。首先，每个视频（共1,000 个）中随机抽取10帧图像，通过MTCNN 处理得到总共1 万张对齐人脸。由于不同的视频可能会有重复574多条件受限的图像纹理语义迁移浙江大学博士学位论文方法身份检索准确性↑姿态误差↓表情误差↓DeepFakes88.394.463.33FaceShifter90.682.552.82SimSwap89.731.942.39MegaFS90.832.642.96本文方法96.702.532.92身份的存在且在一帧中可能包含多个人脸，故需仔细检查对齐的人脸并最终手动将所有视频分类为885 个身份。然后，基于评估指标对每张图像进行处理：利用CosFace 提取身份嵌入并用余弦相似度检索最接近的人脸，使用位姿估计器和三维人脸模型提取图像的位姿和表情向量以进行位姿和表情L2 误差评估。表4.1 显示了不同方法的定量结果，SimSwap具有较高的头部姿态和表情准确性，表明其能够更好地保留目标人脸的属性，但身份一致性有待提高。受益于双分支更细粒度的建模，本文提出的RAFSwap 方法在身份检索准确性指标上得到了明显的提高（+6.97↑），同时在其他两个指标上获得了非常有竞争力的结果。值得注意的是，FaceInpainter 代码由于尚未公开故此处没有做对比，其他方法结果来自于官方公开的代码以进行公平比较。
方法身份相似性↑姿态误差↓表情误差↓MegaFS0.48373.83.13本文方法0.52323.773.15本小节进一步与最先进的MegaFS 进行了高分辨率换脸对比，具体地，CelebA-HQ 测试集中随机选取10 万对人脸图像作为评估数据。表4.2 显示了高分辨率换脸图像生成的定量评估，提出的RAFSwap 在身份相似性和姿态误差方面比MegaFS 有更好的性能，但在表情准确性上略逊一筹。进一步对这一结果和训练过程进行分析发现MegaFS 训练时采用了人脸关键点损失函数，生成的换脸图像会因该较强的脸型约束而保持与目标人脸一致的嘴型和表情，从而具有更低的表情误差和更差的身份一致性。但这种训练约束需要额外的人脸关键点检测模型，会占用跟多的计算和存储资源而不利于实际的应用。总的来58浙江大学博士学位论文4多条件受限的图像纹理语义迁移说，本文方法在使用更少约束的情况下得到了更好的指标结果，尤其是在身份保持方面具有明显的优越性。
4.7.5人工评估实验方法身份一致性感知↑图像质量↑DeepFakses0.070.05FaceShifter0.160.13SimSwap0.130.09MegaFS0.150.16本文方法0.490.57换脸任务面临的一个潜在问题是上述基于模型的定量指标结果很好，但真实图像的视觉效果可能不是令人满意的。基于此，本小节进行了一项人工评估实验来对比不同方法生成的图像给人真实的感觉如何。具体地，1,000 个FaceForensics++ 视频中随机抽样20 对作为输入，经不同方法进行换脸后显示给每个评估人员，让其选择：1）与源人脸身份最相似且与目标人脸属性最相似的换脸图像；2）图像质量最高的换脸图像。这两项评估针对前述换脸任务中的两个挑战进行设计。表4.3 列出了基于50 个评估人员的结果百分比，表明提出的方法明显优于主流对比方法。
4.7.6消融实验AdaINFRASFAFMP身份检索准确性↑姿态误差↓表情误差↓92.482.602.9896.632.582.9493.682.613.0696.692.542.9496.702.532.92594多条件受限的图像纹理语义迁移浙江大学博士学位论文图4.8网络模块剥离实验定性可视化网络模块对模型影响为了进一步验证FRA 和SFA 双分支结构在基于StyleGAN 逆映射框架的优越性，本小节做了定性和定量的剥离实验。具体来说，本实验实现了一个基于AdaIN 进行特征注入的版本作为基础对比模型，依次添加不同的模块来证明每一个模块的有效性。如图4.8 所示，AdaIN 生成的第三列图像不能很好地保留源人脸的面部身份细节，这照应了前面分析的全局信息控制的弊端，即一些干扰信息也会一并通过AdaIN以全局方式注入到目标特征中，导致人脸身份一致性受到挑战。相比之下，本研究提出的RFA 和SFA 模块能够在一定程度上改善该问题，即任意一个模块的应用都能够带来定性结果的提升（第4 列和第5 列），当同时使用两个模块时模型获得了最好的结果（第6 列），生成的人脸既能很好地保留源人脸的身份特征又能继承目标人脸的其它属性。表4.4 从定量实验的角度获得了一致性的实验结果，表明所提出模块的有效性。
注意力模块对模型影响为了验证Transformer 在基于注意力机制的特征交互模块中捕获词元交互的能力，本小节进行了定量剥离实验。特别地，该实验修改了一个基础的对比版本，即将Transformer 层替换为Non-Local 层。表4.5 展示了定量的评估结果，第3行的Tr-0 代表不使用Transformer 层，第4 行表示一个Transformer 层提高了模型性能，第2 行表示Non-Local 不能很好地建模词元交互，导致轻微的性能下降。  此外，第4-6 行实60浙江大学博士学位论文4多条件受限的图像纹理语义迁移方法身份检索准确性↑姿态误差↓表情误差↓+ Non-Local96.502.633.03+ Tr-096.622.603.01+ Tr-196.702.532.92+ Tr-296.712.512.94+ Tr-396.732.542.90验是为了评估Transformer 层数对结果的影响程度，结果表明随着层数的增加模型性能没有明显的提高，即1 层Transformer 足以建模词元交互。同时考虑到模型性能和计算量之间的权衡，其余实验均采用了1 层带有8 个分支头的Transformer 结构。
图4.9FRA 分支注意力图可视化此外，为了进一步解释FRA 模块中Transforme 结构学习到的特征，本小节以一张源图像为例可视化了其中一个注意力头。图4.9 显示了3 个尺度上4 种语义特征之间的相关性，注意力图表明Transformer 在不同尺度上的注意力集中在不同的语义区域，即大尺度上集中于眼睛、中等尺度上集中在嘴唇、小尺度上集中在鼻子。注意到眉毛不太受重视，分析可能的原因是眉毛占据区域较小，且与眼睛的感受野重叠。特别地，为了进一步提升应用时模型的精度和运行效率，本文对Transformer 模块进行了结构优化，受启发于进化算法中的局部种群概念改进一个基于局部和全局双分支建模的结构，能够在保证模型精度不下降的前提下降低模型的参数量并提升运行速度。
614多条件受限的图像纹理语义迁移浙江大学博士学位论文图4.10FMP 模块定性效果可视化FMP 模块效果探究本小节提供了两个定性案例来说明FMP 模块的作用。如图4.10所示，前两列分别为源图像和目标图像，第三列为不带掩膜的直接输出ˆIs→t，可以看到目标图像的背景和发丝等不应发生变化的区域发生了改变。使用第四列的硬掩膜与目标图像It 融合后得到第五列图像，可以发现这种方式会产生过多的信息和不自然的边缘，尤其是在刘海区域。相比之下，采用FMP 模块预测的软掩模生成的最终换脸图像ˆIout 更加和谐与真实。特别地，表4.4 实验中的最后两行也说明FMP 模块能够带来定性的结果提升。
4.7.7换妆任务迁移及效果分析为了进一步测试RAF 模块的优越性，本小节将即插即用的FRA 分支进一步可应用于换妆任务中。该实验以主流的PSGAN 作为基线，将其中的AMM 模块替换为本文的FRA 分支。图4.11 展示了与LADN 和PSGAN 的定性对比结果，提出的改进方法更精确地转换了彩妆颜色，并且很好地保存了源人脸的身份和光线信息，这得益于FRA 结构灵活的词元机制特征交互机制。
方法CPU 速度GPU 速度参数量浮点运算量(s)↓(ms)↓(M)↓(G)↓LADN8.7026.826.99175.77PSGAN8.45128.912.6191.02本文方法0.2839.313.6071.39此外，本小节对比了不同方法的效率，如表4.6 所示，可以看到提出的方法在CPU 和62浙江大学博士学位论文4多条件受限的图像纹理语义迁移GPU 均有明显的速度优势，同时具有相匹配的参数量和浮点运算量。
图4.11换妆任务定性对比可视化4.7.8更多换脸结果展示图4.12 展示了更多的换脸结果，以证明本文方法在高清生成和身份一致性保持上的有效性。
4.8本章小结本章面向更高层次的视觉生成问题，针对图像语义级研究中的多条件受限的图像纹理语义迁移问题进行了讨论，提出了基于区域注意力感知的人脸换脸方法，从日益增长的高清生成应用需求出发对典型的人脸换脸任务进行了深入研究，总结出影响模型效果的两个持续待解决的关键问题：1）如何保证生成人脸的身份与源人脸身份的一致性，包括局部和全局的面部属性和细节。2）如何在基于GAN 的逆映射框架下生成高分辨率换脸图像，同时保持生成人脸图像与目标人脸图像中与身份无关的细节一致。针对第一个问题，本章提出了全新的双分支结构对特征交互进行更全面的建模，其中基于面部区域感知的局部分支FRA 显性地对源人脸和目标人脸的五官区域进行局部建模，源特征适应的全局分支SFA 则用来补充源人脸中非五官区域的全局线索。具体地，局部分支采用编码-特征提取-解码的设计思路，包含区域感知的身份编码器、基于注意力机制的特征交互模块634多条件受限的图像纹理语义迁移浙江大学博士学位论文图4.12高清换脸结果可视化和局部感知的身份投影器三个子模块。针对第二个问题，本章设计了一个全新的人脸掩膜预测模块FMP，其能够以无监督的方式预测人脸软掩膜，融合后的换脸图像在保持人脸身份一致性的基础上实现了更和谐的高分辨率生成。最后，大量定性和定量实验表明本文方法相较于最先进对比方法具有更高的准确性和泛化性，同时在人工评估实验中也具有更好的视觉效果。
645多条件受限的图像几何语义编辑多条件受限的图像几何语义编辑关注语义级的图像语义和几何信息精细理解及高清生成问题，本章节从图像几何编辑出发对典型的人脸驱动任务进行研究，提出了基于多信息源控制的多人脸驱动方法。作为高层次的条件式视觉内容生成任务，其旨在利用多源驱动信号对给定的源人脸图像进行面部表情或头部姿态控制，在泛娱乐和虚拟人等领域具有极大的应用需求和潜力。不同于局部特征建模的低层次图像生成任务，本章研究的任务需要在更高维度的人脸语义层面进行感知，并理解运动信号以进行可控生成，面临着多人脸驱动且多源信息可控的高保真生成难题。对于前者，本章提出面部纹理和几何信息解耦的人脸表征方式，设计了全新的FReeNet 人脸驱动框架实现单个模型对多张人脸的驱动，具体包含一个人脸关键点转换器在几何空间进行表情迁移，以及一个几何感知生成器以转换的人脸关键点为条件控制生成输出图像，同时提出了三元组感知损失函数以加强解耦过程。对于后者，本章基于相同的基础框架，针对音频输入设计了几何感知融合器对输入信息进行特征提取和融合，并设计了几何控制器模块来高效地在生成器中注入控制信号。多个公开数据集上的实验结果表明，提出的方法很好地实现了基于图像或音频信号控制的多人脸驱动任务，相较于最先进方法能够生成更高质量的人脸驱动图像。
5.1引言多条件受限的图像几何语义编辑研究致力于在解耦的几何空间中对图像中的语义信息进行编辑，至少需要两个条件输入分别提供语义图像和几何控制条件，核心解决如何在图像几何空间进行具体任务相关的建模问题，相较于人脸换脸等仅需在语义纹理空间建模的任务具有更大的挑战性，包含图像人脸驱动[60,61,64]、音频人脸驱动[63,67] 和人体驱动[268,269] 等具体任务，最近两年在泛娱乐和元宇宙等领域具有非常高的应用需求。本章以典型的人脸驱动任务为研究对象，对基于图像和音频信息源控制的多人脸驱动展开讨论，并针对几何空间解耦的人脸语义编辑问题展开深入研究。
人脸驱动旨在利用驱动信号对给定的源人脸图像进行面部表情或头部姿态控制，生成的输出图像在保持源人脸身份与背景一致性的前提下，面部表情或头部姿态符合驱动信号的语义信息，直观的输入输出描述及任务流程如图5.1 所示。随着近些年泛娱乐行业的发展和元宇宙概念的火爆，该项技术逐渐进入大众视野并被应用于多种行业，比如短视频655多条件受限的图像几何语义编辑浙江大学博士学位论文领域、影视行业、人脸安全防御和政治武器防御等，具有非常大的应用潜力和经济安全效益。本文从实际应用需求出发对人脸驱动技术进行深入研究，针对当前方法的一些难点提出改进措施，并对图像和音频两种信息源控制模式提出具体的解决方案。
图5.1人脸驱动任务输入输出及流程近十年来，得益于大规模人脸数据集的发布[190–193] 以及高精度人脸检测算法的提出[194–197,270]，人脸驱动研究取得了极大的进步并有许多优秀的方法被提出，按照实现方式主要分为两大类：基于三维模型的合成方法[158,159,199–201] 和基于深度学习的生成方法[60–65,202,203,271–275]。对于前者，人脸由一个手工制作的预定义参数化三维模型进行表征，首先通过学习或者特定设备捕获的方式获得驱动图像/视频/真实人脸的面部运动，并将其拟合到预定义模型上的参数空间中，然后通过形变渲染等计算机图形学的一些流程得到目标图像/视频[199,276–279]。这类方法发展于计算机图形中的角色动画技术，因具有高质量和高分辨率的人脸驱动生成能力在游戏和电影制作等领域中受到广泛应用。然而，基于3D 模型的方法都面临预定义模型制作需要有经验的3D 建模师的问题，制作成本较高周期较长，且串行的步骤较繁琐，对计算量需求较高。对于后者，受益于最近几年硬件算力的提高和深度学习模型训练稳定性的研究深入，基于生成对抗网络的方法取得了明显的进步，因该类方法在利用大规模数据集学习分布模式方面具有天然优势，且端到端的学习模型一般也具有更快的运行速度，具有很高的实际应用价值。这类方法一般采用编码器对输入图像和控制信号进行特征提取和融合，以及解码器用来生成输出图像，并辅以对抗训练的思想以保证生成高质量的人脸驱动图像。进一步的工作[49,60,203,204] 通过引入循环一致损失实现了非成对人脸的训练，极大地降低了对数据的要求。然而，上述方法面临着一旦网络训练完毕后只能对特定身份进行驱动，这极大地限制了该技术的应用价值。随后ReenactGAN 提出了多对一人脸驱动来解决上述问题，引入了转换器模块在潜在的人脸边界空间中将不同目标人员的人脸运动适应到源人脸上。尽管如此，该方法在实际应用中仍然是低效的，因为每个源人脸身份都需要独有的转换器和解码器。因此，X2Face 设计了嵌入网络模型对目标人脸进行运动编码，然后利用驱动网络对源人脸进行控制生成输出人脸，成功地实现一个模型进行多对多人脸驱动，即源人脸和驱动信息均支持多人输66浙江大学博士学位论文5多条件受限的图像几何语义编辑入。这对人脸驱动技术的落地推动具有重要意义，但生成的图像在质量和面部细节方面仍有不足。总而言之，基于图像的人脸驱动仍面临两个主要挑战:
1. 如何通过统一的模型实现多人脸面部驱动。单模型应具备通用人脸的身份理解能力，同时由于源人脸与目标人脸的面部轮廓及五官分布存在差异，建模时需精心设计特征提取及融合的方式。
2. 如何保持输出人脸与源人脸的身份一致性。源人脸图像会面临着色调、光照等外在因素干扰，模型训练时需充分理解图像中影响身份信息的元素。
为了解决上述问题，本章基于纹理和几何信息解耦的思想设计了全新的多人脸驱动框架（Multi-identity Face Reenactment，FReeNet），仅需一个统一模型即可实现用任意目标人脸驱动多个源人脸。具体地，该模型包含人脸关键点转换器和几何感知生成器两个分支。
对于前者，首先利用人脸关键点提取人脸的几何表征信息而忽略无关的外观信息，然后利用人脸关键点转换器在该空间实现目标人脸表情到源人脸的迁移。对于后者，其提取源人脸的外观纹理特征，在前述几何信息约束下生成符合相应面部运动信息的输出图像。然而训练时发现在当人脸身份过少时生成器倾向于学习从几何信息到输出的直接映射，而忽略了源图像的纹理特征。为了缓解这一问题，本研究基于三元组损失 和感知损失设计了三元组感知损失函数，以加强生成器的解耦过程并提高可控生成能力。
另一方面，随着动画制作、虚拟人和游戏等领域的发展，催生了基于音频控制的人脸驱动应用需求。类似地，目前大多数相关方法只能针对特定人脸建模或存在输出图像质量较低的问题[164,208,209,211]，且几乎没有工作考虑模型尺寸和运行速度这两个对实际应用很重要的因素。以前述基于图像的驱动研究作为基础，本文适配了音频输入的人脸驱动研究[283,284]，将基于图像输入的人脸关键点转换器更新为基于音频输入的音频特征融合器，实现了基于音频信号的多人脸驱动模型设计。具体地，本研究提出了一个高质量的AnnVI数据集以支撑高分辨率的图像生成，设计了一种新型的几何控制器模块以更有效地注入驱动信息，实现端到端的训练和推理过程。进一步缓解当前方法运行速度较慢的问题，本研究也对几何感知生成器架构进行了轻量化设计以支持实时的GPU 和CPU 部署能力。
凭借解耦设计的思想，本章提出的FReeNet 框架首次实现了使用统一模型进行多人脸驱动任务，支持图像和音频两种信息源控制，同时多个公开数据上与主流方法的对比实验证明了所提方法的有效性和高效性。总结本章研究内容的贡献如下：
1. 本研究提出了一种基于人脸几何和纹理信息解耦思想设计的多人脸驱动模型FReeNet，包含一个精心设计的人脸关键点转换器分支以在几何空间上进行面部运动迁移，以及几何感知生成器分支生成人脸驱动图像。
675多条件受限的图像几何语义编辑浙江大学博士学位论文2. 本研究基于三元组损失和感知损失设计了三元组感知损失函数，训练时加强了模型的解耦学习能力。
3. 本研究设计了音频特征融合器和几何控制器模块，基于FReeNet 框架实现了基于音频输入的多人脸驱动模型，同时提出了一个高质量的AnnVI 数据集以支持高分辨率的图像生成研究。
4. 广泛的定性和定量实验表明提出的方法很好地实现了多信息源控制的多人脸驱动任务，相比于主流方法在多个公开数据集上获得了更高的指标，人工评估实验也证明了所提方法生成的图像具有更高的质量和真实性。
5.2方法概述本文以日渐增长的实际应用需求出发对人脸驱动任务进行了深入研究，针对图像和音频多信息源控制下的多人脸驱动提出了有效的解决方案。
5.3基于人脸图像控制的多人脸驱动本小节对基于人脸图像控制的多人脸驱动任务（具体为表情驱动，也称表情迁移）进行了深入研究，针对人脸纹理和几何信息解耦提出了有效的设计方法。本研究基于广泛使用的Pix2Pix 作为基础图像生成框架，提出了一种新颖的多人脸驱动模型FReeNet，其通过对抗训练的方式实现了高质量驱动图像的生成与解耦控制。图5.2 展示了研究方法的流程示意图，对于给定的源人脸图像IS,r ∈R3×256×256 和目标人脸图像IT,n ∈R3×256×256，首先采用人脸关键点检测器 提取对应几何空间的关键点表征向量lS,r ∈R106×2 和lT,n ∈R106×2，其中第一个下标代表身份，第二个下标的r 和n 分别代表参考表情和任意其他表情。人脸关键点转换器在几何空间上将目标人脸的表情迁移到源人脸上，过程表示为：ψ : (lS,r, lT,n) →ˆlS,n。然后，几何空间表征的人脸关键点向量lT,n 首先转为图像表征ˆLS,n ∈R1×64×64，其与提供纹理的源图像IS,r 经由几何感知生成器ϕ 得到输出人脸驱动图像ˆIO,n ∈R3×256×256，过程表示为：ϕ : ( ˆLS,n, IS,r) →ˆIO,n。其中ˆLS,n 由lT,n 在二维图像空间中绘制而成。
5.3.1人脸关键点转换器进行人脸表情迁移时除了保证面部运动的正确控制，同时也要保证输出人脸与源人脸其它属性的一致性，比如面部轮廓。这点在ReenactGAN 也有所提到：直接应用不合适的面部轮廓来合成目标图像可能会产生伪影，因此面部运动迁移模块的设计和训练事关68浙江大学博士学位论文5多条件受限的图像几何语义编辑图5.2基于图像控制的多人脸驱动框架示意图重要。具体地，本研究针对多人脸迁移的任务目标设计了人脸关键点转换器ψ（图5.2 上半部分所示），仅使用一个模型即可适应任意目标人脸的表情到参考人脸上，同时能够保持源人脸在几何空间的身份一致性。具体地，人脸关键点转换器包含两个人脸关键点编码器ψα1 和ψα2 分别对源人脸关键点lS,r 和目标人脸关键点lT,n 提取特征，经拼接和解码器ψα3 预测相对于lS,r 的人脸关键点位移lshift，与lS,r 点加后得到输出ˆlS,n，其与IT,n 具有一致性的表情，同时在几何空间保留了IS,r 的身份信息。该过程表示为:
ˆlS,n =lS,r + lshift=lS,r + ψα3(ψα1(lS,r), ψα2(lT,n)).
(5.1)5.3.2几何感知生成器前述的人脸关键点转换器在向量空间进行多人脸表情迁移得到ˆlS,n，绘制图像后获得人脸几何表征ˆLS,n。本小节基于特征编码-融合-解码的思路设计了几何感知生成器ϕ（图5.2 下半部分所示），其以源人脸IS,r 作为纹理信号的输入，以ˆlS,n 作为几何控制信号对源人脸进行驱动。具体地，源人脸图像IS,r 和驱动信号ˆlS,n 分别经由ϕθ1 和ϕθ2 进行特征编码，通过特征融合器ϕθ3 对纹理和几何信息加工，最后经由解码器ϕθ4 生成输出驱动图像ˆIO,n。
其中ϕθ3 由3 组残差模块构成，每组后接一个特征拼接操作用来融合条件信息，这样的多路径信息注入方式确保了几何信息的有效注入。该过程描述为:
695多条件受限的图像几何语义编辑浙江大学博士学位论文ˆIO,n = ϕ(IS,r, ˆLS,n)= ϕθ4(ϕθ3(ϕθ1(IS,r), ϕθ2( ˆLS,n))).
(5.2)如此结构设计的几何感知生成器能够高效地在多人之间进行表情驱动生成，相比于仅能对单人建模的方法具有明显的优越性与实用性。
图5.3三元组感知损失示意图5.3.3三元组感知损失训练阶段发现几何感知生成器在L1 和对抗损失的监督下会倾向于学习条件输入的人脸关键点驱动信号LS,n 到驱动图像ˆIO,n 的直接映射，而忽略源图像IS,r 的纹理特征，尤其是当训练集包含的身份过少时，这对实际应用是非常不利的。通过对模型学习过程分析发现这个问题产生的原因是由于IS,r 和ˆLS,n 分布差异过大所导致：前者包含丰富的纹理特征而后者只有关键点的可视化，导致生成器倾向于从简单的ˆLS,n 直接学习输出图像的映射，即ϕθ1 和ϕθ3 的大部分层不能很好地进行学习过程。为了解决上述问题，本研究结合三元组损失 和感知损失 设计了三元组感知损失函数（Triplet Perceptual，TP），训练时增加类间的感知距离的同时降低类内感知距离，迫使模型从源图像IS,r 中提取有效的特征。图5.3 为TP 损失函数的定性说明，一组训练数据包含2 张属于S 身份的图像IS,n1和IS,n2（n1 和n2 为S 身份人脸中随机选取的2 个表情），1 张属于T 身份的图像IT,n3（n3 为T 身份人脸中随机选取的1 个表情），以及3 张对应的的表情控制信息ˆlS,n2，ˆlS,n3 和ˆlT,n2，经由几何感知生成器得到3 张对应的输出驱动图像ˆIS,n2，ˆIS,n3 和ˆIT,n2。随后，使用预训练的VGG16 模型对驱动图像提取特征，在图像高层感知层面增加不同身份特征间的距离，同时降低相同身份间的特征距离，公式表达如下：
70浙江大学博士学位论文5多条件受限的图像几何语义编辑LTP( ˆIT,n2, ˆIT,n3, ˆIR,n2) =hm + Dκ( ˆIT,n2), κ( ˆIT,n3)−Dκ( ˆIT,n2), κ( ˆIR,n2) i+,(5.3)其中m 为控制类间和类内距离的超参（本研究设置为0.3），κ(·) 表示VGG 特征提取操作，D(·, ·) 表示L2 距离，+ 表示输出值为正。训练时任意一个表情均会参与到所有身份的驱动过程中，在TP 训练损失的加持下，模型被迫从源人脸中学习纹理信息以符合类间距离增大的约束。
5.3.4总体目标函数基于人脸图像控制的多人脸驱动模型训练时对两个子模块分阶段单独训练，即人脸关键点转换器和几何感知生成器分别在几何空间和图像纹理空间做损失约束。
人脸关键点转换器损失训练阶段总的损失函数LAll 定义为:
LAll = λPointLPoint + λCycleLCycle + λAdvLAdv,(5.4)其中λi 为第i 项损失函数的权重，本文训练时设置λPoint = 100、λCycle = 10 和λAdv = 0.1。
1）点级别损失。使用L1 损失计算生成的人脸关键点与真值的坐标误差：
LPoint = ||ˆlS,n −lT,n||1.
(5.5)2）循环一致性损失。
使用生成的ˆlS,n 再一次作为输入，使用循环一致性约束训练过程：
LCycle = ||ψ(lT,r, ψ(lS,r, lT,n)) −lT,n||1.
(5.6)3）对抗训练损失。采用对抗训练过程对模型进行优化，以增加模型基于人脸关键点的表情迁移能力。本损失函数包含两部分：LAdvT F 用来判断给定的人脸关键点向量的真假性，LAdvS 用来估计给定一组人脸关键点的身份相似度。具体定义如下：
LAdvT F =Ex∼pdata(x)[log(DTF(x))] + Ez∼pdata(z)[log(1 −DTF(ψ(z)))],LAdvS =Ex1,x2∼pdata(x)[log(DS(x1, x2))]+Ez∼pdata(z),x1∼pdata(x)[log(1 −DS(x1, ψ(z)))],(5.7)其中x 代表真实的人脸关键点向量分布空间，z 代表ψ 的输入空间。
715多条件受限的图像几何语义编辑浙江大学博士学位论文几何感知生成器损失训练阶段总的损失函数LAll 定义为:
LAll = λPixelLPixel + λAdvLAdv + λTPLTP,(5.8)其中λi 为第i 项损失函数的权重，本文训练时设置λPixel = 100、λAdv = 1 和λTP = 0.1。
1）像素级别损失。使用L1 损失计算生成的人脸驱动图像与真值的像素误差：
LPixel = || ˆIS,n −IS,n||1.
(5.9)2）对抗训练损失。通过鉴别器网络D 引入对抗训练方式以提升生成图像的真实性：
LAdv =Ex∼pdata(x)[log(D(x))] + Ek∼pdata(k),l∼pdata(l)[log(1 −D(ϕ(k, l)))],(5.10)其中x 代表真实的图像空间，k 和l 分别代表输入几何感知生成器ϕ 的源图像和人脸关键点图像空间，鉴别器网络D 与CycleGAN 保持一致。
3）三元组感知损失。LTP 通过约束类内和类间的感知度量距离来帮助生成具有更多细节的图像，并更好地解耦人脸图像的几何和外观纹理信息，具体实现参见前述三元组感知损失小节。
5.4基于音频信号控制的多人脸驱动本小节对基于音频信号控制的多人脸驱动任务做了进一步研究，针对当前音频人脸驱动数据集质量较低的问题构建了高质量的AnnVI 数据集以支持相关研究工作，同时以前述的基于人脸图像控制的多人脸驱动框架FReeNet 作为基础，提出了针对音频信号控制的多人脸驱动框架。图5.4 展示了本章节研究方法的流程示意图，其包含音频特征融合器ψ和几何感知生成器ϕ 两个分支，前者以音频MFCC 特征、人脸姿态和眨眼三种信息作为输入预测人脸几何特征fGeo，后者以源图像IS 作为输入，在fGeo 的驱动下生成输出图像ˆIO。
5.4.1音频特征融合器当前的基于音频的人脸驱动方法只利用音频信息作为驱动信号，但这种处理方式极不合理，只是研究发展需适配当下数据集特性的折中处理方式，因为音频信号只与面部嘴巴局部的运动强相关，而与头部姿态和眨眼信息无直接关联。基于此，本研究发布了一个包含同步的人脸图像、音频片段、人脸关键点、头部姿态和眨眼信息的AnnVI 数据72浙江大学博士学位论文5多条件受限的图像几何语义编辑图5.4基于音频控制的多人脸驱动框架示意图集，并提出了一个音频特征融合器ψ 将以音频为主的多种输入信息转为几何空间上的表征。如图5.4 上半部分所示，输入的音频、头部姿态和眨眼信息经编码器ψA、ψH 和ψE提取特征后得到隐层特征fA、fH 和fE，从通道维度拼接后得到融合特征fFus，最后经解码器ψD 得到几何空间上的人脸表征fGeo，其用来为人脸生成阶段提供面部运动信息。
特别地，ψA 包含10 层卷积层，其中5 层卷积提取时序信息，5 层卷积融合并加强特征；ψH、ψE 和ψD 均由3 层线性层构成。训练时fGeo 经一个线性层（对应参数为WGeo）预测人脸关键点，由几何损失LGeo 进行约束。整个过程可表示为：
fGeo = ψD(fFus) = ψD([fA, fH, fE]).
(5.11)5.4.2几何感知生成器本小节提出的几何感知生成器遵循前述FReeNet 中的特征编码-融合-解码框框架，但对模型的结构和特征融合方式做了修改：
1. 使用基于神经结构搜索技术得到的轻量模型 作为主干网络，在保证模型效果和降低参数量的同时具有实时的GPU 和CPU 运行速度。
2. 提出了新颖的几何控制器模块（Geometry Controller，GC），能够高效地将人脸几何特征fGeo 以向量条件的形式融合到主干网络中，实现几何可控的人脸驱动任务目标。
735多条件受限的图像几何语义编辑浙江大学博士学位论文图5.5几何控制器模块结构示意图图5.4 下半部分展示了几何感知生成器ϕ 的示意图：输入源图像IS 和基于音频预测的人脸几何特征fGeo，ϕ 通过编码器ϕE、特征转换器ϕT =ϕ1T, ϕ2T, . . . , ϕLT	和解码器ϕD 三部分生成人脸驱动图像ˆIO，其在保持IS 身份信息的基础上具有符合fGeo 的面部运动。
该过程描述为：
ˆIO = ϕD(ϕT(ϕE(IS), fGeo)),(5.12)其中L 代表模块重复次数，本研究设置为9 以权衡模型效果和效率，ϕiT(i = 1, · · · , L) 为搜索到的结构块。第l 个结构块的输出F lin 接一个GC 模块以注入几何特征fGeo，得到最终输出F lout。
得益于何控制器GC 模块的引入，改进的几何感知生成器可以直接将运动信息以向量的形式注入到生成器中，实现了单阶段端到端的模型训练过程。图5.5 展示了GC 模块的具体结构：输入的几何特征fGeo 经由多层感知机（本文为2 层）预测卷积核的权重和偏置参数W l ∈RC×(Cg×k×k+1)，其作用于输入特征F lin ∈RC×H×W 后得到输出特征F lout ∈RC×H×W，公式表示如下：
F lout = Conv(F lin; W l) = Conv(F lin; Linearl(fGeo)),(5.13)其中W l 包含C × Cg × k × k 个权重参数和C 个偏置参数，k、C 和g 分别代表卷积核大小、通道数和分组数，且C = Cg ∗g，由此可以通过控制这些参数来控制注入信息的强度。
特别地，当{k = 1, g = C} 时GC 退化为AdaIN 操作，本研究设置{k = 3, g = C} 以用更大的3×3 感受野来更好地融合局部信息。
5.4.3总体目标函数训练阶段采用几何损失LGeo 对人脸关键点进行约束，内容损失LCtx 对生成的驱动人脸图像进行约束，以及对抗损失LAdv 用于提高生成图像的真实性。总的训练损失函数为：
LAll = λGeoLGeo + λCtxLCtx + λAdvLAdv,(5.14)74浙江大学博士学位论文5多条件受限的图像几何语义编辑其中λGeo、λCtx 和λAdv 用来平衡不同损失函数项的比重，本文训练时设置λGeo = 1、λCtx = 100 和λAdv = 1。
1）几何损失。使用L1 损失计算预测的人脸关键点ˆl 与真值l 的坐标误差：
LGeo = ||ˆl −l||1 = ||WGeofGeo −l||1,(5.15)其中几何特征fGeo 经一个线性层回归得到人脸关键点ˆl。
2）内容损失。使用L1 损失计算生成的驱动人脸ˆIO 和真值IO 的像素误差：
LC = || ˆIO −IO||1.
(5.16)3）对抗损失。训练过程中采用鉴别器D 构建对抗训练对几何感知生成器进行优化，以增加生成图像的真实性：
LAdv = E ˆIT ∼pf[D( ˆIT)] −EIT ∼pr[D(IT)],(5.17)其中pf 和pr 分别代表生成和真实的图像分布，鉴别器D 包含5 个卷积层。
5.5AnnVI 数据集当前基于音频的人脸驱动数据集存在分辨率较低且图像和音频质量较差的问题，且一般仅对图像中人脸位置进行标注而缺乏更细粒度的信息，比如更精细的106 人脸关键点、头部姿态和眨眼信息等。因此当前的方法只利用音频信息进行人脸驱动建模，但这是极不合理的一种折中方式，因为音频信号只与人脸嘴巴局部区域的运动强相关，对于人脸姿态和眨眼信息的控制却无能为力。基于此，本章提出了具有细粒度标注的AnnVI 数据集，其具有512×512 的原始分辨率、106 点人脸关键点标注，以及头部姿态和眨眼信息以补充音频无法控制的潜在头部运动。具体地，该数据集包含6 名播音员（3 名男性和3 名女性），总计23,790 帧图像，包含同步的人脸图像、音频片段、人脸关键点、头部姿态和眨眼信息。图5.6 显示了AnnVI 数据集中6 名播音员的样例图像。
图像处理本数据集原始数据来自于YouTube 上采集的6 个1080P 分辨率视频，对视频抽取帧图像后通过Face++ 在线人脸检测接口 对每一帧的人脸关键点和头部姿态进行检测，其中人脸关键点包含106 个二维坐标点信息。基于人脸关键点信息可计算人脸最小外接正方形，使用1.4 倍率扩展后裁剪人脸，确保人脸区域包含在框选区域中，然后将图像大小调整到256×256 分辨率进行本章实验研究。请注意，构建数据集时每幅图像的检测和标注结果都进行了人工复查，以确保图像质量和标签信息的可靠性。
755多条件受限的图像几何语义编辑浙江大学博士学位论文图5.6AnnVI 数据集样例音频处理对于获取的音频重采样至44.1kHz，使用与工作 相同的方式提取音频的Mel-Frequency Cepstral Coeﬀicients（MFCC） 特征，其中FFT 窗口长度为2,048，连续帧间采样数为512，MFCCs 特征数为20，同时增加MFCC 的一阶和二阶特征来保留更多原始信号的信息。对于每一帧图像，以其为中心使用连续64 个MFCC 特征作为对应的音频特征，即得到一张64x60 分辨率的特征图作为音频输入。
头部姿态和眨眼头部姿态信息由Face++ 在线检测接口 得到，包含偏航、俯仰和滚转角三种信息。在AnnVI 数据集中，三个分量的取值范围分别为−0.354 ∼0.196、−0.367 ∼0.379 和−0.502 ∼0.509，单位为弧度。本研究将眨眼信号定义为眼睛的高度除以宽度，经由人脸关键点信息计算得到。
5.6实验结果5.6.1数据集介绍本文主要基于广泛使用的RaFD 和Multi-PIE 数据集进行基于人脸图像控制的多人脸驱动实验，在本文提出的高质量AnnVI 数据集上进行基于音频信号控制的多人脸驱动实验，并在一些小的数据集及In-the-wild 图像上与主流方法进行对比。
具体评测之前，本章先对上述数据集进行简单介绍：
76浙江大学博士学位论文5多条件受限的图像几何语义编辑RaFD该数据集全称Radboud Faces Database，发表于2010 年，包含对67 个人员采集的8,040 张图像，其中每个参与者需采集5 个不同角度下3 种视线方向的8 种表情。本文使用HyperLandmark 对每张图像提取106 人脸关键点，计算人脸中心后以416 × 416大小进行裁切以确保人脸区域包含在框选区域中，然后将图像大小缩放到256×256 分辨率。特别地，本章仅采用45°、90° 和135° 三个角度的数据进行实验。
Multi-PIE该数据集发表于2010 年，基于CMU 3D 采集房间使用15 个高质量视频摄像机和18 个闪光灯组成的硬件同步网络对337 名真人对象采集超过750,000 张人脸图像，本章对该数据的处理方式与RaFD 数据集保持一致。
5.6.2实验设置所有代码基于PyTorch 框架实现，使用Adam 优化器对模型参数进行训练，权重衰减设置为1e−4。对于人脸关键点转换器，设置参数β1 = 0.99, β2 = 0.999，初始学习率3e−4，每300 个数据集迭代降为原来十分之一，以16 批大小总共训练1,000 次迭代。对于几何感知生成器，设置参数β1 = 0.5, β2 = 0.999，初始学习率2e−4，每120 个数据集迭代降为原来十分之一，以4 批大小总共训练400 次迭代，鉴别器使用PatchGAN。对于基于音频信号控制的多人脸驱动，设置参数β1 = 0.5, β2 = 0.999，初始学习率2e−4，每40个数据集迭代降为原来十分之一，以16 批大小总共训练110 次迭代。模型训练时不使用额外复杂的数据增强方法，运行速度均在单卡2080Ti GPU 上以批大小1 进行测试。
5.6.3评测指标为了公平有效地与主流方法进行对比，本研究采用SSIM（Structural Similarity） 在像素级层面评估生成图像与真值的误差，以及FID（Fréchet Inception Distance） 在语义级层面评估生成图像与真实图像的分布差异。基于音频信号控制的多人脸驱动实验，本研究还采用了LPIPS（Learned Perceptual Image Patch Similarity） 指标评估对齐图像在特征感知层面的差异。为了进一步评估不同方法生成图像给人的直观感受，我们雇佣真人进行公平的人工评测实验，同时本章还对不同方法的参数量和运行速度进行了分析。
5.6.4与主流方法对比为了充分评估所提出的基于图像和音频控制的多人脸驱动模型的有效性，本小节与主流方法在多个数据集上进行了定性和定量的对比实验。
775多条件受限的图像几何语义编辑浙江大学博士学位论文图5.7RaFD 人脸数据集驱动对比可视化基于人脸图像控制的多人脸驱动本小节在RaFD 数据集上与基线进行了定性对比实验，以证明所提出方法的有效性和泛化性。图5.7 展示了定性对比结果，第1 列为用来提供身份信息的源图像，第1 行前8 张图像为数据集中随机选取的带有8 种部不同表的不同人员，最后4 张为数据集外的人脸以测试模型的泛化性，其它区域为对应的人脸驱动结果。对比第2-3 行的对比方法和4-5 行的本文方法，可以明显地发现对比方法会将目标人脸的轮廓映射到生成图像中，即不能很好地保持源人脸身份信息，且生成的图像会存在伪影，比如第2 行第2 列生成图像的额头会存在明显的变化。相比之下，本文方法能够很好地保持源人脸身份信息且实现目标人脸的表情迁移，同时在数据集外的驱动测试也证明本文方法具有更好的泛化性。最后两行显示了源图像在其他角度下的测试结果，表明本文方法能够适用于源图像存在多角度的实际应用场景中。
为了进一步对比不同方法的效果，本小节以ReenactGAN 工作中采用的测试数据作为标准来对主流方法进行公平对比。图5.8 展示了本工作与X2Face 和ReenactGAN的对比结果，第1 行代表目标人脸用来提供表情信息，其它3 行图像为不同针对固定身份生成人脸驱动图像，可以看到本文方法具有更好的驱动图像质量和表情一致性，右侧显示了放大图像以更清晰地对比。同时也对不同方法使用了SSIM 准则进行度量，本研究获得了最高的0.589 指标结果，表明生成的图像与真值差异更小。特别地，本实验中驱动人脸的表情和姿态均由目标人脸提供。
考虑到模型的实用性，表5.1 展示了在n 个身份间进行人脸驱动时不同方法的参数量78浙江大学博士学位论文5多条件受限的图像几何语义编辑图5.8In-the-wild 图像测试对比结果和速度定量测试结果，得益于高效的多人脸驱动框架设计，本文方法仅需一个模型即可对n 个人脸进行建模，即总参数量不会随着人员个数n 的增加而发生变化。相较于最先进的多人脸驱动方法X2Face，本文提出的方法极大地降低了参数量（108.8M→17.3M），同时具有3 倍之上的运行提速，意味着更高的实际应用价值。
方法参数量(M)速度转换器生成器(FPS)Pix2Pix-16.7×n(n-1)75Xu 等人-16.7×n(n-1)73ReenactGAN7.8×n61.1×n48X2Face-108.816FReeNet（本文方法）4.517.357基于音频信号控制的多人脸驱动本小节在YouTuber 数据集 上与主流方法进行了定性对比实验。
如图5.9 所示，本文方法相比于Wav2Pix 和MakeItTalk 能够生成更清晰的人脸，与APB2Face 在视觉效果上差异不大，但具有更少的参数量（16.7M→4.1M）和更快的运行速度（8.5FPS→32.6FPS）。
本小节进一步对不同方法在多个指标上进行了定量评估，如表5.2 所示，本文方法获得了最高评估分数，即FID=13.256、SSIM=0.765 和LPIPS=0.0211；同时效率得到了明显提升，相比于主流方法参数量下降至少4 倍为4.1M，在i9-10900K CPU 和2080Ti GPU 平795多条件受限的图像几何语义编辑浙江大学博士学位论文图5.9与主流音频驱动方法对比结果方法FID ↓SSIM ↑LPIPS ↓参数量↓速度↑速度↑(M)(CPU)(GPU)Wav2Pix214.2800.5370.082624.77129.8275.7MakeItTalk33.6400.6570.038774.8288.765.6APB2Face13.3750.7340.022516.6968.5200.4本文方法13.2560.7650.02114.08533.8158.9台分别具有33.8FPS 和158.9FPS 的实时运行速度，说明具有非常高的实用价值。
5.6.5消融实验模型组件对人脸驱动的影响本文提出的基于人脸图像控制的多人脸驱动模型FReeNet包含人脸关键点转换器、几何感知生成器和三元组感知损失三个创新组件，本小节进行了定性和定量剥离实验以对每一项组件的效果进行充分评估。图5.10 显示了不同模型组件对人脸驱动的定性影响，其中第1 行为只使用几何感知生成器的结果，可以看到无论目标图像来自于数据集内还是外，生成的图像质量都有保证，但其不能保持源人脸的身份纹理信息，且脸部轮廓也会发生变化。第2 行为添加了人脸关键点转换器的改进结果，可以看到上述的问题得到了极大的缓解，生成的驱动人脸图像具有很好的身份一致性。第3 行为80浙江大学博士学位论文5多条件受限的图像几何语义编辑图5.10模型组件对人脸驱动结果的定性影响可视化在训练时采用了三元组感知损失的最终结果，生成的图像能够保持更丰富的面部细节，比如眉毛、皱纹和嘴巴，放大蓝框和红框区域以观察更精细的视觉结果。
方法SSIM ↑FID ↓人工评估Pix2Pix0.62912.8441.3%人脸关键点转换器0.65911.67-+ 几何感知生成器0.71113.26-+ 三元组感知损失0.71712.1774.9%的结论，即每个组件对最终的驱动人脸都能产生正向效果，当所有组件一起使用时模型获得了最好的SSIM 指标结果。值得注意的是，由于不同模块组成的实验都使用了对抗训练损失，故他们具有相似的图像特征感知级别的FID 指标，而人脸关键点转换器因在几何空间上做了约束使得该指标值略微提升。74.9% 的人工评估结果说明本文方法生成的图像在真人视觉感官上具有非常明显的优势。
TP 损失函数对模型解耦的影响为了进一步说明TP 损失函数对模型解耦学习的有效性，本小节构建了只包含两个身份的训练集进行实验，因为TP 损失要求训练集至少包含两个身份。图5.11 显示了定性实验结果，第1 列代为两个不同表情的目标图像，第2-5列为不使用TP 损失训练得到的推理结果，对应列的源图像分别为随机选取的一张人脸图像、无特征的黑色图像输入、无特征的白色图像输入以及高斯噪声图像输入，而第6-9 列为使用TP 损失得到的推理结果。实验结果表明，TP 损失在一定程度上可以解耦外观和几815多条件受限的图像几何语义编辑浙江大学博士学位论文图5.11TP 损失函数影响结果可视化何信息，例如，当源图像为无特征的黑色/白色或高斯噪声图像时，使用TP 损失训练的模型生成的驱动人脸包含更少与源人脸相关的身份信息，而源图像为人脸图像时能够生成高质量的人脸驱动图像，说明模型学习到了如何从源图像中提取身份信息而不是简单的目标图像到输出驱动图像的直接映射。
损失项LL1+Lcyc+Lcyc, LDACE7.236 ± 0.0154.526 ± 0.0150.895 ± 0.010损失函数对基于图像控制人脸驱动的影响本章选用平均坐标误差（Average Coordinate-wise Error，ACE）评估不同损失函数项对人脸关键点转换器结果的影响。如表5.4 所示，仅使用LL1 损失函数时模型预测的人脸关键点在256 分辨率尺度下平均像素误差为7.236，当加入循环一致性损失Lcyc 时预测精度得到了显著提高，平均误差降到了4.526。进一步引入对抗训练LD 后人脸关键点转换器获得了最优的结果，ACE 指标为0.895，说明此时模型预测结果与真值非常接近。
损失函数对基于音频人脸驱动的影响表5.5 列出了训练时采用不同损失函数组合对基于音频人脸驱动模型的影响，具体地，该实验采用图像级别的FID 指标、像素级别的SSIM 指标以及图像对齐层面的深度特征LPIPS 指标在AnnVI 数据集上进行测试，以充分地说明每一项损失函数对于模型训练的效果。结果显示每一个损失项的加入均能带来相较于基线更好的结果，当所有损失项都使用时模型获得了最优的结果。
82浙江大学博士学位论文5多条件受限的图像几何语义编辑LCLGLAdvFID ↓SSIM ↑LPIPS ↓15.5560.7350.024214.1720.7860.020312.5280.7490.021511.2060.8050.0187模块名称FID ↓SSIM ↑LPIPS ↓AdaIN11.5150.7920.0196GC11.2060.8050.0187信息注入模块效果对比分析前述方法部分提到AdaIN 可视为GC 模块的一个特例，因此本小节与主流的AdaIN 模块进行了定量对比实验以评估GC 模块的有效性。
表5.6展示了定量对比结果，可以看到GC 模块相比于AdaIN 能够帮助模型获得更高的指标结果，证明了GC 模块设计的有效性。
5.6.6解释性实验角度和光照鲁棒性分析为了进一步证明提出的多人脸驱动模型FReeNet 的效果和鲁棒性，本小节在Multi-PIE 数据集上进行了多角度和复杂光照环境下的人脸驱动实验。
图5.12 中第1 列包含了3 个随机选择的具有不同姿态和表情的目标人脸，第1 行包含连图5.12Multi-PIE 多角度人脸驱动测试可视化835多条件受限的图像几何语义编辑浙江大学博士学位论文图5.13Multi-PIE 复杂光照人脸驱动测试可视化续角度变换的9 张不同身份源人脸，其它区域展示了不同角度的源人脸和驱动人脸在相同光照下的驱动生成结果，可以看到本文方法可以很好地捕获目标人脸的表情信息并进行迁移，同时源人脸对角度具有很好的适应性。类似地，图5.13 展示了光照变化情况模型的生成结果，可以看到本文方法对光照具有很好的鲁棒性。
图5.14人脸关键点解耦操纵实验可视化人脸关键点操纵驱动得益于FReeNet 的面部纹理和几何的解耦设计，驱动生成过程中可以通过对人脸关键点操纵实现对生成驱动图像的控制。图5.14 展示了人脸驱动时在人脸关键点空间对嘴巴形状（关闭→打开）、面部轮廓（宽→窄）和眼睛位置（左眼顺时针，右眼逆时针）操纵的生成结果，可以看到当人脸关键点空间位置发生改变后，对应生成的驱动图像也会相应发生变化。结果表明该方法在能很好地保持源人脸身份的前提下实现对人脸局部区域的控制，提供了一种灵活的方式对驱动图像进行可控生成，同时证明了本文解耦设计的有效性。
84浙江大学博士学位论文5多条件受限的图像几何语义编辑图5.15AnnVI 数据集多人脸之间驱动结果可视化AnnVI 基于音频的多人脸之间驱动可视化为了直观展示本文方法在基于音频输入的多人脸之间驱动的有效性，本章节在AnnVI 数据集上进行了定性的可视化实验。
如图5.15 所示，第1 行和第1 列分别表示随机选择的6 张源人脸图像和6 张目标图像，在驱动过程中前者提供身份纹理信息而后者提供音频、头部姿态和眨眼信号。实验结果表明，本文方法仅需一个模型即可实现基于音频的多人脸驱动任务，在保证生成图像身份一致性且高质量的前提下，可以使用非自身的驱动信息进行控制。
解耦控制生成实验得益于AnnVI 数据集的精细标注和音频特征融合器的解耦输入，头部姿态和眨眼信息可以独立地控制生成过程。图5.16 展示了解耦控制的图像驱动结果，前3 行分别代表使用头部姿态的3 个分量单独控制而其它输入信号保持不变的驱动结果，最后一行只控制眨眼信号。结果表明提出的方法学习到了从头部姿态和眨眼信号单独控855多条件受限的图像几何语义编辑浙江大学博士学位论文图5.16头部姿态和眨眼解耦实验可视化制驱动人脸的对应属性，因此在实际应用中也会更加灵活可控。
5.7本章小结本章针对图像语义级研究中的多条件受限的图像几何语义编辑问题进行了讨论，面向语义层面的视觉理解及生成任务，从实际应用需求出发对典型的人脸驱动任务进行了深入研究。早期基于深度学习的工作仅对一个固定身份进行建模，即人脸驱动模型一旦训练完毕只能针对固定身份进行驱动，这极大地限制了模型的应用性。最近一些工作尝试在多人脸驱动问题上进行研究，但在生成可控性和图像质量方面仍不令人满意。另一方面，当前绝大部分工作以图像控制作为研究对象，而忽略了实际应用中对音频控制的增长需求，这是因为计算机视觉技术发展相对较成熟，而当前的音频数据集质量则较差。针对上述问题，本章在模型结构层面进行了思考，基于人脸几何和纹理信息解耦的思想设计了统一的多人脸驱动模型FReeNet，其包含一个在几何空间进行面部运动迁移的人脸关键点转换器，以及一个在纹理空间生成输出图像的几何感知生成器。为了进一步增强模型的解耦能力，本研究基于三元组损失和感知损失设计了三元组感知损失，能有效地缓解人脸关键点到输出驱动图像的直接映射。此外，本章提出了高质量的AnnVI 数据集，其包含丰富的人脸图像、音频片段、人脸关键点、头部姿态和眨眼信息，支撑了本章对基于音频控制的高分辨率人脸驱动的研究工作。具体地，本章复用FReeNet 框架实现了基于音频输入的多人脸驱动任务，设计了音频特征融合器以对音频信号进行处理，同时提出了几何控制器模块以更好地进行控制信号的注入。至此，本章节实现了基于多信息源控制的多人脸驱动任务目标。大量的定性和定量实验证明了所提方法的优越性，相比于主流方法在多个数据集上获得了最优指标，同时具有更好的视觉效果。
866运动约束下的图像序列生成运动约束下的图像序列生成关注视频语义级的时序合理性及多样性生成问题，本章将条件式视觉内容生成的研究对象从二维图像拓展到包含时间维度的视频级别，针对典型的图像动态化任务开展研究，专注于静态图像到动态视频的生成，提出了运动纹理解耦的图像动态化方法，在泛娱乐和元宇宙等领域中的内容创作板块具有很大的应用需求[9,290]。
不同于前述的图像语义级研究任务，本章研究的内容除了需要对输入单帧图像进行语义级的深度理解外还需要进行时序运动建模，要求输出的预测视频在帧图像质量和时序连续性上都具有较高的要求。最近两年随着元宇宙和短视频行业的快速发展，对视频生成技术的应用需求也陡然增加，然而因算法研究不成熟以及数据集规模和质量较低等问题，导致生成的视频质量较差且不具有多样性，这极大地限制了算法的落地应用。本章针对上述问题提出了针对性的解决方案，在模型结构设计层面基于运动和纹理解耦的思想设计了端到端的动态视频生成框架，实现了高质量和多样化的动态视频生成目标，在数据方面发布了一个大规模的高分辨率QST 数据集以进一步支持算法的研究。多个人脸和延时视频公开数据集上定性和定量的实验结果表明，本文方法相比主流方法能够生成更高质量且更多样化的动态延时视频。
6.1引言运动约束下的图像序列生成研究致力于实现合理且高质量的视频生成目标，同时在内容创作领域还需考虑多样化生成问题，相较于图像语义理解任务因需考虑额外的时序建模而具有更高的挑战性。一般而言，目前大多数条件式生成工作集中于图像建模而很少在视频领域上进行研究，需求较少的视频生成应用一般也可由图像生成方法在时序上逐帧扩展而来，比如视频图像上色、视频人脸换脸和视频图像补全等应用均可由相应的图像应用加上时序运动约束得到，而这种方式也会面临时序不稳定和串行速度慢的问题。本文以典型的图像动态化为例展开工作，从时序运动和纹理解耦以及并行生成等方面提出新的研究思路。
视频生成是一项从噪声、图像或掩码等特殊输入条件下生成图像序列的任务，在多个领域中具有非常高的应用价值和潜力，比如影视制作、虚拟人序列图像预测生成、短视频应用、视频数据集增广和运动预测等。相比于二维图像级别的人脸换脸和人脸驱动，该任876运动约束下的图像序列生成浙江大学博士学位论文务因扩展了时间维度而更具有挑战性。除了对图像内容进行语义理解外，还要对不同语义间的相互关系和运动信息进行建模，比如语义区域明确的人脸五官运动和语义区域模糊的云雾等自然场景的运动，保证生成视频的帧质量和时序合理性成为一件非常有挑战性的目标。本文针对图像动态化进行深入研究，具体为以静态图作为输入预测合理的动态视频，在语义区域定义明确的人脸数据集和更具有挑战性的延时视频数据集上进行了实验，针对当前存在的高质量和生成难题提出了具体的解决方案。
最近几年，随着基于深度学习的计算机视觉的快速发展和生成对抗网络（GenerativeAdversarial Network）技术的深入研究，在视频生成领域有许多优秀的动态视频生成模型被相继提出[165,168,291–294]。VGAN 利用对抗学习方法可以从大量的未标记视频中学习移动前景和静态背景路径的思路，探索了如何利用随机噪声来学习动态视频的生成方法。然而，这种基于噪声输入的方法涉及到分辨率逐渐升高且需要考虑时序生成的问题，会存在训练难度较大且生成质量较低的问题。随后，Villegas 等人 引入LSTM 模型对目标时序信息进行显性建模，即按顺序依次预测未来每一帧中物体的位姿信息，然后使用基于类比的编码器-解码器模型生成对应帧的图像。这类方法将视频生成显性解耦为时序的运动预测和空间的纹理生成两部分，因而可以生成更高质量且时序运动合理的图像，但需要一些额外标记的人体姿态信息及时序预测模型，同时串行的预测过程在实际应用中也是不友好的。最近的MoCoGAN 应用LSTM 模型生成由内容部分和运动部分组成的随机向量序列，然后将它们映射到视频帧序列。这种方法在理论上可以生成包含相同内容的多种动态视频，因为可以在改变随机向量运动部分的同时固定内容部分。然而，受限于向量输入及循环神经网络过程，该方法生成视频的分辨率仍然很低且不具有并行性。为了获得高分辨率和高质量的视频，一些研究人员建议在所有的编码器和解码器模块中使用时空三维卷积，目的是隐式地同时建模视频序列的空间和时间信息，但这类方法会引入大量的训练参数导致训练优化过程极其缓慢。最近，基于3D 卷积的两阶段模型MDGAN 在参数量、训练难度和效果方面做到了比较好的权衡，该方法仅需要一张初始图像作为输入即可自动地预测相应的高分辨率延时视频，且在真实性和运动性方面表现得都很不错。然而，该方法因采用了两阶段训练方式导致参数量和易用性方面较不友好，且缺乏多样化的视频生成能力，限制了该方法的实用性。尽管如此，MDGAN 仍为当前最优秀的图像动态化模型之一，本文以该方法作为基准，针对其设计存在的不足以及实际应用需求改进了端到端的单阶段模型。通过对当前方法进行分析归纳，本研究得出好的图像动态化模型应该具备如下几个基本特征：
1. 运动和纹理信息解耦的端到端模型结构设计，使得模型更具有控制性和解释性，一88浙江大学博士学位论文6运动约束下的图像序列生成定程度上能够提高生成视频的质量。
2. 图像内容不能以向量的形式注入，否则会面临训练困难且生成图像质量较差的问题。
3. 时序预测不能通过类RNN 的循环神经网络进行，否则模型对于单个视频的推理过程不具有并行性，导致较低的实用效率。
对于上述三项总结内容，运动和纹理信息解耦是非常关键的设计，同时运动信息的时序表征应使用并行建模方式以保证实用性，这不禁让研究者思考如何更好地对运动信息进行建模。近年来，一些研究者在训练和测试阶段引入表征运动信息的光流图，旨在明确地向网络提供运动信号。Liang 等人 设计了一种双向运动生成对抗模型，可以同时进行未来帧图像预测和未来流运动预测任务，保证了高质量的视频帧生成，但在未来流的预测上则需花费更多的计算代价和时间消耗。除了质量和分辨率，基于单帧图像所生成的视频的多样化对于模型的实用性至关重要，因视频内容生成需要模拟大自然中物体运动的多样性这一特点，即同一个语义目标在时序上可以有许多合理的运动轨迹。Li 等人 首先对该问题进行了思考并提出了针对性方案：训练时首先将采样的噪声映射到连续的运动流，然后结合提出的视频预测算法从单个静止图像中合成一组包含多个时间步长的可能未来帧，测试时通过在分布中采样不同的样本点实现多样化的视频预测生成。本文研究也对多样性视频生成进行了思考，引入归一化运动矢量概念来控制多样化生成过程。
具体地，本章节针对图像动态化任务提出了一个端到端的动态视频生成框架（DynamicVideo Generation Network, DVGNet），实现了从单张图像生成多样化动态视频的目标。
考虑到泛娱乐和元宇宙等应用领域的内容生成需求，本文研究内容包括语义明确的人脸运动预测，以及更难的自然风景延时视频预测，因其会存在多种静态物体（房屋、大地、树木等）和动态元素（云、雾等），这对模型的设计和训练提出了非常大的挑战。具体地，该框架由对运动信息建模的光流编码器（Optical Flow Encoder，OFE）和对纹理信息建模的动态视频生成器（Dynamic Video Generator，DVG）两部分构成，前者通过无监督光流估计方法获得连续图像之间的运动映射并将其编码为归一化运动向量，后者以运动向量和静态初始图像作为输入并行地预测动态视频每一帧。考虑到当前公开的延时视频数据集存在质量较差的问题，本文进一步构建并发布了一个大规模的高分辨率数据集（QuickSky Time，QST）来支撑算法的研究。总结本章研究内容的贡献如下：
1. 本研究基于运动和纹理解耦的思想设计了端到端的动态视频生成框架，在内容损失、运动损失和对抗损失的约束下实现了高质量和多样化的动态视频生成。
2. 本研究针对运动解耦设计了光流编码器模块，用来将表示视频运动的光流信息编码为归一化向量，同时可通过随机运动向量采样的推理方式实现多样化的视频生成。
896运动约束下的图像序列生成浙江大学博士学位论文3. 本研究基于解耦思想设计了双分支动态视频生成器，包含一个运动分支用来引入运动控制信号，以及一个内容分支用来提取纹理信息，在两者合力作用下生成目标动态视频。
4. 针对当前延时视频数据集质量较差的问题，本研究提出了一个大规模的高分辨率QST 数据集来支持该任务的研究，同时该数据集也可作为高分辨率图像和视频生成等任务的新基准，为多种任务的发展带来新鲜血液。
5. 大量的定性和定量实验证明了本文方法相对于主流方法的优越性，在人脸数据集和多个延时视频数据集上获得了更高的指标和更好的视觉效果，同时能够以一张初始图像作为输入生成多样性的动态视频，具有非常高的应用价值和潜力。
6.2方法概述本章以泛娱乐和短视频等行业实际应用需求为出发点对图像动态化任务进行了研究，针对高质量和多样性视频生成提出了有效的解决方案。图6.1 展示了本文研究方法的示意图，其由建模运动信息的光流编码器ψ（Optical Flow Encoder，OFE）和纹理信息的动态视频生成器ϕ（Dynamic Video Generator，DVG）两部分组成。对于给定的成对训练数据：
初始图像I0 和动态图像序列I0∼T，先通过无监督光流估计模型得到表征运动信息的连续光流U1∼T，表示为：
I1∼T →U1∼T。
然后由光流编码器将其压缩为表征视频运动的归一化运动向量f，经多组线性层得到不同深度的运动向量特征{ ¯f1, ¯f2, . . . , ¯fn}，作为下一阶段视频生成的控制信号。动态视频生成器以初始图像I0 作为纹理输入，在{ ¯f1, ¯f2, . . . , ¯fn}的控制下生成符合相应运动语义的动态图像序列ˆI1∼T。
注意小写和大写字母表示标量，粗体小写字母表示向量，粗体大写表示矩阵。
6.3光流编码器本章选用光流图表征图像序列之间的运动关系，考虑到方法对不同数据集的迁移性及标注成本，选用无监督光流方法SelFlow 作为本文的默认光流估计模型，在人脸数据集上会换用效果更好的ARFlow 模型进行实验，这能略微提升对人脸运动的建模能力。如图6.1 右上角所示，输入的动态图像序列I0∼T = {I0, I1, . . . , IT} 经无监督光流估计模型得到表征运动信息的T 帧连续光流U1∼T =U0→1, U1→2, . . . , U(T−1)→T	，但此时的光流表征完全依赖于输入的图像序列，因表征较具体而与初始图像I0 存在语义运动不匹配的问题，同时不具有多样化的运动控制能力。基于上述问题，本小节设计了光流编码器模块ψ（见图6.1 上半区绿色背景部分），其以光流运动表征U1∼T 作为输入得到归一化的90浙江大学博士学位论文6运动约束下的图像序列生成图6.1运动纹理解耦的图像动态化框架运动向量表示f，如此设计的编码方式不仅可以用更少的参数量表征运动信息，还可以在测试阶段从归一化分布中采样各种运动矢量以生成多样化的视频，该过程表示为：
f = ψ(U1∼T).
(6.1)具体而言，光流编码器模块基于3D 卷积构建，因其相比于2D 卷积更适合同时建模图像的时空特征。随着网络加深，时间和空间维度逐渐降低为1，此时的通道维度代表了全局运动信息。为了进一步将f 以控制条件的形式注入到不同深度的纹理特征图中，该模块引入了多组线性层对f 进行不同程度的特征适应：
¯fi = Lineari(f),(6.2)其中¯fi, i = 1, 2, · · · , n 代表适应第i 层的运动向量，n 代表总的信息注入层数，f 和¯fi 的维度本章设置为512。
6.4动态视频生成器为了生成内容与初始图像一致且语义运动与运动向量一致的高保真动态视频，本小节提出了运动纹理信息解耦的双分支动态视频生成器ϕ，结构示意图见图6.1 下半区蓝色背景部分。具体地，生成器ϕ 由图像编码器ϕE、运动分支ϕM、内容分支ϕC 和视频解码器ϕD 共4 部分构成。
图像编码器ϕE 从输入初始图像I0 中提取共享的内容特征Fshared：
Fshared = ϕE(I0).
(6.3)916运动约束下的图像序列生成浙江大学博士学位论文随后Fshared 被送入运动分支ϕM 和内容分支ϕC 分别建模视频序列的时序运动信息和纹理信息。
对于运动分支ϕM，自适应运动向量¯fi 通过AdaIN 层注入到该分支的第i 个特征块中，目的是通过n 次运动控制条件注入预测符合初始图像语义且运动合理的低分辨率时序运动光流图ˆU1∼T：
ˆU1∼T = ϕM(Fshared, ¯f1, . . . , ¯fn).
(6.4)在训练阶段，ˆU1∼T 由真值光流图监督，目的是将运动信息适应到输入的初始图像中。
具体来说，自适应运动向量¯fi 经由线性层转化为对应AdaIN 层的运动样式zi = (zscalei, zshifti)，标记该AdaIN 层输入和输出特征分别为F ini和F outi，那么可以用下面的公式表达特征映射关系：
F outi= zscaleiF ini−µ(F ini )σ(F ini )+ zshifti,(6.5)其中µ(·) 和σ(·) 分别表示计算均值和方差。
对于内容分支ϕC，其对共享特征Fshared 进一步提取特征后与预测的时序运动光流图ˆU1∼T 拼接，然后送入视频解码器ϕD 中合成动态图像序列ˆI1∼T。
6.5总体目标函数DVGNet 在训练阶段采用三种训练损失函数：像素对齐的内容损失C、保证时序合理性的运动损失LM 和提升图像帧质量的对抗训练损失LAdv。
内容损失采用像素级别的内容损失函数评估生成图像的像素准确率，其计算生成的动态图像序列ˆI1∼T 和真值I1∼T 之间的ℓ1 误差：
LC =TXi=1|| ˆIi −Ii||1.
(6.6)运动损失计算预测低分辨率时序运动光流图ˆU1∼T 与真实的低分辨率光流图U LR1∼T之间的ℓ1 误差，以保证运动信息预测的合理性。其中U LR1∼T 之间的ℓ1 由U1∼T 缩放映射得到：
LM =TXi=1|| ˆUi −U LRi||1.
(6.7)对抗训练损失采用带有梯度惩罚的改进WGAN 方法进行对抗训练，以提高生成视频的图像质量和时序合理性。
具体地，鉴别器D 由6 个(3D-Conv)-(3D-InNorm)-(LeakyReLU)92浙江大学博士学位论文6运动约束下的图像序列生成块组成以捕获更有鉴别力的空间和时间特征：
LGAN =E ˜V ∼pg[D( ˜V )] −EV ∼pr[D(V )] + λE ˆV ∼p ˆV [(||∇ˆV D( ˆV )||2 −1)2],其中pr 和pg 分别是真实和生成的视频分布，pˆx 隐式地定义为从pr 和pg 分布中抽样两点连成的直线上均匀取样的点。
总的训练损失LAll 为上述损失函数的加权平均：
LAll = λCLC + λMLM + λAdvLAdv,(6.8)其中λC = 100, λM = 1 和λAdv = 1 用来平衡训练时不同损失函数的权重。
6.6QST 数据集针对当前公开延时视频数据集存在分辨率较低（128×128）且图像质量较差的问题，本章构建并发布了一个大规模的高分辨率数据集（Quick Sky Time，QST）来支撑本文任务的研究。该数据集除了可作为动态延时视频生成任务的基线外，同时也可扩展到图像生成、视频生成、光流估计等计算机视觉的相关任务上。
6.6.1数据集介绍随着硬件算力的提高和深度学习中大模型的深入研究，以及实际应用中对高分辨率图像/视频生成的需求，当前的延时视频数据集[165,167] 不足以支持高分辨率和高质量的视频生成算法的进一步研究，因为这些数据集包含的图像分辨率较低且质量较差。基于此，本文构建并发布了一个大规模的高分辨率延时视频数据集Quick-Sky-Time（QST），其包含从YouTube 上收集的216 个4K 视频中精心裁切的1,167 个单场景视频片段，场景涵盖天空、白云、山、河流、草地、房子等内容。具体来说，每个视频片段的帧数从最短的58帧到最长的1,200 帧，总共包含285,446 帧，每个视频帧分辨率至少为1,024×1,024。这些视频片段被分为训练集（包含1,000 个片段，共244,930 帧）、验证集（包含100 个片段，共23,200 帧）和测试集（包含67 个片段，共17,316 帧），与主流数据集的具体数据对比见表6.1。QST 数据集包含了多种具有挑战性的场景，如不同的时段(白天和夜晚)、不同的场景对象(山、房子等)、复杂的背景以及光影的移动，示例图像如图6.2 左侧所示。
6.6.2图像生成测试随着算力的提升及计算机视觉生成算法的进步，一些基于GAN 的模型可以生成高分辨率且高质量的图像[85,86,129]。本节使用受欢迎的StyleGAN2 对提出的QST 数据集进行936运动约束下的图像序列生成浙江大学博士学位论文数据集视频片段数总帧数分辨率Sky Time-lapse38,2071,222,624128 × 128Beach6,293201,376128 × 128Quick-Sky-Time1,167285,446≥1, 024 × 1, 024图6.2QST 数据集样例及StyleGAN2 生成测试可视化随机生成实验，以评估该数据集的生成难度，以及是否具有稳定的模态，即QST 中的不同场景是否可以通过相同分布的噪声进行映射。图6.2 右侧展示了一些生成的场景图像，结果表明在QST 数据集上训练的StyleGAN2 可以生成丰富且高质量的多样性图像，证明了QST 数据集的有效性和丰富性。同时，对图像仔细观察可发现，生成图像的一些语义细节存在伪造痕迹，比如一些生成图像中的房子和草地并不是很完美，说明提出的QST 数据集对于生成任务来说仍存在较大挑战性。此外，本小节还使用Fréchet Inception Distance（FID） 指标在语义级上评价生成图像的质量，得到了相对较大的36.889 分，再次印证了QST 数据集对于生成任务的难度。
6.7实验结果6.7.1数据集介绍本文基于广泛使用的VoxCeleb1、Sky Time-lapse 和Beach 数据集进行图像动态化技术的研究，并在本文提出的QST 数据集上与主流方法进行了定性和定量对比。
评测之前，本小节先对上述三个公开数据集进行简单介绍：
94浙江大学博士学位论文6运动约束下的图像序列生成VoxCeleb1该数据集包含从YouTube 上采集的1,251 位名人的共计约10 万段音视频，每段视频的图像帧都包含了由FAN 人脸对齐器 得到的5 个人脸关键点，其用来从原始视频中裁剪人脸区域图像并缩放到256×256 原始分辨率。本文遵循对比方法MDGAN的实验设置，每一帧图像调整为128×128 的正方形图像，并将颜色值归一化为[-1, 1]。
Sky Time-lapse该数据集包含超过5,000 个从Youtube 上裁剪的的延时视频片段，其主要包含一些动态天空场景，如云在天空中移动的和星星在星空中移动。整个数据集分为训练集和测试，分别包含35,392 和2,815 个视频片段，每个视频片段包含32 帧图像。对于每一帧图像，其原始大小为640×360，本文遵循相同的实验设置，每一帧图像调整为128×128 的正方形图像，并将颜色值归一化为[-1, 1]。
Beach该数据集包含了从248 个视频中裁剪的6,293 个视频片段，主要由包含天空、云、水、沙、岩石等内容的海岸沙滩场景组成。
每个视频片段包含32 帧图像，总计201,376帧。每一帧图像的原始大小为128×128，本研究的实验保持这一分辨率并将颜色值归一化为[-1, 1]。
6.7.2实验设置对于每个实验数据集，先使用无监督光流估计模型SelFlow 在相应数据集上进行训练，结束之后固定模型参数作为本文方法的固定模块。n 设置为6，即动态视频生成器中的运动分支包含6 个特征提取块且运动向量信息注入6 次，每个特征提取块为标准的ResnetBlock。本文遵循MDGAN 的实验设置，对实验中的一些数据维度列写如下：
连续光流U1∼T ∈R2×32×128×128，归一化运动向量f ∈R512，适应运动向量¯fi ∈R512，初始图像I0 (∈R3×128×128)，低分辨率时序运动光流图ˆU1∼T ∈R2×32×64×64，真实及生成的动态图像序列I1∼T, ˆI1∼T ∈R3×32×128×128。所有代码基于PyTorch 框架实现，使用Adam优化器（参数β1 = 0.99, β2 = 0.999）对模型参数进行训练，权重衰减设置为1e−4，学习率设置为3e−4，每150 个数据集迭代降为原来十分之一，以12 批大小在1 块1080Ti GPU上共训练200 次迭代。模型体积为8.42M，是最先进方法MDGAN（78.14M）的近十分之一。
6.7.3评测指标本章使用峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）来评估采用真值运动向量作为控制条件时生成的视频帧与真值在像素级别的差异，使用结构相似度（Structural Sim-956运动约束下的图像序列生成浙江大学博士学位论文ilarity，SSIM） 来衡量生成视频帧与真值之间的结构相似性。为了更好地度量生成视频序列与真值的运动差异性，本文引入Flow-MSE 指标计算生成的视频序列与真值序列的光流差值。此外，本文进行了人工评测实验，即雇佣真人对不同方法生成的动态视频进行主观视觉质量评判。
6.7.4与主流方法对比本小节与最先进的MoCoGAN 和MDGAN 在多个公开数据集以及本文发布的QST 数据集上进行了定性和定量对比实验，以证明本文方法的优越性。
图6.3VoxCeleb1 数据集上与主流方法对比可视化定性对比结果本小节在VoxCeleb1 人脸数据集上与主流的MoCoGAN 和MDGAN进行了定性比较，图6.3 展示了不同方法的单帧人脸图像动态化预测结果，第一列表示初始人脸图像，第1 行和第5 行表示真值，其它行表示了不同方法的预测结果，最右一列显示了第30 帧的放大结果以更好地对不同方法效果进行对比。通过对结果分析可以发现MoCoGAN 可以建模人脸的运动信息，但生成的图像存在伪影，而MDGAN 则完全不能胜任人脸建模任务，可能原因是该方法针对延时视频生成任务设计而不具有在人脸数据集96浙江大学博士学位论文6运动约束下的图像序列生成上的泛化性。对比之下，本文方法生成的人脸随着时间增加会略微变得模糊，但相比于主流方法明显具有更好的生成图像质量和运动预测结果。
图6.4Sky Time-Lapse 与Beach 数据集上与主流方法对比可视化进一步，本小节在Sky Time-Lapse 和Beach 延时视频数据集上与主流的MoCoGAN和MDGAN 进行了定性比较，并对结果进行了分析。图6.4 展示了不同方法的图像动态化结果，其中上下两区域分别在两个数据集的测试集中随机抽取一个测试样例，第1 列的图像表示初始图像，与之同行的为不同时刻的视频帧真值，其它行表示了不同方法的生成结果，最右一列显示了第30 帧的放大结果，红色和蓝色的矩形框分别标记了不同方法间存在明显优劣的动态和静态细节。结果表明，本文研究相比于最先的方法可以更好地保持初始图像中的内容信息，同时很好地捕获语义目标在时序上的运动信息。比如，MoCoGAN生成的图像序列随着时间的推移会变得越来越扭曲，且图像质量会因模式坍塌问题的出现而严重下降，即不能很好地建模图像内容和预测运动信息；MDGAN 生成的图像序列在颜色上存在严重失真，同时不能合理地保持语义内容，即仍不能很好地实现图像动态化任务目标。相比之下，本章提出的方法可以在较好地保持静态对象内容的基础上预测合理的运动信息，相比于对比方法具有明显的优越性。
976运动约束下的图像序列生成浙江大学博士学位论文图6.5QST 数据集上与主流方法对比可视化方法PSNR ↑SSIM ↑Flow-MSE ↓MDGAN21.6050.7631.242MoCoGAN23.8820.8090.802本文方法31.2810.9380.615本小节遵循相同的实验设置在提出的QST 数据集进行对比实验。图6.5 列出了本文方法与对比方法的序列生成结果以定性评估不同方法的效果，红色和蓝色的矩形框分别标记了图像中有显著区别的动态和静态细节，可以看到不同方法生成的图像质量相比于前述公开数据集有了较大提升。然而，MoCoGAN 在训练过程中存在模式崩溃的问题，生成的每帧图像几乎相同而不含有明显的运动信息。MDGAN 在生成图像质量上相对较好一些，且具有一定的运动建模能力，但仍不能很好地保持输入图像中一些目标的语义信息。本文方法一致性地获得了最好的生成结果，在保持图像质量的前提下有效地建模了场景中物体的运动预测。
98浙江大学博士学位论文6运动约束下的图像序列生成方法PSNR ↑SSIM ↑Flow-MSE ↓MoCoGAN23.8670.8491.365MDGAN23.0420.8221.406本文方法29.9170.9161.275方法PSNR ↑SSIM ↑Flow-MSE ↓MDGAN16.1950.8021.046MoCoGAN21.4130.8260.822本文方法26.2280.8790.764定量对比结果本小节实验选用PSNR、SSIM 和Flow-MSE 度量准则来定量地评估本文方法与最先进方法在VoxCeleb1、Sky Time-lapse 和Beach 数据集上的效果。
对于测试集中的每一个样例，不同的方法以样例视频第一帧作为初始图像生成对应的动态视频，其中运动向量由真值光流序列编码得到。表6.2 对比了不同方法在VoxCeleb1 人脸数据集上的定量结果，相比于对比方法本文研究在所有指标上均得到了明显提升，说明生成的图像序列与真值更为接近，这与前述图6.3 的定性对比实验结果相互佐证。进一步，表6.3 展示了多种方法在Sky Time-lapse 数据集上的定量结果，相比于MoCoGAN 和MDGAN，本文方法在PSNR 指标上分别获得了+6.05↑和+6.875↑的提高，在SSIM 指标上分别获得了+0.067↑和+0.094↑的增益，说明本文方法在像素级别的预测与真值更为接近。对于表征运动差异的Flow-MSE 指标，本文方法具有最小的运动误差，意味着在运动向量控制下生成的视频片段与真实片段具有更接近的运动状态。表6.4 进一步展示了Beach 数据集上不同方法的定量评估结果，本文方法相比主流对比方法在PSNR 和SSIM 均获得了显著提升，同时具有更小的Flow-MSE 度量，一致性地证明了本文方法能够生成与真值更接近的动态视频。此外，本文发现人脸数据集上的指标明显高于延时视频数据集，可能原因是人脸模式因较单一而更易建模，而富含多种语义目标的延时视频数据集对于研究的图像动态化任务则更具有挑战性。后续小节默认在Sky Time-lapse 延时视频数据集上进行相关实验。
为了评估不同方法在高质量视频生成数据集上的效果，本小节进一步在提出的QST数据集上进行了定量评估。如表6.5 所示，本文方法在所有指标上均取得了最好的结果，996运动约束下的图像序列生成浙江大学博士学位论文方法PSNR ↑SSIM ↑Flow-MSE ↓MDGAN20.7910.7831.470MoCoGAN23.5650.8241.406本文方法27.5440.8751.332即PSNR=27.544，SSIM=0.875 和Flow-MSE=1.332，证明了DVGNet 在高质量图像生成和视频运动预测方面的优越性。
总的来说，不同数据集上多个指标评估结果表明，本文研究方法相较于对比方法一致性地获得了更高的结果，生成的动态视频无论是在帧质量还是运动预测合理性层面均具有明显的优越性。
6.7.5人工评估实验上述定量实验中采用的PSNR、SSIM 和Flow-MSE 通常可以作为不同方法好坏的评估标准，但对于某些特殊样例可能会存在视觉感官很好但指标却很低的问题，亦或相反的情况。因此本小节进行了人工评估实验，在真实应用环境中进一步测试真人对不同方法生成视频的喜爱度。具体地，本实验的测试数据来源于Sky Time-lapse 测试数据集中随机选取的100 张初始图像，经由不同方法生成对应的动态延时视频。对于每个对比实验，选取两种不同方法生成的图像/视频依次展示给真人观看，判断给出“哪个图像/视频更好”的结果。对于每组对比雇佣30 个人进行测试，结果统计平均后得到最终的人工评估结果。
成视频中选择最中间的第16 帧图像），而第3 列针对视频进行测试（从视频质量和运动合理性两方面综合打分）。注意图像展示时仅显示2 秒，视频只播放一次。结果显示，本文方法和对比方法在图像和视频质量评估两项均明显低于真值（第2、3 和4 行），意味着动态视频生成任务仍然存在许多挑战，当前的算法仍不能生成非常逼真的图像和视频。尽管如此，本文方法相较于对比方法MoCoGAN 和MDGAN 具有明显的优势，在图像质量评估结果上分别具有94% 和97% 的喜爱度，而在视频质量评估一项更是分别获得了96% 和99% 的得分。
100浙江大学博士学位论文6运动约束下的图像序列生成对比方法帧图像质量得分视频质量得分MoCoGAN vs. 真值3 vs. 972 vs. 98MDGAN vs. 真值2 vs. 981 vs. 99本文方法vs. 真值12 vs. 887 vs. 93本文方法vs. MoCoGAN94 vs. 696 vs. 4本文方法vs. MDGAN97 vs. 399 vs. 16.7.6消融实验光流编码器模块影响本小节进行了针对光流编码器OFE 的消融实验以评估该模块对本文研究方法的有效性。如图6.6 所示，第1 行为真值动态视频在不同时间点对应的图像帧，第2 和3 行分别表示不使用和使用光流编码器模块时生成的结果。对结果进行分析可以发现，当不使用OFE 模块时，生成的图像序列因无法准确捕获云的纹理分布和运动信息导致场景几乎处于静止状态，随着时间的推移只是变得更模糊。当加入OFE 模块正常训练时，生成的视频不仅在图像质量上表现更好，同时也具有显著的运动过程，这说明光流编码器模块在逼真的动态视频生成中发挥了极大的作用。为了更直观地感受运动的变化，可对比红色矩形中对应时刻的图像在时序上的差异。 图6.6光流编码器模块定性影响可视化损失函数对结果影响本小节在Sky Time-lapse 数据集上进行了损失函数的定性和定量实验，以评估本研究采用的内容损失、运动损失和对抗训练损失每一项对模型影响。
如图6.7 所示，第1 行表示动态视频的真值，其余3 行为依次增加损失函数的生成结果，1016运动约束下的图像序列生成浙江大学博士学位论文图6.7损失函数对图像动态化的定性影响可视化损失函数组合PSNR ↑SSIM ↑Flow-MSE ↓LC28.7680.8971.624LC + LM29.3640.9061.509LC + LM + LAdv29.9170.9161.275带有时间标注的奇数列表示对应时间的图像帧，而偶数列可视化了两张相邻图像之间的光流信息。结果表明，当依次增加运动损失和对抗损失后，生成的视频质量更高且细节更清晰，极大地提高了模型的生成性能，同时表征运动的光流信息也会因运动损失和对抗损失的加持而愈发明显。此外，表6.7 显示对不同损失函数的定量剥离结果并得到了一致性的结论：模型指标结果随着运动损失和对抗损失的添加而逐渐提升，当所有损失函数同时使用时模型获得了最佳的度量结果，即PSNR=29.917，SSIM=0.916 和Flow-MSE=1.275。
6.7.7多样化生成测试除了从单个初始图像生成固定的动态视频外，多样化视频生成对于实际应用至关重要，比如在内容创作和数据集扩增等领域中，多样性生成能力是一个非常重要的关键指标。得益于归一化运动向量的设计，对固定的初始图像推理时可以在标准正态分布中随机采样运动向量表征，以其作为运动控制信号进行多样化的视频生成。图6.8 显示了本文方102浙江大学博士学位论文6运动约束下的图像序列生成图6.8多样化风景视频生成可视化图6.9多样化人脸视频生成可视化法在不同的运动向量控制下生成的多样化风景延时视频，其中第1 行以真实视频编码得到的运动向量作为控制输入，而第2 和3 行则来自于标准正态分布中的两次随机采样。结果表明，本文方法可以通过控制归一化运动向量的方式生成具有相同语义内容、不同运动形式的多样化视频，具有非常大的实际应用价值和潜力。
进一步，本小节也在VoxCeleb1 人脸数据集上进行了随机生成测试，如图6.9 所示，对于同一张输入人脸图像，本文方法能够在不同运动控制向量的控制下生成多样化的人脸运动视频。此外，图6.9 偶数列使用ARFlow 可视化方法展示了相邻图像之间的光流信息，可以看到本文方法具有很好的时序运动建模能力。
6.8本章小结从最开始的图像像素级任务到前两章的图像语义级任务，研究内容按信息理解程度逐渐加深，本章针对视频语义级研究中的运动约束下的图像序列生成问题进行了讨论，面向更高维度的视频语义级生成任务，对典型的运动纹理解耦的图像动态化视频生成任务进行了深入研究。尽管一些工作实现了视频生成目标，但受限于算法研究不成熟以及数据1036运动约束下的图像序列生成浙江大学博士学位论文集规模和质量较低等问题，这些方法存在视频生成质量较差或多样化不足的问题。另一方面，基于循环神经网络的时序运动建模是合理且有效的方案，但当时序较长时串行的运动预测会导致低效的模型推理速度，限制了模型的实际应用潜力。本章针对上述问题提出了有效的解决方案，在模型结构设计层面基于运动和纹理解耦的思想设计了端到端的动态视频生成框架，其包含光流编码器模块对运动信息进行归一化编码，以及动态视频生成器以并行的方式生成动态视频；在数据层面发布了一个大规模的高分辨率QST 数据集以支持并推动相关研究的发展。多个公开数据集上的定性和定量实验表明，本文方法相比于主流对比方法能够生成更高质量且更多样化的动态视频，在多个指标上获得了更高的结果，同时在真人评估测试中也更受青睐。
1047总结与展望7.1本文工作总结随着深度学习技术的发展和硬件算力的提高，以及泛娱乐创造、元宇宙技术、影视制作、安全对抗和政治防御等领域的应用需求，基于深度学习的条件式视觉内容生成技术受到学术界和工业界的广泛关注，吸引了越来越来的科研人员及应用者投入到该领域的研究中。本文聚焦于该领域中一系列条件式视觉内容生成任务的研究，根据对条件信息的理解程度将研究的典型问题分类讨论，分别进行了图像像素级的联合图像上色和超分辨率框架研究，图像语义级的基于区域注意力感知的人脸换脸和基于多信息源控制的多人脸驱动研究，以及视频语义级的运动纹理解耦的图像动态化研究。
本文首先从深度学习在图像生成领域的研究背景与应用需求出发，讨论了对条件式视觉内容生成研究的必要性和应用价值，针对一系列任务总结了相关工作的优点，并对存在的局限性和面临的挑战进行了客观分析与评价，最后对本文的研究内容和组织结构进行了概括性描述。具体地，本文的研究内容分为四个部分，第一部分针对旧照片修复的实际应用需求，提出了一个高效的联合图像上色和超分辨率框架。第二部分引入注意力机制对人脸五官进行精细建模，实现了身份一致性更优的人脸换脸结果。第三部分研究基于图像/音频信息源控制下的人脸驱动任务，采用人脸几何和纹理解耦思路设计了多信息源控制的多人脸驱动模型。第四部分扩展了时间维度，研究图像动态化视频生成问题，基于运动和纹理解耦的思想设计了端到端的动态延时视频生成框架，支持快速的并行视频解码和多样化测试生成以支撑实际应用需求。本小节对各部分主要研究内容及贡献总结如下：
局部结构感知的图像增强本文从局部结构感知的图像增强出发，针对典型的联合图像上色和超分辨率框架设计问题进行了研究。老照片修复应用一般采用串行的图像上色和图像超分辨率算法以获得视觉友好的增强图像，然而这种离散的流程存在效率低下且生成图像质量较差的问题。基于此，本文首次提出联合图像上色和超分应用任务，并设计了一个高效的联合图像上色和超分辨率端到端框架SCSNet 去实现该任务目标，同时在支持自动上色和参考上色两种模式的基础上实现了任意倍率图像放大。具体而言，针对上色任务，本研究基于交叉注意力设计了一个即插即用的金字塔阀控交叉注意力模块，不仅能1057总结与展望浙江大学博士学位论文更好地在多尺度上理解并聚合参考图像的颜色信息，同时因建模了全局位置之间的相关性而具有较强的解释性。针对超分任务，本研究精心设计了连续像素映射模块，以非常小的参数量和计算量实现了在连续空间中任意倍率的图像放大。大量定性和定量实验证明提出的方法具有明显更好的生成效果和运行效率优势，且生成的图像相比于主流方法具有更高的真实性。
多条件受限的图像纹理语义迁移本文从多条件受限的图像纹理语义迁移出发，针对典型的基于区域注意力感知的人脸换脸问题进行了研究。主流方法均基于以StyleGAN 为高分辨率生成器的逆映射方案进行改进，人脸的身份信息一般以全局向量的表征形式通过AdaIN 操作方法注入，这忽略了对局部身份信息的建模交互。基于此，本文提出了一种基于区域注意力感知的换脸方法RAFSwap 对人脸进行更精细的建模，其包含一个新颖的面部区域感知的局部分支和源特征适应的全局分支：前者通过引入Transformer 来有效地建模不重合的多尺度面部语义交互，设计了区域感知的身份编码器、基于注意力机制的特征交互模块和局部感知的身份投影器三个子模块，并在多尺度上进行了扩展；后者通过补充全局身份相关的线索来进一步保证生成图像的身份一致性。此外，基于向量的条件渐进式生成依赖于参数共享机制，这会导致换脸图像中不该发生变化的区域不可避免地存在像素差异，为了解决当前的逆映射框架会受到物体遮挡和背景扭曲的影响，本研究提出了一种无监督人脸软掩膜预测模块，提升模型在实际应用时的准确性与泛化性。多个公开基准上的多项指标测试结果表明，本文方法相较于最先进对比方法具有明显的优势，生成的图像在保证高保真度的基础上获得了更好的身份一致性表现，同时在人工评估实验中也更受青睐。
多条件受限的图像几何语义编辑本文从多条件受限的图像几何语义编辑出发，针对典型的基于多信息源控制的多人脸驱动问题进行了研究。针对当前大多数方法仅支持单人脸驱动建模以生成高质量的图像，或少数多人脸驱动模型在生成图像质量上却不令人满意的问题，本文提出了一种基于人脸几何和纹理信息解耦思想设计的多人脸驱动模型FReeNet，其包含一个精心设计的人脸关键点转换器分支以在几何空间上进行面部运动迁移，以及几何感知生成器分支生成人脸驱动图像，在保证图像生成质量的基础上实现了多人脸驱动任务目标。此外，受限于音频的缓慢发展导致基于音频控制的人脸驱动鲜有人问津，本研究基于FReeNet 框架实现了基于音频输入的多人脸驱动模型，设计了音频特征融合器和几何控制器模块分别进行音频特征提取及高效注入，同时提出了一个高质量的106浙江大学博士学位论文7总结与展望AnnVI 数据集以支持高分辨率的图像生成研究。广泛的定性和定量实验表明提出的方法很好地实现了多信息源控制的多人脸驱动任务，相比于主流方法在多个公开数据集上获得了更高的指标，人工评估实验也证明了所提方法生成的图像具有更高的质量和真实性。
运动约束下的图像序列生成本文从运动约束下的图像序列生成出发，针对典型的运动纹理解耦的图像动态化问题进行了研究。针对当前图像动态化方法不能生成高质量视频的问题，且因采用循环神经网络对时序运动进行显性建模导致推理时具有额外的模型参数和较慢的串行速度，本文基于运动和纹理解耦的思想设计了端到端的动态视频生成框架，在内容损失、运动损失和对抗损失的约束下实现了高质量和多样化的动态视频生成。
具体地，本研究针对运动解耦空间设计了光流编码器模块，用来将表示视频运动的光流信息编码为归一化向量，同时可通过随机运动向量采样的推理方式实现多样化的视频生成。
同时，基于解耦思想设计了双分支动态视频生成器，包含一个运动分支用来引入运动控制信号，以及一个内容分支用来提取纹理信息，在两者合力作用下生成目标动态视频。针对当前延时视频数据集质量较差的问题，本研究提出了一个大规模的高分辨率QST 数据集来支持该任务的研究，同时该数据集也可作为高分辨率图像和视频生成等任务的新基准。
广泛的实验证明了本文方法相对于主流方法的优越性，在人脸数据集和多个延时视频数据集上获得了更高的指标和更好的视觉效果，同时实现了单张初始图像生成多样性的动态延时视频目标，具有非常高的应用价值。
7.2未来工作展望本文围绕基于深度学习的条件式视觉内容生成这一问题，分别对像素级的图像上色、超分辨率和语义级的人脸换脸、人脸驱动以及图像动态化任务进行了深入研究，并针对当前挑战提出了有效解决方案，在该领域研究上取得了国内外领先的研究成果。然而，由于时间和技术发展的限制，本文的工作仍然存在一定的局限性，比如：(1) 联合图像上色和超分辨率框架研究中，本文虽然相较于主流方法显著地提升了模型效果和效率，但对于一些极端案例仍会出现上色不自然或局部不清晰的问题，沿用的基础模型块结构和训练策略都存在改进的空间。(2) 在基于区域注意力感知的人脸换脸研究中，由于StyleGAN 生成器采用预训练权重，会减低模型在图像分布差异较大的场景下的泛化能力，有效的联合微调训练有助于提高模型在应用图像域的生成结果。(3) 基于多信息源控制的多人脸驱动研究中，由于开源数据集存在背景简单或不变的问题，导致模型在复杂背景下泛化性较弱，更丰富的数据集能够进一步提高模型的驱动效果和应用潜力。(4) 运动纹理解耦的图像动态1077总结与展望浙江大学博士学位论文化研究中，本文采用隐性向量控制运动是当前算力与效果的权衡方案，结合高分辨率光流运动场显性地建模运动能够获得更好的效果与解释性，且能适应到高分辨率动态视频生成任务中。
总而言之，伴随着深度学习算法的发展和计算能力的提高，虽然条件式视觉内容生成研究在多个细分任务上已经取得了较大的进步，但仍存在许多研究难点与应用挑战亟待解决。结合学术界的研究现状和工业界的应用需求，本文计划未来从如下几个方面开展对条件式视觉内容生成的后续研究工作：
全身图像生成研究生成对抗网络针对单一固定模式可以生成逼真的图像，但由于人脸身份、身体衣着、发型和姿态的多样性变化，使用单一模型建模高清全身人体图像的生成仍然较困难。因此，有必要对生成模型基础架构进行重新设计以适应于更复杂模态的生成任务，同时对模型结构组件改进以实现高保真度图像生成。此外，使用多个特有的生成网络针对不同语义区域单独生成，然后在全局层面进行对齐和拼接，并对边界区域做匹配和图像和谐化等操作也可做为一种可能的基于局部到整体思路的实现方案。
换脸与人脸驱动统一模型结构设计人脸换脸和人脸驱动作为人脸研究领域最常见的两种应用，研究者一般会将其作为独立的任务进行研究和改进，设计的模型缺乏泛化到其它任务的能力导致应用价值受限。考虑到两个任务具有许多一致性属性，比如处理对象都是人脸图像、输入均为一张源图像和一张目标图像以及输出均为一张编辑后的高清图像，因此可以对统一的条件式人脸操作模型结构进行研究与设计，针对两者任务的相同点和相异点设计通用的处理模块，实现单一模型支持两种人脸任务的目标。
基础生成模型结构设计现有的图像生成方法一般采用StyleGAN 框架以实现高清的图像生成，然而，这种方式对训练算力要求极高且在移动端设备上很难进行实时的高清图像生成，同时在许多生成案例中存在明显的缺陷痕迹。因此，有必要对轻量型的基础生成模型结构进行设计，在实现加速运行且参数量更少的前提下保持或提升模型精度，以满足日益增加的移动端设备应用需求。
生成式模型收敛性研究以StyleGAN 为首的生成模型虽然能够生成高质量图像，但由于需要较长的训练时间导致模型在研究和应用层面需要面对高算力需求和模型迭代周期长的压力。因此，有必要对生成式模型的收敛性进行研究，期望更快的收敛性带来更短的实验周期和更高的实用价值，而对于这一问题的相关研究工作目前较少。具体地，未来的研究可以从模型结构改进优化、训练梯度分析、训练损失函数、训练策略和适配生成模型的优化器等多个方面进行研究。
108

参考文献
 Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton. Imagenet classification with deep con-volutional neural networks[J]. Advances in neural information processing systems, 2012.
25.
 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei. Imagenet: A large-scale hierarchical image database[C]//2009 IEEE conference on computer vision and pat-tern recognition. Ieee, 2009:248–255.
 Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep residual learning for imagerecognition[C]//Proceedings of the IEEE conference on computer vision and pattern recog-nition. 2016:770–778.
 中国互联网络信息中心. 第48 次中国互联网络发展状况统计报告[R]. : 中国互联网络信息中心, 2021.
 腾讯优图实验室腾讯研究院. Ai 生成内容发展报告2020[R]. : 腾讯科技（深圳）有限公司, 2020.
 前瞻经济学人. 中国ai 数字商业产业展望2021-2025[R]. : 前瞻经济学人, 2021.
 量子位. 2021 量子位十大前沿科技报告[R]. : 量子位, 2021.
 量子位. 虚拟数字人深度产业报告[R]. : 量子位, 2021.
 马天诣. 2030 年的元宇宙产业[R]. : 民生证券, 2021.
 Daniel J Felleman, David C Van Essen. Distributed hierarchical processing in the primatecerebral cortex[J]. Cerebral cortex (New York, NY: 1991), 1991. 1(1):1–47.
 David C Van Essen, Charles H Anderson, Daniel J Felleman.
Information process-ing in the primate visual system: an integrated systems perspective[J].
Science, 1992.
255(5043):419–423.
 Pankaj Bhowmik, Md Jubaer Hossain Pantho, Christophe Bobda. Bio-inspired smart visionsensor: toward a reconfigurable hardware modeling of the hierarchical processing in thebrain[J]. Journal of Real-Time Image Processing, 2021. 18(1):157–174.
109

参考文献
浙江大学博士学位论文 Chenyang Lei, Qifeng Chen. Fully automatic video colorization with self-regularization anddiversity[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2019:3753–3761.
 Satoshi Iizuka, Edgar Simo-Serra. Deepremaster: temporal source-reference attention net-works for comprehensive video enhancement[J]. ACM Transactions on Graphics (TOG),2019. 38(6):1–13.
 Jheng-Wei Su, Hung-Kuo Chu, Jia-Bin Huang.
Instance-aware image coloriza-tion[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition. 2020:7968–7977.
 Zezhou Cheng, Qingxiong Yang, Bin Sheng. Deep colorization[C]//Proceedings of theIEEE international conference on computer vision. 2015:415–423.
 Richard Zhang, Phillip Isola, Alexei A Efros. Colorful image colorization[C]//Europeanconference on computer vision. Springer, 2016:649–666.
 Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min Jin Chong, David Forsyth. Learningdiverse image colorization[C]//Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition. 2017:6837–6845.
 Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee. Enhanced deepresidual networks for single image super-resolution[C]//Proceedings of the IEEE conferenceon computer vision and pattern recognition workshops. 2017:136–144.
 Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu. Image super-resolution using very deep residual channel attention networks[C]//Proceedings of the Eu-ropean conference on computer vision (ECCV). 2018:286–301.
 Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu. Residual dense network forimage super-resolution[C]//Proceedings of the IEEE conference on computer vision andpattern recognition. 2018:2472–2481.
 Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham,Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang,et al.
Photo-realistic single image super-resolution using a generative adversarial net-work[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2017:4681–4690.
110浙江大学博士学位论文

参考文献
 Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao,Chen Change Loy.
Esrgan:
Enhanced super-resolution generative adversarial net-works[C]//Proceedings of the European conference on computer vision (ECCV) workshops.
2018:0–0.
 Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai Deng, YanwuXu, Mingkui Tan. Closed-loop matters: Dual regression networks for single image super-resolution[C]//Proceedings of the IEEE/CVF conference on computer vision and patternrecognition. 2020:5407–5416.
 Chang Chen, Zhiwei Xiong, Xinmei Tian, Zheng-Jun Zha, Feng Wu. Real-world imagedenoising with deep boosting[J]. IEEE transactions on pattern analysis and machine intel-ligence, 2019. 42(12):3071–3087.
 Kaixuan Wei, Ying Fu, Jiaolong Yang, Hua Huang. A physics-based noise formation modelfor extreme low-light raw denoising[C]//Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition. 2020:2758–2767.
 Ding Liu, Bihan Wen, Jianbo Jiao, Xianming Liu, Zhangyang Wang, Thomas S Huang.
Connecting image denoising and high-level vision tasks via deep learning[J]. IEEE Trans-actions on Image Processing, 2020. 29:3695–3706.
 Kaiming He, Jian Sun, Xiaoou Tang.
Single image haze removal using dark chan-nel prior[J].
IEEE transactions on pattern analysis and machine intelligence, 2010.
33(12):2341–2353.
 Yanyun Qu, Yizi Chen, Jingying Huang, Yuan Xie.
Enhanced pix2pix dehazing net-work[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2019:8160–8168.
 Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu,Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao.
Pre-trained image processing trans-former[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2021:12299–12310.
 Yinglong Wang, Yibing Song, Chao Ma, Bing Zeng. Rethinking image deraining via rainstreaks and vapors[C]//European Conference on Computer Vision. Springer, 2020:367–382.
111

参考文献
浙江大学博士学位论文 Yingjun Du, Jun Xu, Xiantong Zhen, Ming-Ming Cheng, Ling Shao. Conditional varia-tional image deraining[J]. IEEE Transactions on Image Processing, 2020. 29:6288–6301.
 Chenghao Chen, Hao Li. Robust representation learning with feedback for single imagederaining[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2021:7742–7751.
 Junsoo Lee, Eungyeup Kim, Yunsung Lee, Dongjun Kim, Jaehyuk Chang, Jaegul Choo.
Reference-based sketch image colorization using augmented-self reference and dense se-mantic correspondence[C]//Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 2020:5801–5810.
 Wenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu, Chen Fang,Fisher Yu, James Hays.
Texturegan: Controlling deep image synthesis with texturepatches[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-nition. 2018:8456–8465.
 Zhongyou Xu, Tingting Wang, Faming Fang, Yun Sheng, Guixu Zhang. Stylization-basedarchitecture for fast deep exemplar colorization[C]//Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition. 2020:9363–9372.
 Bo Zhang, Mingming He, Jing Liao, Pedro V Sander, Lu Yuan, Amine Bermak, Dong Chen.
Deep exemplar-based video colorization[C]//Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 2019:8052–8061.
 ManojKumar,DirkWeissenborn,NalKalchbrenner.
Colorizationtrans-former[C]//International Conference on Learning Representations. 2021.
 Falong Shen, Shuicheng Yan, Gang Zeng.
Neural style transfer via meta net-works[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-nition. 2018:8061–8069.
 Sunnie SY Kim, Nicholas Kolkin, Jason Salavon, Gregory Shakhnarovich. Deformablestyle transfer[C]//European Conference on Computer Vision. Springer, 2020:246–261.
 Dmytro Kotovenko, Matthias Wright, Arthur Heimbrecht, Bjorn Ommer. Rethinking styletransfer: From pixels to parameterized brushstrokes[C]//Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 2021:12196–12205.
 Guoqing Hao, Satoshi Iizuka, Kazuhiro Fukui. Image harmonization with attention-baseddeep feature modulation.[C]//BMVC. 2020.
112浙江大学博士学位论文

参考文献
 Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, Liqing Zhang.
Dovenet:
Deep image harmonization via domain verification[C]//Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020:8394–8403.
 Zonghui Guo, Dongsheng Guo, Haiyong Zheng, Zhaorui Gu, Bing Zheng, Junyu Dong. Im-age harmonization with transformer[C]//Proceedings of the IEEE/CVF International Con-ference on Computer Vision. 2021:14870–14879.
 Yijun Li, Sifei Liu, Jimei Yang, Ming-Hsuan Yang.
Generative face comple-tion[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2017:3911–3919.
 Jie Yang, Zhiquan Qi, Yong Shi. Learning to incorporate structure knowledge for imageinpainting[C]//Proceedings of the AAAI Conference on Artificial Intelligence. volume 34.
2020:12605–12612.
 Wentao Wang, Jianfu Zhang, Li Niu, Haoyu Ling, Xue Yang, Liqing Zhang. Parallel multi-resolution fusion network for image inpainting[C]//Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision. 2021:14559–14568.
 Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros.
Image-to-image translationwith conditional adversarial networks[C]//Proceedings of the IEEE conference on computervision and pattern recognition. 2017:1125–1134.
 Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros. Unpaired image-to-image transla-tion using cycle-consistent adversarial networks[C]//Proceedings of the IEEE internationalconference on computer vision. 2017:2223–2232.
 Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catan-zaro.
High-resolution image synthesis and semantic manipulation with conditionalgans[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2018:8798–8807.
 Rui Huang, Shu Zhang, Tianyu Li, Ran He. Beyond face rotation: Global and local percep-tion gan for photorealistic and identity preserving frontal view synthesis[C]//Proceedingsof the IEEE international conference on computer vision. 2017:2439–2448.
 Jie Cao, Yibo Hu, Hongwen Zhang, Ran He, Zhenan Sun. Learning a high fidelity poseinvariant model for high-resolution face frontalization[J]. Advances in neural informationprocessing systems, 2018. 31.
113

参考文献
浙江大学博士学位论文 Xin Ning, Fangzhe Nan, Shaohui Xu, Lina Yu, Liping Zhang. Multi-view frontal faceimage generation: a survey[J]. Concurrency and Computation: Practice and Experience,2020:e6147.
 Yi-Ting Cheng, Virginia Tzeng, Yu Liang, Chuan-Chang Wang, Bing-Yu Chen, Yung-YuChuang, Ming Ouhyoung. 3d-model-based face replacement in video. //SIGGRAPH’09:
Posters. 2009. 1–1.
 Yuval Nirkin, Iacopo Masi, Anh Tran Tuan, Tal Hassner, Gerard Medioni. On face segmen-tation, face swapping, and face perception[C]//2018 13th IEEE International Conference onAutomatic Face & Gesture Recognition (FG 2018). IEEE, 2018:98–105.
 Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Umé,Mr Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, et al. Deepfacelab: A simple, flexibleand extensible face swapping framework[J]. arXiv preprint arXiv:2005.05535, 2020.
 Yuhao Zhu, Qi Li, Jian Wang, Cheng-Zhong Xu, Zhenan Sun. One shot face swapping onmegapixels[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2021:4834–4844.
 Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen. Faceshifter: Towards highfidelity and occlusion aware face swapping[J]. arXiv preprint arXiv:1912.13457, 2019.
 Renwang Chen, Xuanhong Chen, Bingbing Ni, Yanhao Ge. Simswap: An eﬀicient frame-work for high fidelity face swapping[C]//Proceedings of the 28th ACM International Con-ference on Multimedia. 2020:2003–2011.
 Runze Xu, Zhiming Zhou, Weinan Zhang, Yong Yu. Face transfer with generative adver-sarial network[J]. arXiv preprint arXiv:1710.06090, 2017.
 Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, Chen Change Loy. Reenactgan: Learn-ing to reenact faces via boundary transfer[C]//Proceedings of the European conference oncomputer vision (ECCV). 2018:603–619.
 Albert Pumarola, Antonio Agudo, Aleix M Martinez, Alberto Sanfeliu, FrancescMoreno-Noguer.
Ganimation: Anatomically-aware facial animation from a single im-age[C]//Proceedings of the European conference on computer vision (ECCV). 2018:818–833.
114浙江大学博士学位论文

参考文献
 Olivia Wiles, A Koepke, Andrew Zisserman. X2face: A network for controlling face gen-eration using images, audio, and pose codes[C]//Proceedings of the European conferenceon computer vision (ECCV). 2018:670–686.
 Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe. Firstorder motion model for image animation[J]. Advances in Neural Information ProcessingSystems, 2019. 32.
 Yurui Ren, Ge Li, Yuanqi Chen, Thomas H Li, Shan Liu. Pirenderer: Controllable portraitimage generation via semantic neural rendering[C]//Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision. 2021:13759–13768.
 Hyeong-Seok Choi, Changdae Park, Kyogu Lee. From inference to generation: End-to-endfully self-supervised generation of human face from speech[C]//International Conferenceon Learning Representations. 2020.
 Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis,Dingzeyu Li. Makelttalk: speaker-aware talking-head animation[J]. ACM Transactionson Graphics (TOG), 2020. 39(6):1–15.
 Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan, Wenwu Zhu, Liang Lin.
Beautygan: Instance-level facial makeup transfer with deep generative adversarial net-work[C]//Proceedings of the 26th ACM international conference on Multimedia. 2018:645–653.
 Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, Shuicheng Yan. Psgan: Poseand expression robust spatial-aware gan for customizable makeup transfer[C]//Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020:5194–5202.
 Si Liu, Wentao Jiang, Chen Gao, Ran He, Jiashi Feng, Bo Li, Shuicheng Yan. Psgan++:
Robust detail-preserving makeup transfer and removal[J]. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2021.
 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherjilOzair, Aaron Courville, Yoshua Bengio. Generative adversarial nets[J]. Advances in neuralinformation processing systems, 2014. 27.
 Zhengwei Wang, Qi She, Tomas E Ward. Generative adversarial networks: A survey andtaxonomy[J]. arXiv preprint arXiv:1906.01529, 2019. 2.
115

参考文献
浙江大学博士学位论文 Abdul Jabbar, Xi Li, Bourahla Omar. A survey on generative adversarial networks: Vari-ants, applications, and training[J]. ACM Computing Surveys (CSUR), 2021. 54(8):1–49.
 Mehdi Mirza, Simon Osindero. Conditional generative adversarial nets[J]. arXiv preprintarXiv:1411.1784, 2014.
 Geoffrey E Hinton, Richard Zemel.
Autoencoders, minimum description length andhelmholtz free energy[J]. Advances in neural information processing systems, 1993. 6.
 Diederik P Kingma, Max Welling. Auto-encoding variational bayes[J]. arXiv preprintarXiv:1312.6114, 2013.
 Durk P Kingma, Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolu-tions[J]. Advances in neural information processing systems, 2018. 31.
 Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning[J]. Ad-vances in neural information processing systems, 2017. 30.
 Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models usinga laplacian pyramid of adversarial networks[C]. volume 28. 2015.
 Alec Radford, Luke Metz, Soumith Chintala. Unsupervised representation learning withdeep convolutional generative adversarial networks[C]//International Conference on Learn-ing Representations. 2016.
 Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen. Progressive growing of gans forimproved quality, stability, and variation[C]//International Conference on Learning Repre-sentations. 2018.
 Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena.
Self-attention gen-erative adversarial networks[C]//International conference on machine learning.
PMLR,2019:7354–7363.
 Andrew Brock, Jeff Donahue, Karen Simonyan. Large scale gan training for high fidelitynatural image synthesis[C]//International Conference on Learning Representations. 2019.
 Jeff Donahue, Karen Simonyan. Large scale adversarial representation learning[J]. Ad-vances in Neural Information Processing Systems, 2019. 32.
 Tero Karras, Samuli Laine, Timo Aila. A style-based generator architecture for generativeadversarial networks[C]//Proceedings of the IEEE/CVF conference on computer vision andpattern recognition. 2019:4401–4410.
116浙江大学博士学位论文

参考文献
 Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila.
Analyzing and improving the image quality of stylegan[C]//Proceedings of the IEEE/CVFconference on computer vision and pattern recognition. 2020:8110–8119.
 Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen,Timo Aila. Alias-free generative adversarial networks[J]. Advances in Neural InformationProcessing Systems, 2021. 34.
 Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner. Gradient-based learning ap-plied to document recognition[J]. Proceedings of the IEEE, 1998. 86(11):2278–2324.
 Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tinyimages[J]. 2009.
 Adam Coates, Andrew Ng, Honglak Lee. An analysis of single-layer networks in unsuper-vised feature learning[C]//Proceedings of the fourteenth international conference on artifi-cial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011:215–223.
 Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, Jianxiong Xiao.
Lsun: Construction of a large-scale image dataset using deep learning with humans in theloop[J]. arXiv preprint arXiv:1506.03365, 2015.
 Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang. Deep learning face attributes in thewild[C]//Proceedings of the IEEE international conference on computer vision. 2015:3730–3738.
 Arsha Nagrani, Joon Son Chung, Andrew Zisserman. Voxceleb: a large-scale speaker iden-tification dataset[J]. arXiv preprint arXiv:1706.08612, 2017.
 Joon Son Chung, Arsha Nagrani, Andrew Zisserman. Voxceleb2: Deep speaker recogni-tion[J]. arXiv preprint arXiv:1806.05622, 2018.
 Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha. Stargan v2: Diverse image synthe-sis for multiple domains[C]//Proceedings of the IEEE/CVF conference on computer visionand pattern recognition. 2020:8188–8197.
 Martin Arjovsky, Soumith Chintala, Léon Bottou. Wasserstein generative adversarial net-works[C]//International Conference on Machine Learning. PMLR, 2017:214–223.
 Aaron Van Oord, Nal Kalchbrenner, Koray Kavukcuoglu.
Pixel recurrent neural net-works[C]//International conference on machine learning. PMLR, 2016:1747–1756.
117

参考文献
浙江大学博士学位论文 Sebastian Nowozin, Botond Cseke, Ryota Tomioka. f-gan: Training generative neural sam-plers using variational divergence minimization[C]//Conference on Neural Information Pro-cessing Systems. 2016:271–279.
 Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen.
Improved techniques for training gans[C]//Conference on Neural Information ProcessingSystems. 2016:2234–2242.
 Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu. Seqgan: Sequence generative adversarialnets with policy gradient[C]//Proceedings of the AAAI conference on artificial intelligence.
volume 31. 2017.
 Alireza Makhzani, Brendan J Frey. Pixelgan autoencoders[J]. Advances in Neural Infor-mation Processing Systems, 2017. 30.
 Arnab Ghosh, Viveka Kulharia, Vinay P Namboodiri, Philip HS Torr, Puneet K Dokania.
Multi-agent diverse generative adversarial networks[C]//Proceedings of the IEEE confer-ence on computer vision and pattern recognition. 2018:8513–8521.
 Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, Wenjie Li. Mode regularizedgenerative adversarial networks[J]. arXiv preprint arXiv:1612.02136, 2016.
 Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira. On convergence and stability ofgans[J]. arXiv preprint arXiv:1705.07215, 2017.
 Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, Liqiang Wang. Improving the improvedtraining of wasserstein gans: A consistency term and its dual effect[J].
arXiv preprintarXiv:1803.01541, 2018.
 Jonas Adler, Sebastian Lunz. Banach wasserstein gan[J]. Advances in Neural InformationProcessing Systems, 2018. 31.
 Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo,Zhizhen Zhao, David Forsyth, Alexander G Schwing. Max-sliced wasserstein distance andits use for gans[C]//Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 2019:10648–10656.
 Litu Rout, Indranil Misra, S Manthira Moorthi, Debajyoti Dhar. S2a: Wasserstein ganwith spatio-spectral laplacian attention for multi-spectral band synthesis[C]//Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.
2020:188–189.
118浙江大学博士学位论文

参考文献
 Guo-Jun Qi. Loss-sensitive generative adversarial networks on lipschitz densities[J]. Inter-national Journal of Computer Vision, 2020. 128(5):1118–1140.
 Junjie Li, Junwei Zhang, Xiaoyu Gong, Shuai Lü. Evolutionary generative adversarial net-works with crossover based knowledge distillation[C]//2021 International Joint Conferenceon Neural Networks (IJCNN). IEEE, 2021:1–8.
 Ben Thorne, Lloyd Knox, Karthik Prabhu. A generative model of galactic dust emissionusing variational autoencoders[J]. Monthly Notices of the Royal Astronomical Society,2021. 504(2):2603–2613.
 Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, Jieping Ye. A review on generative ad-versarial networks: Algorithms, theory, and applications[J]. IEEE Transactions on Knowl-edge and Data Engineering, 2021.
 Junbo Zhao, Michael Mathieu, Yann LeCun.
Energy-based generative adversarial net-work[C]//International Conference on Learning Representations. 2017.
 Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-patrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell.
Progressive neural net-works[J]. arXiv preprint arXiv:1606.04671, 2016.
 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan NGomez, Łukasz Kaiser, Illia Polosukhin. Attention is all you need[C]//Advances in neu-ral information processing systems. 2017:5998–6008.
 Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, Stephen Paul Smolley.
Least squares generative adversarial networks[C]//Proceedings of the IEEE internationalconference on computer vision. 2017:2794–2802.
 Yang Song, Stefano Ermon. Generative modeling by estimating gradients of the data dis-tribution[J]. Advances in Neural Information Processing Systems, 2019. 32.
 Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet. Are ganscreated equal? a large-scale study[J]. Advances in neural information processing systems,2018. 31.
 Yang Song, Stefano Ermon. Improved techniques for training score-based generative mod-els[J]. Advances in neural information processing systems, 2020. 33:12438–12448.
119

参考文献
浙江大学博士学位论文 Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron C Courville.
Improved training of wasserstein gans[C]//Conference on Neural Information ProcessingSystems. 2017:5767–5777.
 Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida. Spectral normaliza-tion for generative adversarial networks[C]//International Conference on Learning Repre-sentations. 2018.
 Lars Mescheder, Andreas Geiger, Sebastian Nowozin. Which training methods for gans doactually converge?[C]//International conference on machine learning. PMLR, 2018:3481–3490.
 Yilun Du, Igor Mordatch. Implicit generation and modeling with energy based models[J].
Advances in Neural Information Processing Systems, 2019. 32.
 Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing fromstandard gan[J]. arXiv preprint arXiv:1807.00734, 2018.
 Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila.
Training generative adversarial networks with limited data[J]. Advances in Neural Infor-mation Processing Systems, 2020. 33:12104–12114.
 Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel. Info-gan: Interpretable representation learning by information maximizing generative adversar-ial nets[J]. Advances in neural information processing systems, 2016. 29.
 Olaf Ronneberger, Philipp Fischer, Thomas Brox.
U-net: Convolutional networks forbiomedical image segmentation[C]//International Conference on Medical image comput-ing and computer-assisted intervention. Springer, 2015:234–241.
 Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo.
Stargan: Unified generative adversarial networks for multi-domain image-to-image transla-tion[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2018:8789–8797.
 Animesh Karnewar, Oliver Wang. Msg-gan: Multi-scale gradients for generative adversar-ial networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2020:7799–7808.
120浙江大学博士学位论文

参考文献
 Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or. Designing anencoder for stylegan image manipulation[J]. ACM Transactions on Graphics (TOG), 2021.
40(4):1–14.
 Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro,Daniel Cohen-Or.
Encoding in style: a stylegan encoder for image-to-image transla-tion[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition. 2021:2287–2296.
 Zili Yi, Hao Zhang, Ping Tan, Minglun Gong. Dualgan: Unsupervised dual learning forimage-to-image translation[C]//Proceedings of the IEEE international conference on com-puter vision. 2017:2849–2857.
 Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz. Multimodal unsupervised image-to-image translation[C]//Proceedings of the European conference on computer vision (ECCV).
2018:172–189.
 Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, Ming-Hsuan Yang. Di-verse image-to-image translation via disentangled representations[C]//Proceedings of theEuropean conference on computer vision (ECCV). 2018:35–51.
 Ben Usman, Nick Dufour, Kate Saenko, Chris Bregler. Puppetgan: Cross-domain imagemanipulation by demonstration[C]//Proceedings of the IEEE/CVF International Conferenceon Computer Vision. 2019:9450–9458.
 Jiaze Sun, Binod Bhattarai, Tae-Kyun Kim. Matchgan: a self-supervised semi-supervisedconditional generative adversarial network[C]//Proceedings of the Asian Conference onComputer Vision. 2020.
 Zewei Sun, Shujian Huang, Hao-Ran Wei, Xin-yu Dai, Jiajun Chen. Generating diversetranslation by manipulating multi-head attention[C]//Proceedings of the AAAI Conferenceon Artificial Intelligence. volume 34. 2020:8976–8983.
 Deblina Bhattacharjee,Seungryong Kim,Guillaume Vizier,Mathieu Salzmann.
Dunit: Detection-based unsupervised image-to-image translation[C]//Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020:4787–4796.
 Lingke Kong, Chenyu Lian, Detian Huang, Yanle Hu, Qichao Zhou, et al. Breaking thedilemma of medical image-to-image translation[J]. Advances in Neural Information Pro-cessing Systems, 2021. 34.
121

参考文献
浙江大学博士学位论文 Aditya Deshpande, Jason Rock, David Forsyth.
Learning large-scale automatic imagecolorization[C]//Proceedings of the IEEE international conference on computer vision.
2015:567–575.
 Bo Li, Yu-Kun Lai, Matthew John, Paul L Rosin. Automatic example-based image coloriza-tion using location-aware cross-scale matching[J]. IEEE Transactions on Image Processing,2019. 28(9):4606–4619.
 Huiwen Chang, Ohad Fried, Yiming Liu, Stephen DiVerdi, Adam Finkelstein. Palette-basedphoto recoloring.[J]. ACM Trans. Graph., 2015. 34(4):139–1.
 Saeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal Mian, Fahad Shahbaz Khan, Ab-dul Wahab Muzaffar.
Image colorization: A survey and dataset[J].
arXiv preprintarXiv:2008.10774, 2020.
 Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, Xiaodong Liu. Language-basedimage editing with recurrent attentive models[C]//Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition. 2018:8721–8729.
 Hyunsu Kim, Ho Young Jhoo, Eunhyeok Park, Sungjoo Yoo. Tag2pix: Line art colorizationusing text tag with secat and changing loss[C]//Proceedings of the IEEE/CVF InternationalConference on Computer Vision. 2019:9056–9065.
 Changqing Zou, Haoran Mo, Chengying Gao, Ruofei Du, Hongbo Fu. Language-basedcolorization of scene sketches[J]. ACM Transactions on Graphics (TOG), 2019. 38(6):1–16.
 Gustav Larsson, Michael Maire, Gregory Shakhnarovich. Learning representations for au-tomatic colorization[C]//European conference on computer vision. Springer, 2016:577–593.
 Safa Messaoud, David Forsyth, Alexander G Schwing. Structural consistency and control-lability for diverse colorization[C]//Proceedings of the European Conference on ComputerVision (ECCV). 2018:596–612.
 Jialu Huang, Jing Liao, Sam Kwong. Semantic example guided image-to-image transla-tion[J]. IEEE Transactions on Multimedia, 2020.
 Ziyu Wan, Bo Zhang, Dongdong Chen, Jing Liao. Bringing old films back to life, 2022.
122浙江大学博士学位论文

参考文献
 Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. Image super-resolution usingdeep convolutional networks[J]. IEEE transactions on pattern analysis and machine intel-ligence, 2015. 38(2):295–307.
 Chao Dong, Chen Change Loy, Xiaoou Tang. Accelerating the super-resolution convolu-tional neural network[C]//European conference on computer vision. Springer, 2016:391–407.
 Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen Lu, Jie Zhou. Structure-preservingsuper resolution with gradient guidance[C]//Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 2020:7769–7778.
 Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, Jian Sun. Meta-sr:
A magnification-arbitrary network for super-resolution[C]//Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 2019:1575–1584.
 Yuval Nirkin, Yosi Keller, Tal Hassner. Fsgan: Subject agnostic face swapping and reen-actment[C]//Proceedings of the IEEE/CVF international conference on computer vision.
2019:7184–7193.
 Ryota Natsume, Tatsuya Yatagawa, Shigeo Morishima. Fsnet: An identity-aware generativemodel for image-based face swapping[C]//Asian Conference on Computer Vision. Springer,2018:117–132.
 Justus Thies, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger,Christian Theobalt. Real-time expression transfer for facial reenactment.[J]. ACM Trans.
Graph., 2015. 34(6):183–1.
 Pablo Garrido, Michael Zollhöfer, Dan Casas, Levi Valgaerts, Kiran Varanasi, PatrickPérez, Christian Theobalt. Reconstruction of personalized 3d face rigs from monocularvideo[J]. ACM Transactions on Graphics (TOG), 2016. 35(3):1–15.
 Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niess-ner, Patrick Pérez, Christian Richardt, Michael Zollhöfer, Christian Theobalt. Deep videoportraits[J]. ACM Transactions on Graphics (TOG), 2018. 37(4):1–14.
 Xianfang Zeng, Yusu Pan, Mengmeng Wang, Jiangning Zhang, Yong Liu. Realistic facereenactment via self-supervised disentangling of identity and pose[C]//Proceedings of theAAAI Conference on Artificial Intelligence. volume 34. 2020:12757–12764.
123

参考文献
浙江大学博士学位论文 Najmeh Sadoughi, Carlos Busso. Speech-driven expressive talking lips with conditionalsequential generative adversarial networks[J]. IEEE Transactions on Affective Computing,2019.
 Le Minh Ngo, Sezer Karaoglu, Theo Gevers, et al. Unified application of style transferfor face swapping and reenactment[C]//Proceedings of the Asian Conference on ComputerVision. 2020.
 Pablo Garrido, Levi Valgaerts, Hamid Sarmadi, Ingmar Steiner, Kiran Varanasi, PatrickPerez, Christian Theobalt. Vdub: Modifying face video of actors for plausible visual align-ment to a dubbed audio track[J]. 2015. 34(2):193–204.
 Amanda Cardoso Duarte, Francisco Roldan, Miquel Tubau, Janna Escur, Santiago Pas-cual, Amaia Salvador, Eva Mohedano, Kevin McGuinness, Jordi Torres, Xavier Giro-iNieto.
Wav2pix: Speech-conditioned face generation using generative adversarial net-works.[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech andSignal Processing (ICASSP). 2019:8633–8637.
 Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo. Learning to generate time-lapsevideos using multi-stage dynamic generative adversarial networks[C]//Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition. 2018:2364–2373.
 Masaki Saito, Eiichi Matsumoto, Shunta Saito. Temporal generative adversarial nets withsingular value clipping[C]//Proceedings of the IEEE international conference on computervision. 2017:2830–2839.
 Carl Vondrick, Hamed Pirsiavash, Antonio Torralba. Generating videos with scene dynam-ics[C]//Conference on Neural Information Processing Systems. 2016:613–621.
 Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz. Mocogan: Decomposing mo-tion and content for video generation[C]//Proceedings of the IEEE conference on computervision and pattern recognition. 2018:1526–1535.
 Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, Dahua Lin. Pose guidedhuman video generation[C]//Proceedings of the European Conference on Computer Vision(ECCV). 2018:201–216.
 Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang.
Flow-grounded spatial-temporal video prediction from still images[C]//Proceedings of the Eu-ropean Conference on Computer Vision (ECCV). 2018:600–615.
124浙江大学博士学位论文

参考文献
 Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa. Let there be color! joint end-to-endlearning of global and local image priors for automatic image colorization with simultane-ous classification[J]. ACM Transactions on Graphics (TOG), 2016. 35(4):1–11.
 Yun Cao, Zhiming Zhou, Weinan Zhang, Yong Yu. Unsupervised diverse colorization viagenerative adversarial networks[C]//Joint European conference on machine learning andknowledge discovery in databases. Springer, 2017:151–166.
 Patricia Vitoria, Lara Raad, Coloma Ballester. Chromagan: Adversarial picture colorizationwith semantic class distribution[C]//Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision. 2020:2445–2454.
 Chie Furusawa, Kazuyuki Hiroshiba, Keisuke Ogaki, Yuri Odagiri.
Comicolorization:
semi-automatic manga colorization[C]//SIGGRAPH Asia 2017 Technical Briefs. 2017:1–4.
 Tsai-Ho Sun, Chien-Hsun Lai, Sai-Keung Wong, Yu-Shuen Wang. Adversarial colorizationof icons based on contour and color conditions[C]//Proceedings of the 27th ACM Interna-tional Conference on Multimedia. 2019:683–691.
 Jiaojiao Zhao, Jungong Han, Ling Shao, Cees GM Snoek. Pixelated semantic coloriza-tion[J]. International Journal of Computer Vision, 2019:1–17.
 Peng Lu, Jinbei Yu, Xujun Peng, Zhaoran Zhao, Xiaojie Wang. Gray2colornet: Transfermore colors from reference image[C]//Proceedings of the 28th ACM International Confer-ence on Multimedia. 2020:3210–3218.
 Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, Lei Zhang. Toward real-world sin-gle image super-resolution: A new benchmark and a new model[C]//Proceedings of theIEEE/CVF International Conference on Computer Vision. 2019:3086–3095.
 Xuaner Zhang, Qifeng Chen, Ren Ng, Vladlen Koltun.
Zoom to learn, learn tozoom[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2019:3762–3770.
 Volker Blanz, Kristina Scherbaum, Thomas Vetter, Hans-Peter Seidel. Exchanging faces inimages[C]//Computer Graphics Forum. volume 23. Wiley Online Library, 2004:669–676.
 Dmitri Bitouk, Neeraj Kumar, Samreen Dhillon, Peter Belhumeur, Shree K Nayar. Faceswapping: automatically replacing faces in photographs. //ACM SIGGRAPH 2008 papers.
2008. 1–8.
125

参考文献
浙江大学博士学位论文 Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, Matthias Nießner.
Face2face: Real-time face capture and reenactment of rgb videos[C]//Proceedings of theIEEE conference on computer vision and pattern recognition. 2016:2387–2395.
 Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, Gang Hua. Towards open-set identitypreserving face synthesis[C]//Proceedings of the IEEE conference on computer vision andpattern recognition. 2018:6713–6722.
 Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou. In-domain gan inversion for real imageediting[C]//European conference on computer vision. Springer, 2020:592–608.
 Rameen Abdal, Yipeng Qin, Peter Wonka. Image2stylegan: How to embed images intothe stylegan latent space?[C]//Proceedings of the IEEE/CVF International Conference onComputer Vision. 2019:4432–4441.
 Rameen Abdal, Yipeng Qin, Peter Wonka. Image2stylegan++: How to edit the embeddedimages?[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2020:8296–8305.
 Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan,Gang Hua, Nenghai Yu.
A simple baseline for stylegan inversion[J].
arXiv preprintarXiv:2104.07661, 2021.
 Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou. Generative hierarchicalfeatures from synthesizing images[C]//Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition. 2021:4432–4442.
 Jia Li, Zhaoyang Li, Jie Cao, Xingguang Song, Ran He.
Faceinpainter: High fidelityface adaptation to heterogeneous domains[C]//Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 2021:5089–5098.
 Gary B Huang, Marwan Mattar, Tamara Berg, Eric Learned-Miller.
Labeled facesin the wild:
A database forstudying face recognition in unconstrained environ-ments[C]//Workshop on faces in’Real-Life’Images: detection, alignment, and recognition.
2008.
 Gary B Huang, Erik Learned-Miller. Labeled faces in the wild: Updates and new reportingprocedures[J]. Dept. Comput. Sci., Univ. Massachusetts Amherst, Amherst, MA, USA,Tech. Rep, 2014. 14(003).
126浙江大学博士学位论文

参考文献
 Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, Simon Baker. Multi-pie[J]. Im-age and Vision Computing, 2010. 28(5):807–813.
 Shuo Yang, Ping Luo, Chen-Change Loy, Xiaoou Tang. Wider face: A face detection bench-mark[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2016:5525–5533.
 Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, Qiang Zhou. Look at boundary:
A boundary-aware face alignment algorithm[C]//Proceedings of the IEEE conference oncomputer vision and pattern recognition. 2018:2129–2138.
 Jiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng, Xi Zhou. A deep regressionarchitecture with two-stage re-initialization for high performance facial landmark detec-tion[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2017:3317–3326.
 Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang. Style aggregated network for facial land-mark detection[C]//Proceedings of the IEEE Conference on Computer Vision and PatternRecognition. 2018:379–388.
 Xiaojie Guo, Siyuan Li, Jiawan Zhang, Jiayi Ma, Lin Ma, Wei Liu, Haibin Ling. Pfld: Apractical facial landmark detector[J]. arXiv preprint arXiv:1902.10859, 2019.
 Volker Blanz,Thomas Vetter.
A morphable model for the synthesis of 3dfaces[C]//Proceedings of the 26th annual conference on Computer graphics and interactivetechniques. 1999:187–194.
 Daniel Vlasic, Matthew Brand, Hanspeter Pfister, Jovan Popovic. Face transfer with multi-linear models. //ACM SIGGRAPH 2006 Courses. 2006. 24–es.
 Supasorn Suwajanakorn, Steven M Seitz, Ira Kemelmacher-Shlizerman.
Synthesizingobama: learning lip sync from audio[J]. ACM Transactions on Graphics (ToG), 2017.
36(4):1–13.
 Justus Thies, Michael Zollhöfer, Christian Theobalt, Marc Stamminger, Matthias Nießner.
Headon: Real-time reenactment of human portrait videos[J]. ACM Transactions on Graph-ics (TOG), 2018. 37(4):1–13.
 Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, Kun Zhou. Warp-guided gans forsingle-photo facial animation[J]. ACM Transactions on Graphics (TOG), 2018. 37(6):1–12.
127

参考文献
浙江大学博士学位论文 Xiaohan Jin,Ye Qi,Shangxuan Wu.
Cyclegan face-off[J].
arXiv preprintarXiv:1712.03451, 2017.
 Lingxiao Song, Zhihe Lu, Ran He, Zhenan Sun, Tieniu Tan. Geometry guided adversarialfacial expression synthesis[C]//Proceedings of the 26th ACM international conference onMultimedia. 2018:627–635.
 Kai Li, Feng Xu, Jue Wang, Qionghai Dai, Yebin Liu. A data-driven approach for facialexpression synthesis in video[C]//2012 IEEE Conference on Computer Vision and PatternRecognition. IEEE, 2012:57–64.
 Joon Son Chung, Amir Jamaludin, Andrew Zisserman. You said that?[J]. arXiv preprintarXiv:1705.02966, 2017.
 Tae-Hyun Oh,Tali Dekel,Changil Kim,Inbar Mosseri,William T Freeman,Michael Rubinstein, Wojciech Matusik.
Speech2face:
Learning the face behind avoice[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2019:7539–7548.
 Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, Michael J Black. Capture,learning, and synthesis of 3d speaking styles[C]//Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 2019:10101–10111.
 Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, Yong-Jin Liu. Audio-driven talking facevideo generation with natural head pose[J]. arXiv preprint arXiv:2002.10137, 2020.
 Yeqi Bai, Tao Ma, Lipo Wang, Zhenjie Zhang.
Speech fusion to face:
Bridgingthe gap between human’s vocal characteristics and facial imaging[J].
arXiv preprintarXiv:2006.05888, 2020.
 Guanzhong Tian, Yi Yuan, Yong Liu. Audio2face: Generating speech/face animation fromsingle audio with attention-based bidirectional lstm networks[C]//2019 IEEE internationalconference on Multimedia & Expo Workshops (ICMEW). IEEE, 2019:366–371.
 Tero Karras, Timo Aila, Samuli Laine, Antti Herva, Jaakko Lehtinen. Audio-driven fa-cial animation by joint end-to-end learning of pose and emotion[J]. ACM Transactions onGraphics (TOG), 2017. 36(4):1–12.
 KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, CV Jawahar. A lip sync expertis all you need for speech to lip generation in the wild[C]//Proceedings of the 28th ACMInternational Conference on Multimedia. 2020:484–492.
128浙江大学博士学位论文

参考文献
 Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, Hans Peter Graf. Condi-tional gan with discriminative filter generation for text-to-video synthesis.[C]//IJCAI. vol-ume 1. 2019:2.
 Yitong Li, Martin Min, Dinghan Shen, David Carlson, Lawrence Carin. Video generationfrom text[C]//Proceedings of the AAAI Conference on Artificial Intelligence. volume 32.
2018.
 Lijie Fan, Wenbing Huang, Chuang Gan, Junzhou Huang, Boqing Gong.
Controllableimage-to-video translation: A case study on facial expression generation[C]//Proceedingsof the AAAI Conference on Artificial Intelligence. volume 33. 2019:3510–3517.
 Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada. Hierarchicalvideo generation from orthogonal information: Optical flow and texture[C]//Proceedingsof the AAAI Conference on Artificial Intelligence. volume 32. 2018.
 Michaël Mathieu, Camille Couprie, Yann LeCun. Deep multi-scale video prediction beyondmean square error[C]//4th International Conference on Learning Representations, ICLR2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. 2016.
 Sandra Aigner, Marco Körner.
Futuregan: Anticipating the future frames of video se-quences using spatio-temporal 3d convolutions in progressively growing gans[J]. ISPRS- International Archives of the Photogrammetry, Remote Sensing and Spatial InformationSciences, 09 2019. XLII-2/W16:3–11.
 Seonghyeon Nam, Chongyang Ma, Menglei Chai, William Brendel, Ning Xu, Seon JooKim. End-to-end time-lapse video synthesis from a single outdoor image[C]//Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019:1409–1418.
 Haoye Cai, Chunyan Bai, Yu-Wing Tai, Chi-Keung Tang. Deep video generation, predictionand completion of human action sequences[C]//Proceedings of the European Conference onComputer Vision (ECCV). 2018:366–382.
 Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, BryanCatanzaro. Video-to-video synthesis[C]//Advances in Neural Information Processing Sys-tems (NeurIPS). 2018.
129

参考文献
浙江大学博士学位论文 Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang.
Video generation from single semantic label map[C]//Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition. 2019:3733–3742.
 Xiaodan Liang, Lisa Lee, Wei Dai, Eric P Xing. Dual motion gan for future-flow embeddedvideo prediction[C]//proceedings of the IEEE international conference on computer vision.
2017:1744–1752.
 Hao Zhu, Man-Di Luo, Rui Wang, Ai-Hua Zheng, Ran He. Deep audio-visual learning: Asurvey[J]. International Journal of Automation and Computing, 2021. 18(3):351–376.
 Jiangning Zhang, Chao Xu, Jian Li, Yue Han, Yabiao Wang, Ying Tai, Yong Liu. Sc-snet: An eﬀicient paradigm for learning simultaneously image colorization and super-resolution[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022.
 Mingming He, Dongdong Chen, Jing Liao, Pedro V Sander, Lu Yuan. Deep exemplar-basedcolorization[J]. ACM Transactions on Graphics (TOG), 2018. 37(4):1–16.
 Xun Huang, Serge Belongie. Arbitrary style transfer in real-time with adaptive instancenormalization[C]//Proceedings of the IEEE International Conference on Computer Vision.
2017:1501–1510.
 Karen Simonyan, Andrew Zisserman. Very deep convolutional networks for large-scaleimage recognition[C]//International Conference on Learning Representations. 2015.
 Maria-Elena Nilsback, Andrew Zisserman. Automated flower classification over a largenumber of classes[C]//2008 Sixth Indian Conference on Computer Vision, Graphics & Im-age Processing. IEEE, 2008:722–729.
 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra-manan, Piotr Dollár, C Lawrence Zitnick.
Microsoft coco: Common objects in con-text[C]//European conference on computer vision. Springer, 2014:740–755.
 Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutionalneural networks applied to visual document analysis[C]//Icdar. volume 3. 2003.
 Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary De-Vito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer. Automatic differentiationin pytorch[J]. 2017.
130浙江大学博士学位论文

参考文献
 Diederik P. Kingma, Jimmy Ba. Adam: A method for stochastic optimization[C]//3rd In-ternational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,May 7-9, 2015, Conference Track Proceedings. 2015.
 Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Ky-rola, Andrew Tulloch, Yangqing Jia, Kaiming He. Accurate, large minibatch sgd: Trainingimagenet in 1 hour[J]. arXiv preprint arXiv:1706.02677, 2017.
 Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, Richard Socher. A closer look atdeep learning heuristics: Learning rate restarts, warmup and distillation[J]. arXiv preprintarXiv:1810.13243, 2018.
 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochre-iter. Gans trained by a two time-scale update rule converge to a local nash equilibrium[J].
Advances in neural information processing systems, 2017. 30.
 Emin Zerman, Aakanksha Rana, Aljosa Smolic. Colornet-estimating colorfulness in nat-ural images[C]//2019 IEEE International Conference on Image Processing (ICIP). IEEE,2019:3791–3795.
 Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli. Image quality assessment:
from error visibility to structural similarity[J]. IEEE transactions on image processing,2004. 13(4):600–612.
 Roman Zeyde, Michael Elad, Matan Protter.
On single image scale-up using sparse-representations[C]//International conference on curves and surfaces. Springer, 2010:711–730.
 David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik. A database of human seg-mented natural images and its application to evaluating segmentation algorithms and mea-suring ecological statistics[C]//Proceedings Eighth IEEE International Conference on Com-puter Vision. ICCV 2001. volume 2. IEEE, 2001:416–423.
 Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang. Ntire2017 challenge on single image super-resolution: Methods and results[C]//Proceedings ofthe IEEE conference on computer vision and pattern recognition workshops. 2017:114–125.
 Shuai Yang, Kai Qiao. Shapeediter: a stylegan encoder for face swapping[J]. arXiv preprintarXiv:2106.13984, 2021.
131

参考文献
浙江大学博士学位论文 Zhiliang Xu, Xiyu Yu, Zhibin Hong, Zhen Zhu, Junyu Han, Jingtuo Liu, Errui Ding, XiangBai. Facecontroller: Controllable attribute editing for face in the wild[J]. arXiv preprintarXiv:2102.11464, 2021.
 Yuhan Wang, Xu Chen, Junwei Zhu, Wenqing Chu, Ying Tai, Chengjie Wang, Jilin Li,Yongjian Wu, Feiyue Huang, Rongrong Ji. Hififace: 3d shape and semantic prior guidedhigh fidelity face swapping[J]. arXiv preprint arXiv:2106.09965, 2021.
 Xun Huang, Serge Belongie. Arbitrary style transfer in real-time with adaptive instancenormalization[C]//Proceedings of the IEEE International Conference on Computer Vision.
2017:1501–1510.
 Chao Xu, Jiangning Zhang, Miao Hua, Qian He, Zili Yi, Yong Liu. Region-aware faceswapping[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2022:7632–7641.
 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXivpreprint arXiv:2010.11929, 2020.
 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted win-dows[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision.
2021:10012–10022.
 Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan,Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda. Visual transformers:
Token-based image representation and processing for computer vision[J]. arXiv preprintarXiv:2006.03677, 2020.
 Jiangning Zhang, Chao Xu, Jian Li, Wenzhou Chen, Yabiao Wang, Ying Tai, Shuo Chen,Chengjie Wang, Feiyue Huang, Yong Liu. Analogous to evolutionary algorithm: Designinga unified sequence model[C]. volume 34. 2021:26674–26688.
 Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang. Bisenet:
Bilateral segmentation network for real-time semantic segmentation[C]//Proceedings of theEuropean conference on computer vision (ECCV). 2018:325–341.
132浙江大学博士学位论文

参考文献
 Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka. Sean: Image synthesis with seman-tic region-adaptive normalization[C]//Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition. 2020:5104–5113.
 Rameen Abdal, Peihao Zhu, Niloy J Mitra, Peter Wonka. Labels4free: Unsupervised seg-mentation using stylegan[C]//Proceedings of the IEEE/CVF International Conference onComputer Vision. 2021:13970–13979.
 Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou. Arcface: Additive angular mar-gin loss for deep face recognition[C]//Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition. 2019:4690–4699.
 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. The unrea-sonable effectiveness of deep features as a perceptual metric[C]//Proceedings of the IEEEconference on computer vision and pattern recognition. 2018:586–595.
 Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies,Matthias Nießner.
Faceforensics++:
Learning to detect manipulated facial im-ages[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision.
2019:1–11.
 Faceswap. https://github.com/MarekKowalski/FaceSwap/, 2018. Accessed: 2018-10-29.
 Deepfakes github. https://github.com/deepfakes/faceswap/, 2018. Accessed: 2018-10-29.
 Justus Thies, Michael Zollhöfer, Matthias Nießner. Deferred neural rendering: Image syn-thesis using neural textures[J]. ACM Transactions on Graphics (TOG), 2019. 38(4):1–12.
 Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li,Wei Liu. Cosface: Large margin cosine loss for deep face recognition[C]//Proceedings ofthe IEEE conference on computer vision and pattern recognition. 2018:5265–5274.
 Jiangning Zhang, Chao Xu, Xiangrui Zhao, Liang Liu, Yong Liu, Jinqiang Yao, ZaishengPan. Learning hierarchical and eﬀicient person re-identification for robotic navigation[J].
International Journal of Intelligent Robotics and Applications, 2021. 5(2):104–118.
 Nataniel Ruiz, Eunji Chong, James M Rehg. Fine-grained head pose estimation withoutkeypoints[C]//Proceedings of the IEEE conference on computer vision and pattern recog-nition workshops. 2018:2074–2083.
 Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, Xin Tong.
Accurate3d face reconstruction with weakly-supervised learning: From single image to image133

参考文献
浙江大学博士学位论文set[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition Workshops. 2019:0–0.
 Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao. Joint face detection and alignmentusing multitask cascaded convolutional networks[J]. IEEE Signal Processing Letters, 2016.
23(10):1499–1503.
 Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He.
Non-local neural net-works[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2018:7794–7803.
 Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, Chi-Keung Tang. Ladn: Localadversarial disentangling network for facial makeup and de-makeup[C]//Proceedings of theIEEE/CVF International Conference on Computer Vision. 2019:10481–10490.
 Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, Victor Lempitsky. Coordinate-based texture inpainting for pose-guided human image generation[C]//Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019:12135–12144.
 Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov,Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, et al.
Textured neural avatars[C]//Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 2019:2387–2397.
 Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman. Deep face recognition[J]. 2015.
 Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, Luc Van Gool. Pose guidedperson image generation[J]. Advances in neural information processing systems, 2017. 30.
 Yuqian Zhou, Bertram Emil Shi. Photorealistic facial expression synthesis by the con-ditional difference adversarial autoencoder[C]//2017 seventh international conference onaffective computing and intelligent interaction (ACII). IEEE, 2017:370–376.
 Yibo Hu, Xiang Wu, Bing Yu, Ran He, Zhenan Sun. Pose-guided photorealistic face rota-tion[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.
2018:8398–8406.
 Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia Zhu, Jian Yin.
Soft-gatedwarping-gan for pose-guided person image synthesis[J]. Advances in neural informationprocessing systems, 2018. 31.
134浙江大学博士学位论文

参考文献
 Hui Ding, Kumar Sricharan, Rama Chellappa. Exprgan: Facial expression editing withcontrollable expression intensity[C]//Proceedings of the AAAI Conference on Artificial In-telligence. volume 32. 2018.
 Chen Cao, Yanlin Weng, Stephen Lin, Kun Zhou. 3d shape regression for real-time facialanimation[J]. ACM Transactions on Graphics (TOG), 2013. 32(4):1–10.
 Ayush Tewari, Michael Zollhöfer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim,Patrick Pérez, Christian Theobalt.
Self-supervised multi-level face model learning formonocular reconstruction at over 250 hz[C]//Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition. 2018:2549–2559.
 Michael Zollhöfer, Justus Thies, Pablo Garrido, Derek Bradley, Thabo Beeler, PatrickPérez, Marc Stamminger, Matthias Nießner, Christian Theobalt. State of the art on monoc-ular 3d face reconstruction, tracking, and applications[C]//Computer Graphics Forum. vol-ume 37. Wiley Online Library, 2018:523–550.
 Luming Ma,Zhigang Deng.
Real-time hierarchical facial performance cap-ture[C]//Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphicsand Games. 2019:1–10.
 Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu, Yong Liu,Yu Ding, Changjie Fan. Freenet: Multi-identity face reenactment[C]//Proceedings of theIEEE/CVF conference on computer vision and pattern recognition. 2020:5326–5335.
 Florian Schroff, Dmitry Kalenichenko, James Philbin. Facenet: A unified embedding forface recognition and clustering[C]//Proceedings of the IEEE conference on computer visionand pattern recognition. 2015:815–823.
 Justin Johnson, Alexandre Alahi, Li Fei-Fei. Perceptual losses for real-time style transferand super-resolution[C]//European conference on computer vision. Springer, 2016:694–711.
 Jiangning Zhang, Liang Liu, Zhucun Xue, Yong Liu. Apb2face: Audio-guided face reen-actment with auxiliary pose and blink signals[C]//ICASSP 2020-2020 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020:4402–4406.
 Jiangning Zhang, Xianfang Zeng, Chao Xu, Yong Liu. Real-time audio-guided multi-facereenactment[J]. IEEE Signal Processing Letters, 2021. 29:1–5.
135

参考文献
浙江大学博士学位论文 Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, Zhangyang Wang.
Autogan-distiller: Searching to compress generative adversarial networks[J]. arXiv preprintarXiv:2006.08198, 2020.
 Face++. https://www.faceplusplus.com/attributes/, 2020. Accessed September 16, 2020.
 Lindasalwa Muda, Mumtaj Begam, Irraivan Elamvazuthi. Voice recognition algorithmsusing mel frequency cepstral coeﬀicient (mfcc) and dynamic time warping (dtw) tech-niques[J]. arXiv preprint arXiv:1003.4083, 2010.
 Oliver Langner, Ron Dotsch, Gijsbert Bijlstra, Daniel HJ Wigboldus, Skyler T Hawk,AD Van Knippenberg. Presentation and validation of the radboud faces database[J]. Cog-nition and emotion, 2010. 24(8):1377–1388.
 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. The unrea-sonable effectiveness of deep features as a perceptual metric[C]//Proceedings of the IEEEconference on computer vision and pattern recognition. 2018:586–595.
 北京清博智能科技有限公司. 元宇宙与虚拟数字人[R]. : 北京清博智能科技有限公司, 2022.
 Baoyang Chen, Wenmin Wang, Jinzhuo Wang. Video imagination from a single imagewith transformation generation[C]//Proceedings of the on Thematic Workshops of ACMMultimedia 2017. 2017:358–366.
 Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, Dimitris Metaxas. Learning to forecastand refine residual motion for image-to-video generation[C]//Proceedings of the Europeanconference on computer vision (ECCV). 2018:387–403.
 Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, Jian Yin. Fw-gan: Flow-navigated warping gan for video virtual try-on[C]//Proceedings of the IEEE/CVFInternational Conference on Computer Vision. 2019:1161–1170.
 Yaohui Wang, Piotr Bilinski, Francois Bremond, Antitza Dantcheva. Imaginator: Condi-tional spatio-temporal gan for video generation[C]//Proceedings of the IEEE/CVF WinterConference on Applications of Computer Vision. 2020:1160–1169.
 Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, Honglak Lee.
Learning to generate long-term future via hierarchical prediction[C]//International Con-ference on Machine Learning. PMLR, 2017:3560–3569.
136浙江大学博士学位论文

参考文献
 Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, Jürgen Schmidhuber.
Lstm: A search space odyssey[J]. IEEE transactions on neural networks and learning sys-tems, 2016. 28(10):2222–2232.
 Jiangning Zhang, Chao Xu, Liang Liu, Mengmeng Wang, Xia Wu, Yong Liu, YunliangJiang. Dtvnet: Dynamic time-lapse video generation via single still image[C]//EuropeanConference on Computer Vision. Springer, 2020:300–315.
 Pengpeng Liu, Michael Lyu, Irwin King, Jia Xu.
Selflow: Self-supervised learning ofoptical flow[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2019:4571–4580.
 Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo,Chengjie Wang, Jilin Li, Feiyue Huang. Learning by analogy: Reliable supervision fromtransformations for unsupervised optical flow estimation[C]//Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 2020:6489–6498.
 Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri. Learningspatiotemporal features with 3d convolutional networks[C]//Proceedings of the IEEE inter-national conference on computer vision. 2015:4489–4497.
 Adrian Bulat, Georgios Tzimiropoulos. How far are we from solving the 2d & 3d facealignment problem?(and a dataset of 230,000 3d facial landmarks)[C]//Proceedings of theIEEE International Conference on Computer Vision. 2017:1021–1030.
137

申请人简历
学习经历:
• 2020 年3 月-2020 年9 月，浙江大学，控制科学与工程学院，电子信息专业，硕转博• 2017 年9 月-2020 年3 月，浙江大学，控制科学与工程学院，控制科学与工程专业，硕士• 2013 年9 月-2017 年6 月，武汉大学，电子信息学院，测控技术与仪器，工学学士研究方向:
• 大类：计算机视觉、生成对抗网络• 小类：图像上色、图像超分辨率、人脸换脸、人脸驱动、图像动态化获奖情况:
• 2021 年，优秀博士中期考核，浙江大学• 2020 年，优秀研究生，浙江大学• 2019 年，Discover ZJU 摄影比赛第二名，浙江大学• 2019 年，航天人工智能创新挑战赛三等奖，中国航天科工集团第三研究院• 2018 年，优秀研究生，浙江大学实习经历:
• 2020 年12 月-2022 年06 月，腾讯优图实验室（上海），算法研究员• 2020 年06 月-2020 年10 月，华为2012 中央媒体技术院（杭州），算法研究员• 2019 年07 月-2019 年09 月，腾讯光子实验室（深圳），算法研究员• 2019 年04 月-2019 年06 月，网易伏羲人工智能实验室（杭州），算法研究员139

攻读博士学位期间的主要学术成果
已发表及录用的一作（共一）论文:
1. Jiangning Zhang, Chao Xu, Jian Li, et al. SCSNet: An Eﬀicient Paradigm for Learn-ing Simultaneously Image Colorization and Super-Resolution[C]. Proceedings of the AAAIConference on Artificial Intelligence (AAAI), 2022.
（中国计算机学会推荐A 类会议，本文第三章）2. Chao Xu*, Jiangning Zhang*, Miao Hua, et al. Region-Aware Face Swapping[C]. Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2022.（中国计算机学会推荐A 类会议，本文第四章）3. Jiangning Zhang, Chao Xu, Xiangrui Zhao, et al. Learning Hierarchical and Eﬀicient Per-son Re-Identification for Robotic Navigation[J]. International Journal of Intelligent Roboticsand Applications, vol. 5, pp. 104-118, 2021.（EI，Best Paper Reward，本文第四章）4. Jiangning Zhang, Chao Xu, Jian Li, et al. Analogous to Evolutionary Algorithm: De-signing a Unified Sequence Model[C]. Advances in Neural Information Processing Systems(NeurIPS), 2021.（中国计算机学会推荐A 类会议，本文第四章）5. Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, et al. FReeNet: Multi-Identity FaceReenactment[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), 2020.（中国计算机学会推荐A 类会议，本文第五章）6. Jiangning Zhang, Liang Liu, Zhucun Xue, et al. APB2Face: Audio-Guided Face Reen-actment with Auxiliary Pose and Blink Signals[C]. ICASSP 2020-2020 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), 2020.（中国计算机学会推荐A 类会议，本文第五章）7. Jiangning Zhang, Xianfang Zeng, Chao Xu, et al. Real-Time Audio-Guided Multi-FaceReenactment[J]. IEEE Signal Processing Letters, vol. 29, pp. 1-5, 2022.（中科院SCI 期刊2 区，本文第五章）141

攻读博士学位期间的主要学术成果
浙江大学博士学位论文8. Jiangning Zhang, Chao Xu, Liang Liu, et al. DTVNet: Dynamic Time-Lapse Video Gen-eration via Single Still Image[C]. Proceedings of the European Conference on ComputerVision (ECCV), 2020.（中国计算机学会推荐B 类会议，本文第六章）9. Chao Xu*, Jiangning Zhang*, Yue Han, et al. Designing One Unified Framework forHigh-Fidelity Reenactment and Swapping[C]. Proceedings of the European Conference onComputer Vision (ECCV), 2022.（中国计算机学会推荐B 类会议）10. Tianxin Huang*, Jiangning Zhang*, Jun Chen, et al. Resolution-free Point Cloud Sam-pling Network with Data Distillation[C]. Proceedings of the European Conference on Com-puter Vision (ECCV), 2022.（中国计算机学会推荐B 类会议）已发表及录用的论文:
1. Liang Liu, Yong Liu, and Jiangning Zhang. Learning-Based Hand Motion Capture andUnderstanding in Assembly Process[J]. IEEE Transactions on Industrial Electronics (TIE),2018, 66(12): 9703–9712.（中国自动化学会推荐A 类期刊）2. Chao Xu, Jiangning Zhang, Mengmeng Wang, et al. Multi-level Spatial-temporal FeatureAggregation for Video Object Detection[J]. IEEE Transactions on Circuits and Systems forVideo Technology (TCSVT), 2022.（中国自动化学会推荐A 类期刊）3. Liang Liu, Jiangning Zhang, Ruifei He, et al. Learning by Analogy: Reliable Supervisionfrom Transformations for Unsupervised Optical Flow Estimation[C]. Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.（中国计算机学会推荐A 类会议）4. Tianxin Huang, Xuemeng Yang, Jiangning Zhang, et al. Learn to Train a Point Cloud Re-construction Network without Matching Prior[C]. Proceedings of the European Conferenceon Computer Vision (ECCV), 2022.（中国计算机学会推荐B 类会议）5. Xianfang Zeng, Yusu Pan, Mengmeng Wang, Jiangning Zhang, et al. Realistic Face Reen-actment via Self-Supervised Disentangling of Identity and Pose[C]. Proceedings of theAAAI Conference on Artificial Intelligence (AAAI), 2020.（中国计算机学会推荐A 类会议）6. Haohan Wang, et al. Iterative Few-shot Semantic Segmentation from Image Label Text[C].
142浙江大学博士学位论文

攻读博士学位期间的主要学术成果
International Joint Conference on Artificial Intelligence (IJCAI), 2022.（中国计算机学会推荐A 类会议）7. Tianxin Huang, et al. RFNet: Recurrent Forward Network for Dense Point Cloud Com-pletion[C]. Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV), 2021.（中国计算机学会推荐A 类会议）8. Tian, Guanzhong, et al. Adding Before Pruning: Sparse Filter Fusion for Deep Convolu-tional Neural Networks via Auxiliary Attention[J]. IEEE Transactions on Neural Networksand Learning Systems (TNNLS), 2020.（中科院SCI 期刊1 区）投递中的论文:
1. Jiangning Zhang, Xiangtai Li, Yabiao Wang, et al. EAFormer: Improving Vision Trans-former Inspired by Evolutionary Algorithm. International Journal of Computer Vision (IJCV).
（中国计算机学会推荐A 类期刊）2. Yufei Liang*, Jiangning Zhang*, Yong Liu, et al. Omni-Frequency Channel-SelectionRepresentations for Unsupervised Anomaly Detection. IEEE Transactions on Image Pro-cessing (TIP).（中国计算机学会推荐A 类期刊）3. Xiangtai Li, Jiangning Zhang, Yibo Yang, et al. SFNet: Faster and Accurate SemanticSegmentation viaSemantic Flow. International Journal of Computer Vision (IJCV).（中国计算机学会推荐A 类期刊）4. Chao Xu, Jiangning Zhang, Junwei Zhu, et al. EmotionCLIP: Text-guided Audio-drivenEmotional Face Generation. Advances in Neural Information Processing Systems (NeurIPS).
（中国计算机学会推荐A 类会议）5. Tianxin Huang, Zhonggan Ding, Jiangning Zhang, et al. Contrastive Adversarial PointCloud Reconstruction Loss. Advances in Neural Information Processing Systems (NeurIPS).
（中国计算机学会推荐A 类会议）6. Tianxin Huang, Jiangning Zhang, Jun Chen, et al. 3QNet: 3D Point Cloud GeometryQuantization Compression Network. SIGGRAPH Asia.
（中国计算机学会推荐A 类会议）发明专利及软著:
143

攻读博士学位期间的主要学术成果
浙江大学博士学位论文1. 发明专利: 一种基于深度学习的传送带上物料检测系统，CN201810300347.6。
（已授权，除导师外第一完成人）2. 发明专利: 一种基于深度学习的电梯内异常行为检测系统，CN201810833881.3。
（已授权，除导师外第一完成人）3. 发明专利: 一种基于生成对抗机制的多人表情迁移方法，CN202010494513.8。
（已授权，除导师外第一完成人，本文第五章）4. 发明专利: 一种基于生成对抗机制的多样化动态延时视频生成方法，CN202010795760.1。
（已授权，除导师外第二完成人，本文第六章）5. 发明专利: 一种基于深度学习的流水线人员规范操作检测系统，CN201810296798.7。
（已授权，除导师外第二完成人）6. 发明专利: 一种基于对抗学习的灰度图像同时上色超分方法，CN202110925769.4。
（实质审查中，除导师外第一完成人，本文第三章）7. 发明专利: 一种基于深度学习的音频生成人脸图像方法，CN202010494445.5。
（实质审查中，除导师外第一完成人，本文第五章）8. 软著: 流水线操作员操作检测软件，2018SR299488。
（已授权，除导师外第一完成人）9. 软著: 地面遮挡目标检测与识别软件，2019SR0123216。
（已授权，导师外第一完成人）10. 软著: 全开放式无人便利柜购物智能识别与结算软件，2020SR0977254。
（已授权，导师外第一完成人）11. 软著: 基于深度对抗学习的音频转人脸图像软件，2020SR0977262。
（已授权，导师外第一完成人，本文第五章）12. 软著: 基于生成对抗机制的多人之间表情迁移软件，2020SR0977270。
（已授权，导师外第一完成人，本文第五章）144