标题：基于深度学习的条件式视觉内容生成研究及应用

摘要
随着深度学习的快速发展和计算能力的不断提高，条件式视觉内容生成技术蓬勃发展并取得了许多瞩目的科研成果，在火热的泛娱乐、元宇宙和虚拟人等领域具有极大的应用价值。然而，受限于当前技术与高标准应用需求之间的差距，生成模型在效率和易用性等方面有待进一步提高，且如何通过给定的条件输入（如图像，音频，运动信息等）生成高质量且合理的视觉内容仍是当前面对的棘手问题。
本文聚焦基于深度学习的条件式视觉内容生成技术，按照对信息理解程度逐渐升高的原则，由浅及深地对不同层次的问题进行分类总结并展开具体研究。一方面，低层次的图像像素级任务核心关注局部结构感知及分布预测问题，面临的最大挑战是如何生成合理且高质量的图像，以及设计高效的模型来支持各种终端应用。另一方面，高层次的图像语义级生成任务不仅需要理解图像中的语义信息，同时对高分辨率、高精度生成模型研究及训练优化提出了极大挑战。尤其过渡到视频生成任务中这些难点愈发凸显，且需要额外地对时序合理性及生成多样性进行建模。针对上述挑战，本文研究典型的图像上色和超分辨率任务，以期从低层次的感知中生成更高质量的视觉内容；以及语义层面的人脸换脸、人脸驱动和图像动态化任务，以期从高层次的理解中可控地生成更丰富的图像和视频。本文的研究内容与主要创新点如下：
1. 从局部结构感知的图像增强出发，本文针对图像上色和超分辨率问题展开研究，提出了一种高效的联合图像上色和超分辨率端到端框架，设计了金字塔阀控交叉注意力模块以支持自动和参考两种上色模式，不仅能更好地理解并聚合参考图像的颜色信息，同时具有较强的解释性。此外，本文针对任意倍率图像放大应用需求提出了连续像素映射模块，使用更少计算量的同时提升了模型的预测精度。
2. 从多条件受限的图像纹理语义迁移出发，本文针对人脸换脸问题展开研究，提出了一种基于区域注意力感知的换脸方法以对人脸进行更精细的建模，其包含新颖的面部区域感知的局部分支和源特征适应的全局分支：前者通过引入全局注意力机制来有效地建模不重合的多尺度面部语义交互，而后者补充全局身份相关的线索来进一步保证生成图像的身份一致性。此外，本研究提出了一种无监督人脸软掩膜预测模块，进一步提升了模型的准确性与实用性。
3. 从多条件受限的图像几何语义编辑出发，本文针对人脸驱动问题展开研究，提出了III摘要浙江大学博士学位论文一种基于人脸几何和纹理信息解耦思想设计的多人脸驱动模型，其包含一个精心设计的人脸关键点转换器分支以在几何空间上进行不同身份的面部运动迁移，以及一个几何感知生成器分支生成人脸驱动图像，在保证图像生成质量的基础上实现了多人脸驱动任务目标。同时本文将该框架扩展到了音频多人脸驱动任务，设计了音频特征融合器和几何控制器模块分别进行音频特征提取及高效注入，并提出了一个高质量的AnnVI 数据集以支持高分辨率的音频人脸驱动研究。
4. 从运动约束下的图像序列生成出发，本文针对图像动态化问题展开研究，基于运动和纹理解耦的思想设计了端到端的动态视频生成框架，其包含光流编码器模块和双分支动态视频生成器：前者将表示视频运动的光流信息编码为归一化向量，同时可通过随机运动向量采样的推理方式实现多样化的视频生成；后者在运动向量的控制下基于单帧输入图像生成合理的目标动态视频。此外，针对当前延时视频数据集质量较差的问题，本研究提出了大规模的高分辨率QST 数据集来支持该任务的持续研究。
针对以上研究内容和成果，本文在多个主流数据集上进行了大量的实验评估，证明了所提方法的有效性和优越性，在基于深度学习的条件式视觉内容生成领域取得了出色的研究成果，同时提出的部分算法模型已用于商业产品中，具有较大的应用价值。
关键词：计算机视觉；深度学习；条件式视觉内容生成；图像上色；图像超分辨率；人脸换脸；人脸驱动；图像动态化IV

Abstract
With the rapid development of deep learning and the continuous improvement of comput-ing power, the technology of conditional visual content generation is booming and has achieved alot of remarkable achievements, which has significant application values in the hot fields of pan-entertainment, meta-universe, and virtual human. However, limited by the gap between the currenttechnology and the requirements of high-standard applications, the eﬀiciency and ease-of-use ofthe generation model need to be further improved. Also, how to generate high-quality and reason-able visual content through the given conditional inputs (i.e., image, audio, motion information,etc.) is still a challenging problem to be solved at present.
This thesis focuses on the research of deep learning-based conditional visual content gen-eration, and specific problems at different levels are summarized and studied from shallow todeep, according to the principle of increasing understanding of information. On the one hand, lowpixel-level tasks focus on local structure perception and distribution prediction, and the biggestchallenge is how to generate reasonable and high-quality images while designing eﬀicient modelsto support various terminal applications. On the other hand, high semantic-level tasks need to un-derstand the semantic information in images, which also put forward significant challenges aboutthe high-resolution image generation, high-precision model design, and the optimization proce-dure. Especially in the transition to video generation tasks, these diﬀiculties become increasinglyprominent, and additional modelings of temporal rationality and diversity generation need to beconsidered. Given the above challenges, this thesis studies typical image colorization and super-resolution tasks that generate higher-quality visual content from low-level perception; as well assemantic face swapping, face animation, and image dynamic tasks to generate richer images andvideos from high-level understanding controllably. The research contents and main contributionsof this thesis are as follows:
1. Starting from the local structure perception of image translation, this thesis studies the im-age colorization and super-resolution problems, and we propose an end-to-end framework to solvesimultaneously image colorization and super-resolution eﬀiciently. Specifically, a novel pyramidvalve cross attention module is designed to support both automatic and referential colorization,VABSTRACT浙江大学博士学位论文which can not only understand and aggregate the color information of reference images better butalso has a strong interpretation. Also, a continuous pixel mapping module is proposed to meet theapplication requirements of arbitrary image magnification, improving the model accuracy withless computation.
2. Starting from multi-condition constrained texture transfer, this thesis studies the image-level face swapping problem and proposes a novel region-aware face swapping method for moredelicate modeling, which includes a facial region-aware branch and a source feature-adaptivebranch. The former effectively models non-overlapping multi-scale facial semantic interactionsby introducing a global attention mechanism, while the latter complements global identity-relatedcues to ensure identity consistency for the generated image further. In addition, this study proposesan unsupervised facial mask prediction module further to improve the accuracy and practicabilityof the model.
3. Starting from multi-condition constrained geometry editing, this thesis studies the image-level face animation problem and proposes a multi-identity face animation model, which followsthe decoupling idea of face geometry and texture information. This model consists of a well-designed face landmark converter branch for facial movement migration of different identities ingeometric space, as well as a geometry-aware generator branch to generate animated face images,realizing the multi-identity face animation task on the basis of ensuring the generation quality.
Simultaneously, this framework is extended to the audio-guided multi-identity face animation task,and we design an audio feature fuser module as well as a geometry controller module for eﬀicientaudio feature extraction and injection, respectively. Also, a high-quality AnnVI dataset is proposedto support high-resolution audio-guided multi-identity face animation research.
4. Starting from the motion constrained image sequence generation, this thesis studies theimage dynamic problem and designs an end-to-end dynamic video generation framework based onthe idea of decoupling motion and texture information. In detail, our approach consists of an opticalflow encoder and a dynamic video generator. The former encodes the optical flow informationrepresenting video motion into a normalized vector, and it provides an easy inference mannerto generate various videos by randomly sampling motion vector; The latter generates reasonabletarget dynamic video based on a single input image under the control of the motion vector. Inaddition, given the poor quality of the current time-lapse video dataset, this study proposes alarge-scale high-resolution QST dataset to support the ongoing research on this task.
VI浙江大学博士学位论文ABSTRACTOn the above research contents and achievements, this thesis conducts massive experimentalevaluations on several mainstream datasets, and the results prove the effectiveness and superiorityof the proposed methods. In this thesis, excellent research results have been achieved in conditionalvisual content generation based on deep learning, and some studied models have been used incommercial products with great application values.
Keywords: Computer Vision; Deep Learning; Conditional Visual Content Generation; ImageColorization; Image Super-Resolution; Face Swapping; Face Animation; Image DynamicVII