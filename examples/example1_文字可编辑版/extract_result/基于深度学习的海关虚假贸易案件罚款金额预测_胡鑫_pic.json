{"图 1-1": "图 1-1 CBOW 利用上下文预测中心词“吃”及 Skip-gram 利用中心词“吃”来预测词上下文", "[是神经网络模型，其中编码器将一个句子映射为固定长度的向量表示，而解码器则根据该向量表示生成另一个句子。": "[是神经网络模型，其中编码器将一个句子映射为固定长度的向量表示，而解码器则根据该向量表示生成另一个句子。", "图 2-1": "图 2-1 Quick-thoughts 模型结构图", "图 2-2": "图 2-2 传统 Transformer 模型各阶段流程图", "图 2-3": "图 2-3 Transformer-XL 模型各阶段流程图", "[式为": "[式为 Hf =LSTMf(X)。反向 LSTM 层的输入为序列从末时刻起的反向输入，得到并保存每", "图 2-5": "图 2-5 注意力机制工作流程图", "图 3-1": "图 3-1 基于海关虚假贸易案件的罚款金额预测模型框架图", "图3-2": "图3-2 ", "图 3-3": "图 3-3 文本预处理操作示例图", "图 3-4": "图 3-4 基于 Transformer-XL 改进的 Quick-thoughts 模型编码流程图", "图 3-5": "图 3-5 多头注意力层流程图", "图 3-6": "图 3-6 循环机制示例图", "图 3-7": "图 3-7 基于 Encoder-Decoder 的罚款金额预测模块结构图", "图 3-8": "图 3-8 Bi-LSTM Encoder 结构示意图", "图 3-9": "图 3-9 注意力机制结构", "图 3-10": "图 3-10 Decoder 模块结构示意图", "图 3-11": "图 3-11 案件描述文本矩阵图", "图 4-1": "图 4-1 罚款金额数据分布直方图", "图 4-2": "图 4-2 案情离散化示例", "图 4-3": "图 4-3 对于罚款金额预测注意力分布可视化", "图 4-4": "图 4-4 不同词向量维度在两种评估指标上的表现", "图 4-5": "不同神经元个数与隐含层层数在两种评估指标上的表现", "图 4-6": "Dropout 的敏感实验结果"}