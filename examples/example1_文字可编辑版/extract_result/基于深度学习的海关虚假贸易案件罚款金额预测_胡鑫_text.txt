标题：基于深度学习的海关虚假贸易案件罚款金额预测

摘要
 近年来，中国对外进出口经济贸易呈飞速增长趋势，与此同时，海关进出口违规违法案件数量也随之不断增加。为保证进出口贸易健康有序发展，海关法务人员需要加大对虚假贸易案件的打击力度，并根据案件描述文本预测对应罚款金额。然而，海关相关法务人员目前主要依赖于人工方法来对案件文本进行分析，并据此预测罚款金额，工作量大且效率低下。 
针对以上分析，本文提出一种基于深度学习的海关虚假贸易案件罚款金额预测模型。
具体而言，本文主要研究内容如下： 
（1）在虚假贸易案件文本分析过程中，本文采用传统文本分析模型Quick-thoughs来对案件文本进行编码。而考虑到Quick-thoughs 模型采用GRU 来对文本进行编码，而案情文本通常较长，GRU 无法捕获案情文本中的长距离依赖问题，从而会影响到案件分析的准确性。针对这一问题，本文使用Transformer-XL 模型替代Quick-thoughts 模型中的GRU 模块，来解决GRU 无法对超过固定长度的依赖关系建模的问题，以生成准确性较高且语义表达较为充分的文本向量表示。 
（2）为从文本向量中充分学习语义信息从而准确预测罚款金额，本文基于Encoder-Decoder 框架设计并建立罚款预测模块。
具体而言，本文使用Bi-LSTM 模型作为Encoder，以更好地从案件文本向量中更准确地学习语义信息；在Encoder 与Decoder 之间，本文引入注意力机制模块，以区分不同句子对最终罚款金额的不同影响程度，从而更准确地对Encoder 中所得的各个句子语义进行表示；最后，本文使用CNN 模型作为Decoder，以处理高维度的向量表示，从而实现最终的罚款金额预测。 
（3）基于真实海关贸易数据的多项对比实验表明，本文提出的模型在真实数据集上能有效提高罚款金额预测任务的RSME、MAPE 和一致率指标。 
本文工作在海关司法领域有效提高了案件罚款金额预测的准确率，是海关虚假贸易案件的罚款金额处理办法的新思路和新方法，也是深度学习技术应用于海关司法领域的一次新的探索。 
   关键词：深度学习；罚款金额预测模型；文本向量；Quick-thoughts 模型；注意力机制 

Abstract
  The expeditious growth of China's import and export economy and trade has been witnessed recently. Simultaneously, the violations in imports and export are increased.  In order to ensure the healthy and orderly development of import and export trade, customs legal officers need to increase efforts to crack down on false trade cases and predict the corresponding fines based on the case description text. However, customs legal officer currently mainly rely on manual methods to analyze case texts and predict the number of fines based on this, which is heavy and inefficient. 
Because of the above analysis, this thesis proposes a deep learning-based model for predicting the number of fines in customs false trade cases. The main contents of the thesis as followed: 
(1) In the process of text analysis of false trade cases, this thesis uses the traditional text analysis model Quick-thoughts to encode the case text. Considering that the Quick-thoughts model uses GRU to encoder the text, and the case text is usually longer, GRU can not capture the long-distance dependence problem in the case text, which will affect the accuracy of the case analysis. In response to this problem, this thesis uses the Transformer-XL model to replace the GRU module in the Quick-thought model to solve the problem that GRU cannot model dependencies that exceed a fixed length, to generate text with high accuracy and sufficient semantic expression Vector Representation. 
(2) In order to fully learn semantic information from the text vector to accurately predict the number of fines, a suitable prediction model based on the Encoder-Decoder framework has been designed and built in this thesis. Specifically, this thesis uses the Bi-LSTM model as the Encoder to better learn semantic information from the case text vector; The attention mechanism module has been introduced between the Encoder and Decoder to achieve the semantic expression of each sentence, to classify the influential caused by the final forfeit. 
Finally, this thesis uses the CNN model as the Decoder to process high-dimensional vector representations, to achieve the final fine amount prediction.  (3) Multiple comparison experiments based on real customs trade data show that the model proposed in this thesis can effectively improve the RMSE, MAPE,  and consistency rate indicators of the fine amount prediction task on the real data set.  The work of this thesis has effectively improved the accuracy in customs false trade cases in the customs judicial field, which can be regarded as a novel idea and method for handling   III fines in customs false trade cases, also a new exploration of deep learning applied in the customs justice. 
 Keywords: Deep Learning; Penalty Amount Prediction Model; Text Vector; The Quick-thoughts Model; Attentional Mechanism 

目录：
第一章 绪论
1.1 研究背景及意义
1.2 研究现状及分析
1.2.1 海关虚假贸易案件执法研究现状
1.2.2 自然语言处理领域研究现状
1.3 论文研究内容
1.4 论文组织结构
第二章 相关理论和技术
2.1 句向量技术
2.2 深度学习模型介绍
2.2.1 快速思维向量模型
2.2.2 Transformer-XL 模型
2.2.3 卷积神经网络
2.2.4 双向长短期记忆网络
2.2.5 注意力机制
2.3 本章小结
第三章 海关虚假贸易罚款金额预测模型研究
3.1 符号定义与问题描述
3.1.1 符号定义
3.1.2 问题描述
3.2 整体模型概述
3.3 句向量编码模块
3.3.1 文本预处理
3.3.2 句向量编码
3.4 罚款金额预测模块
3.4.1 Bi-LSTM Encoder
3.4.2 注意力机制
3.4.3 CNN Decoder
第四章 实验设计与结果分析
4.1 实验数据集
4.1.1 数据描述
4.1.2 数据分析
4.2 实验数据预处理
4.2.1 预测值变化
4.2.2 数字离散化
4.3 实验设置
4.4 实验评估指标
4.5 实验结果分析
4.5.1 性能对比实验
4.5.2 模型部件验证
4.5.3 超参数敏感实验
4.5 本章小结
总结与展望
本文工作总结
未来工作展望
参考文献
攻读硕士学位期间取得的研究成果
致 谢



第一章 绪论 
 1 

第一章 绪论 


1.1 研究背景及意义 
随着中国经济贸易的飞速发展，海关进出口贸易规模不断增加。根据海关总署发布的数据显示，2019 年我国前11 个月外贸进出口总值28.5 万亿元，同比增长2.4%。然而在进出口贸易高速增长的同时，由于市场环境、竞争规则、企业诉求、金融监管以及地方政府等多方利益因素的不断变化，不少企业在法律监管不足的现状下片面追求利益最大化，铤而走险。在这种环境下通过虚假贸易获取不当利益的情况日益严重。虚假贸易是指买卖双方制作虚假购销合同，制造虚假发票、货运单据等资料，进而骗取出口退税、出口补贴或出口奖励的贸易行为。 
在我国对外贸易经济飞速发展的大环境大背景下，虚假贸易违法违规活动的大量存在，一方面严重影响海关贸易统计的数据质量，影响国家宏观调控政策的制定，影响经济市场的稳定发展和平稳运行，扰乱进出口秩序，抑制真实贸易的发展，并且对于国家经济政策的执行与实施，经济转型升级和产业结构调整有更深远的影响；另一方面影响海关的执法公信力，引发社会对口岸数据的质疑，基于不准数据而开展的监测预警无法准确判断出我国涉外经贸的具体情况。另外，由于本地企业进出口数据是地方政府政绩的一项重要指标，利用虚假贸易“刷数据”的方式容易引发权力寻租、腐败滋生等现象出现。因此对于虚假贸易违规违法案件的治理打击，已经成为国家海关严查和重点关注的方向。然而，由于全国各地海关每天都会查获大量的违规进出口案件，而案件的识别与处理一直以来都是一项十分繁琐的工作，这些与日递增的违法违规案件更是大大增加了海关执法人员的工作量。 
虚假贸易案件的传统识别是由法务人员根据相关法律法规进行手动判别并确定案件类型及处罚结果。但这种人工识别方法不仅费时费力，并且效率并不高——案件的处理结果主要由执法人员的主观因素来主导评判，虽然有相关法律法规作为执法依照，但不同的执法人员仍然可能产生不同的评判结果。针对该问题，越来越多的学者开始尝试利用自然语言技术对虚假贸易案件进行调查研究。 
自然语言处理（Nature Language Processing，NLP）是从自然语言中获取其中所包含的语义信息，使得机器能对自然语言所表达的真正含义进行理解，从而实现人与机器之间能够通过人类自然语言进行交流、沟通的技术。自然语言处理技术的本质是促使机器能够理解人类的语言，即把自然语言通过计算机转化成离散的数学形式，使得自然语言华南理工大学硕士学位论文  2 可以在机器上进行运行处理。结合海关执法现状，将自然语言处理技术应用于海关打击违法犯罪活动，设计出一套可供海关人员使用的模型，来解决不同案件发生情况下的海关虚假贸易案件执法问题，能够及时有效地打击海关违法犯罪行为，降低海关违规违法犯罪活动的发生。 
基于以上分析，海关虚假贸易案件的高效处理需要实时案件的准确描述、对案件的性质和处罚办法的正确判断、以及在抓获违法案件上人员的着重分配，从而提高海关工作人员的办案效率，进而提高对虚假贸易违规违法案件的打击力度、降低海关虚假贸易案件的发生率。 
因此，本文聚焦此类平台中的关键任务——“海关虚假贸易案件罚款金额预测”，系统性的分析了在复杂的案件文本语义下，根据简要的案件文本来预测该类虚假贸易案件罚款金额。进而，本文分别在文本向量生成、罚款金额预测两个阶段所涉及的核心任务与关键问题展开研究，设计并构建有效的模型以对案件文本进行准确的向量表示与金额预测。本文工作是深度学习技术应用于海关司法领域的一次新的探索，为海关进出口贸易执法智能化提供了有效的新方法与新思路，其研究成果具有一定的应用价值与研究价值。 


1.2 研究现状及分析 
与本文研究相关的工作主要分为两部分，一部分是现有的海关虚假贸易案件执法办法，另一部分是与本文研究相关的若干自然语言处理任务的研究现状，如文本向量表示，文本相似性等。本小节将分别对这两方面的研究内容进行简单的介绍与总结。 


1.2.1 海关虚假贸易案件执法研究现状 
在海关打击违法犯罪领域，海关虚假贸易案件执法的目的是保证精确打击虚假贸易、保证进出口贸易正常进行。通常情况下，海关通过对企业申报的进出口物资与实际进出口物资进行比对核实，针对进出口公司实际运营情况、物流单据真实性情况进行分析，依据分析情况参考海关进出口法律法规对虚假贸易进行打击。与此同时，海关依据执法情况，对执法效果进行评估，进而提高执法水平和工作效率。因此，打击虚假贸易的执法方法是海关能否准确识别并有力打击虚假贸易的关键问题，执法效果评估是保证执法质量的重要措施。下面将围绕这两个方向的研究现状展开介绍。 
在打击虚假贸易执法方法方面，众多研究者提出应建立海关贸易审查制度，以实现虚假贸易的准确打击。海关贸易核查制度可以认为是基于贸易数据的执法方法。其基本

第一章 绪论 
 3 思路是：海关对贸易数据进行检测与分析，并依照法律规定在期限内对相关企事业单位的相关业务和相关货物的会计帐本和其他材料进行检查核实，来确定企业公司活动的合法性。通过对贸易数据的分析，针对不同的贸易数据，相应的海关贸易核查制度相继建立与提出。海关总署出台了暂缓纳入海关统计的制度——《中华人民共和国海关统计工作管理规定》（海关总署令第242 号）——暂缓纳入统计的贸易无法获得国家补贴和退税优惠直至重新纳入统计数据为止。另外，为阻止没有真实付汇的虚假贸易申报，海关与金融管理局联合出台了外汇联网核查制度。刘鹏承认为海关统计部门应该着手建设和完善出口货物的申报价格管控制度，加强防范企事业单位对出口货物价格进行虚假上报的行为。随后，宋江培、梅莹提出，海关统计部门应当加强对虚假贸易数据的排查和监管力度，对于有问题的企事业单位进行摸底调查和重点关注。考虑到新一代数据技术的迅速发展，于少卿提出，应通过引入“大数据”、“云计算”等新型技术的分析能力，随时调动海量数据，改变以往海关年度分析、月度分析的习惯，实现统计分析的即时性，及时发现风险，做好预警工作。为进一步完善海关贸易核查制度，宁波海关统计处课题组提出，应通过“产品出口增速”、“出口关别和各类特殊监管区域出口增速”、“出口交货值增速”等七个我国监测虚假贸易的指标，结合微观数据审核和宏观数据监控，监测和管控双管齐下，充分发挥海关统计数据资源优势，开展虚假贸易统计分析和监测预警，定期通报外贸进出口整体数据及虚假贸易情况。 
在对执法效果评估方面，宋雅君指出了海关在反走私、监管、征税、统计评估四大职能的一级绩效指标，并建立了相应的二级指标体系。为进一步评估执法强度和震慑力，张舫、李响等人参考了Gary Becker 提出的执法震慑不等式，提出了违法者得到有效处罚的概率以及执法人员关于处罚轻重的执法评价方法。 
因此，海关依据执法情况，通过对执法效果进行评估检验，能够很好的提高执法水平和工作效率。海关使用打击虚假贸易的有效执法方法，能够准确识别并有力打击虚假贸易违规违法行为，这是保证执法质量的重要措施。 


1.2.2 自然语言处理领域研究现状 
目前，自然语言处理领域研究主要包括文本向量化、文本分类、情感分析、多标签分类、文本相似度计算等方向。在深度学习被人们广泛使用之前，自然语言处理研究较少，自然语言处理任务也都较为简单。大多自然语言处理任务通过文本预处理工具，如：语句分词、词性标记、命名实体识别、依存关系分析等进行文本预处理，华南理工大学硕士学位论文  4 然后通过特征抽取等传统方法，结合支持向量机、朴素贝叶斯等机器学习方法来进行学习训练处理相关任务。之后伴随着深度学习的不断发展，自然语言处理技术开始逐渐在各个领域“大显身手”。配合着各种神经网络模型的不断提出与完善，如卷积神经网络、循环神经网络、递归神经网络、生成式对抗网络、强化学习等等，这些模型应用于自然语言处理的众多任务，如命名实体识别，机器翻译，问答系统，阅读理解，关系抽取等，均对其结果有着很大程度的改善与提升。 
海关虚假贸易案件处罚判断生成的核心问题是如何将复杂的案件描述转化为可供计算机处理的段落文本向量化表示，即获取一个适合海关处理违规案件的段落嵌入模型。
文本向量化是将文本表示成低维、稠密的实数向量的一种方法，可以对文本所蕴含的语义信息进行表达，是文本表示中的一种重要方式。文本表示作为自然语言处理技术的一项基础工作，对整个自然语言处理系统的性能产生直接的影响。目前而言，对文本向量化的研究工作主要包括词的向量化和句子向量化，本文将从这两个方向的研究现状分别进行介绍。 
词向量又称为词嵌入（Word Embedding），在自然语言处理领域中词向量研究一直都是人们的研究热门与重点。词嵌入就是有效的将词语映射到一个向量空间，使得后续模型快速获取词语的语义信息加以运用。
词嵌入最开始是在潜在语义学（Latent Semantic Analysis，LSA）中以一种统计信息的表示方法出现的，之后才应用于自然语言处理的各个方向。一个好的词嵌入向量不仅能很好地反映出语义和句法的复杂特征，还能很好地适应不同上下文之间的变换。
最早用来表示词语的表达模型是词袋模型（Bag of Words，BOW），其假定对于一个文本而言，忽略其词序，语法和句法，仅仅将其看作一个个词的集合或组合，文本中的每个词都相互独立且不依赖于其他词是否存在。最典型的词袋模型为独热编码（One Hot Vector），该模型对于句子中的每一个单词都用长度为n 的向量表示，n 为词典的长度，并且该向量中单词在词典中的索引序列号位置为1，其余位置均为0。虽然该模型非常易于理解实施且灵活性高，但缺点也十分明显：这种向量是一种稀疏向量，向量的维度随着词典的增大而增大，当文本增大时向量的计算复杂度也随之增大；该向量丢弃了词序，忽略了上下文之间的关系，无法衡量相似度，影响在文档中词语的语义。 
为获得更进一步的词向量表示方法，学者们研究并提出了TF-IDF。TF-IDF 是一种应用于信息检索和文本挖掘的常用加权技术。
其主要思想是根据一个单词或者短语在不同的文章中出现频次不同来进行判断，若这个词在本篇文章中出现的频率越高越重要，

第一章 绪论 
 5 在其他文章中出现的越少越不重要，则说明这个词越有区分度，区分度明显则认为该单词或短语具有很好的类别区分能力，适合用来做分类任务，反之则不适合。由于既考虑了词语在文本中出现的频率也强调了单词在文本与文本之间的重要性，TF-IDF 方法作为一种统计学习方法被广泛使用。
但是这种方法简单地认定文本出现频率小的单词就越有用，文本出现频率大的单词就越无用，这显然是很片面的。
TF-IDF 方法也无法准确地给出单词的重要程度与特征词语的分布情况，无法有效地完成对权值的调整。 
为了获得更加充分的词的语义相关性表达，Hinton等人提出了词的分布式特征表示（Distributed Representation)。分布式表达的基本思路是，在对一个词语状态进行表达时，状态寄存器和状态不再是一一对应映射存储的，而是多对多的关系。具体而言，在词的分布式特征表达中，一个词语状态可以用多个状态寄存器共同定义表达，同时一个状态寄存器也可以参与多个不同状态的表达。比如“大红灯笼”这个词，如果用分布式特征来表示，那么可能是一个状态寄存器代表大小，一个状态寄存器代表颜色，一个状态寄存器代表物品种类。当且仅当这三个状态寄存器同时激活时，要表达的物体才能得到比较准确的描述。
最早将分布式表示应用处理到词向量上来的是Bengio等人，他们提出的神经网络语言模型（Neural Probabilistic Language Model，NNLM）将每个词转换表示为一个稠密的实数向量。该模型可以自由定义词向量的维度，且维度并不会因为词典的扩展而发生改变，并且该模型生成的词向量能够很好的根据特征距离度量词语之间的相关性。但该模型的缺点也十分明显，计算复杂度过大且参数较多。 
真正让分布式词向量得以广泛运用且性能效果较好的是由Tomas Mikolov提出的Word2Vec 模型。它的本质就是简单化的神经网络模型，能够将不可计算的非结构化的词语转化为可计算且结构化的向量表示。Word2Vec 共有两种不同的表示形式，跳字模型（Skip-gram）和连续词袋模型（Continuous Bag of Words，CBOW）。CBOW 是一种统计语言模型，即根据某个词前面的几个词或者前后出现的几个词，来计算预测该词出现的概率。而与CBOW 模型相反，Skip-gram 根据当前词分别计算预测该词出现上下文词的概率。尽管两者的目标优化函数并不一致，但是其目的都是为了获得单词与相邻单词之间的语义相关性。以“我喜欢吃蔬菜水果”为例，假设此时的关键词为“吃”，训练好的CBOW 模型就是将“我”“喜欢”“蔬菜”“水果”的one hot 表示方式作为输入，来计算预测“吃”的分布式表示。
Skip-gram 模型与其相反，它们的模型流程如图1-1 所示： 
华南理工大学硕士学位论文  6  图1-1 CBOW 利用上下文预测中心词“吃”及Skip-gram 利用中心词“吃”来预测词上下文 Word2Vec 模型的优点在于考虑了词语和词语之间的语义关系以及词序关系，同时能够应用于不同场景的上下文语句。除此之外，该模型还能够在其他大型语料上进行模型预训练，获取训练好的词向量的表达方法，随后迁移到其他任务中。但是其仍然存在一些不可忽视的缺点，由于词语和向量是一一对应的关系，一些词语只考虑了当前语料库中所表达的词义，而没法更多的考虑整句话或者词语所处的其他上下文环境，比如“苹果”到底是指日常生活中所吃的水果“苹果”，还是指美国的电子商务公司的“苹果”，这对于词向量来说是很难进行辨别的，一个词的词向量不应该仅仅只在语料中进行学习，而是应该考虑该词语实际具体所处的位置，才能得到有效语义更丰富以及更为准确的表征。
 为了更加充分地考虑句子中词语与词语之间的关系，以及当前词语所处的语境，研究人员们通过研究提出了词语在句子级别中的特征表示，进行句子表示学习，从而获得语义表达较好的句子向量（Sentence Embedding）。
传统获取句向量的方法是对一个句子中所有词的词向量进行组合，但这种方法虽然足够简单高效但，并没有考虑词序信息。
例如平均句向量模型就是将句子中所有词的词向量表达进行相加取平均运算，将计算得到的向量当作最终的句子向量。但在该模型中，句子中所有单词对于表达句子含义的重要性是相同的，这很显然是不准确的。更典型的句子向量表达就是14 年由Tomas Mikolov提出的Doc2vec 模型，用于学习句子和文档的分布式表示。该模型在训练过程中引入了段落特征（Paragraph Vector），即将每一个段落表示为一个向量，其中不同的段落具有不同的段落向量。该段落向量能够让模型更加充分考虑上下文信息，对全局的文本信息起到一个概括作用，即该段落特征可以作为该段落或句子的一个主旨，对该段落或句子的主要信息进行概括表达。
Doc2vec 模型的每一次训练不仅仅得到了词向量，而且每一次输入都会共享该段落特征，并对段落特征不断更新使得其表达更加准确。并且，Doc2vec 模型通过该段落向量可以构建语言模型来预测下一个词，且段落向量在获

第一章 绪论 
 7 取的过程中，能够对无标记的数据进行训练。此处仍然以“我喜欢吃蔬菜”为例，输入“我”“喜欢”“吃”的one hot 表示方式以及段落特征作为输入，计算预测出下一个单词“蔬菜”的分布式表示，模型流程如图1-2 所示。但是该方法仍然存在着一些不足之处，例如对段落文本级别特征的考虑过于粗糙，语义表征的精确度准确性不够高等。 
 图1-2 Doc2vec 添加段落特征预测下一个词 除了Dov2vec 之外，还有在NIPS-2015 上Kiros R等人提出的Skip-thought 模型，该模型使用的是传统的端对端（End-to-End）方法，该方法使用的编码器以及解码器都是RNN 模型，并根据上下文关系进行预测，以获得更好特征编码的语义表征。以及在2018 年的ICLR 上Logeswaran等人提出的Quick-thoughts 模型，该模型是在Skip-thought 模型的基础上进行的改进优化。这个模型将在本论文第二章进行详细描述。 


1.3 论文研究内容 
本文以海关进出口虚假贸易打击执法为主要应用场景，围绕海关虚假贸易案件的罚款金额预测问题展开。下面将对本文的主要研究内容和工作进行展开介绍： 
（1）本文通过对句向量生成模型的相关工作进行分析与总结，设计句向量编码模块，提出基于Transformer-XL 改进的Quick-thoughts 句向量编码模块，使用Transformer-XL 替代传统Quick-thoughts 模型中的GRU 模块来对案件文本描述进行编码，生成更加准确并且语义表达更为充分的文本向量表示。 
（2）在对罚款金额预测模块的设计建立过程中，传统方法大多直接使用CNN 卷积网络模型学习预测罚款金额，但简单的CNN 模型无法有效的学习案件文本复杂的语义信息，因而罚款金额预测的准确率往往不高。
针对该问题，本文采用Encoder-Decoder 框架来建立罚款金额预测模型，并引入句子层级的注意力机制来体现案件文本中不同句子代表的不同含义。 
（3）本文在包含1 万5 千条海关虚假贸易案件的真实数据集上进行了多组实验进华南理工大学硕士学位论文  8 行对比研究。实验结果显示，本文提出的罚款金额预测模型相对于目前主流的模型，在海关虚假贸易违规违法案件领域能有效提高案件罚款金额预测的准确率，达到了预期的效果。 


1.4 论文组织结构 
本文的研究目的是根据海关虚假贸易案件的案情描述帮助海关执法人员准确地判断并预测出该描述案件的罚款金额情况，研究重点在于如何准确生成该案件描述的文本特征表达以及建立预测准确的罚款预测模型。其中，各个章节的具体安排如下。 
第一章，绪论。首先介绍了海关虚假贸易案件罚款金额预测课题的研究背景、研究目和研究意义；然后对海关虚假贸易案件执法研究情况以及相关自然语言处理技术中的向量模型的发展过程进行介绍，并对相关句向量模型进行了阐述；最后简单梳理了本文的主要研究内容与组织结构。 
第二章，相关理论和技术。首先从句子的有监督表示学习、无监督表示学习和多任务表示学习三个方向简单地介绍了句向量技术的研究现状；其次对本文涉及到的深度学习模型进行介绍，主要包括无监督的句向量模型（Qucik-thoughts）和基于自注意力机制的Transformer-XL 模型；最后对本章的内容进行总结。 
第三章，模型整体框架的设计。首先介绍了模型整体架构，介绍了模型的两个主要模块——句向量生成模块和罚款金额预测模块；然后对本文的两个模块进行详细介绍，在句向量生成模块中，利用基于Transformer-XL 改进的Quick-thoughts 语言模型，训练海关虚假贸易相关句向量；在罚款金额预测模块中，使用Encoder-Decoder 框架设计罚款金额预测模型，采用Bi-LSTM 编码器对案情文本进行编码，引入Attention 机制并使用CNN 解码器生成最终案件的罚款金额，不断提升预测的准确性；最后对本章内容进行总结。 
第四章，实验设计与结果分析。
首先对本文所使用的实验数据集进行了介绍与分析；其次对数据进行预处理的两种方法进行了简单介绍；接着对实验所使用的评估指标与实验流程进行了介绍；最后通过对比不同模型在海关虚假贸易案件数据集上的表现并对实验结果进行分析，证明了本文所提模型的可行性以及稳定性。 
第五章，总结与展望。首先分析总结了本文的主要工作内容以及研究结果，然后对基于深度学习的海关虚假贸易案件数据的罚款金额预测方法的未来工作进行展望。 
 

第二章 相关理论和技术 
 9 

第二章 相关理论和技术 
本章将对本文研究工作所涉及的相关理论和技术进行介绍，着重阐述与本课题研究相关的基础知识和对应的算法模型。首先，简单介绍了句向量技术及其分类；其次，分别对几种常用于文本向量编码与用于回归预测的深度学习模型进行了详细介绍与分析。
本章内容为第三章罚款金额预测模型设计提供理论基础。 


2.1 句向量技术 
在自然语言处理任务中，句子的向量表达常常都是通过词向量的简单相加或加权求和等方式堆叠而来。现阶段常用的句向量方法通常可分为三类，即对句子的有监督表示学习、无监督表示学习和多任务表示学习。 
对于句子的有监督表示学习方法，2015 年由Iyyer等人提出了一种深度平均网络模型DAN（Deep Averaging Networks），该网络模型是在传统词袋模型的基础上，通过添加多个隐藏层的方式来增加整体网络的深度，实现在句法上的信息提取，并提高训练准确度。结果表明DAN 模型在句子和文档级别的情感分析以及问答任务上的准确性都有所提升，训练时间也比其他复杂神经网络方法要少得多。为了进一步提高句向量表示的准确率，2016 年由Sanjeev等人提出了一种加权词袋模型SIF（Smooth Inverse Frequency），该方法使用当前主流的词向量方法来计算句子中每个词语的词向量表示，接着通过计算词向量的加权平均值来表示该句子，然后通过主成分分析或奇异值分解的方法对它们进行调整。
这种加权获得句向量的方法可以将文本相似性任务的性能提高约10%到30%，并且优于包括RNN 和LSTM 在内的复杂的监督表示学习方法。为了提高句向量模型的泛化能力，2017 年由Conneau等人提出了一种基于SNLI 语料库训练出来的通用的句向量表达。作者认为从SNLI 语料库中训练得到的句向量也适合迁移到其它的NLP 任务当中，该方法通过使用多种模型在多种数据集上反复训练得到一个通用的句向量模型，再将训练得到的句向量模型应用到其它NLP 任务当中，通过对比任务效果得到最终的句向量模型。
这种方法在自然语言推理任务中训练得到的模型性能要优于其他条件下学习得到的模型并具备可迁移性。 
对于句子的无监督表示学习方法，2014 年由Tomas Mikolov提出了一种用于学习句子和文档的分布式表示的Doc2vec 模型。该模型增加了一个新的句子向量，将其作为句子的主旨用于对之前的信息进行记忆。虽然该模型能够处理不同长度的句子文本，但其准确率往往偏低，2015 年由Kiros R等人提出了一种利用中心句子来预测上下文句华南理工大学硕士学位论文  10 子的Skip-thought 模型。
该模型采用Encoder-Decoder结构作为基本框架，使用循环神经网络作为模型的编解码器，目的是通过训练得到一个性能良好的编码器，并将这个编码器作为句向量的通用模型。结果表明虽然Skip-thought 模型生成的句向量质量不是最优的，但是普遍表现都很好，说明该模型具有优秀的通用性。
随后在2018 年Logeswaran等人提出了Quick-thoughts 模型，该模型在Skip-thought 模型的基础上进行了改进和优化，在模型的Decoder 端仅使用一个分类器来学习句子的向量表示，在降低训练时间的同时也成功地在一定程度上忽略了句子形式，学习到了真正的语义表示。 
对于句子的多任务表示学习方法，2018 年由Subramanian等人提出了一种一对多的多任务学习框架，通过在不同的任务间进行切换来联合学习一个通用的句子表征。结果证明了在这几个分类任务上，使用联合训练学习的句子表征在分类效果上要比单独训练学习的要好得多，并且作者还指出联合训练中不同的任务对句子表征中的不同方面有着不同的贡献。
同样在2018 年，由谷歌的Cer等人提出一种类似的多任务学习框架来对句子的向量表示进行学习，不过该作者使用了Transformer 和DAN 两种框架来作为句子的Encoder，在十个分类任务上进行迁移学习的评估，效果也都很不错。 


2.2 深度学习模型介绍 
深度学习在2006 年由Hinton等人提出，又称为深度神经网络，旨在学习样本数据的内在规律和表示层次，是机器学习领域的重要研究方向之一。深层的神经网络是典型的深度学习模型，比较常见的如用于计算机视觉方面的卷积神经网络（Convolutional Neural Networks，CNN），在对图片的处理方面表现的十分出色；用于自然语言处理等方面的循环神经网络（Recurrent Neural Networks，RNN），在对文本语义表达方面表现十分出色；以及用于搜索、推广、广告等其他需要大量抽象特征功能的场景的深度神经网络（Deep Neural Networks，DNN）。在实践中，深度学习能够利用大量数据不断学习训练，更为全面的了解数据特征，模拟预测任务出现的各种情况，对预测任务进行更为准确判断。本文在2.2.1、2.2.2 小节介绍涉及句向量生成任务中文本向量研究的深度学习方法，在2.2.3、2.2.4、2.2.5 小节介绍预测任务中所涉及的深度学习方法。 


2.2.1 快速思维向量模型 
快速思维句向量（Quick-thoughts vector，QT）模型是一种可用于从未标记的数据中学习句子表示的句向量模型。Quick-thoughts 模型采用一种分类器的方式学习句子表示，使用Encoder-Decoder 框架作为模型的基础框架。在编码器部分，Quick-thoughts 模

第二章 相关理论和技术 
 11 型使用某个函数（如循环神经网络）用来压缩所选择的中心句子信息。
而在解码器部分，Quick-thoughts 模型使用一个分类器来选择的正确上下文语句。即给定一个输入句子，Encoder 部分对其进行编码，生成该句子的编码表示。以此为基础，Decoder 从一组候选句子中选择正确的目标句子。简单来说，Decoder 使用一个分类器从所有的候选句子中选择一个最可能与目标句子形成上下文关系的句子，这里可以近似看作是一种判别，判别候选句子中的哪一个句子是正确的句子。具体模型的结构如图2-1 所示： 
 图2-1 Quick-thoughts 模型结构图 编码器：如图2-1 中的/所示，作用是将文本句子表示为分布式语义向量。假设表示所有训练数据的集合，表示输入的目标句子，表示输入句子的上下文相关句子集合（也就是的前一个句子或后一个句子），表示除输入句子之外的候选句子集合，它包括上下文句子以及其它不是目标句子的上下文句子。
定义、作为参数化函数，、以一个句子作为输入，将句子编码成一个固定长度的向量。编码器通过参数化函数、对目标句子以及候选句子集合中的每个句子进行编码，然后输入到解码器中。
循环神经网络模型（RNN）中的门控循环单元（GRU）在近年句子表示学习方法中得到了广泛的应用，Quick-thoughts 模型将GRU 作为参数化函数和。在Quick-thoughts 模型的编码器部分，句子中的单词被顺序输入GRU 中进行句子向量编码，GRU 最后的隐藏状态作为句子的向量表示输出。 
解码器：如图2-1 所示该模型的解码器是一个分类选择器。在给定一个句子及其上下文的情况下，分类器根据上下文句子的向量表示，对编码器输出的向量进行比较选择，将目标句子与其他句子区分开来，并判断选出中的。具体而言，解码器对的向量表示和中的所有句子向量表示计算内积得分，然后将计算结果输入到( )Enc f( )Enc gSsctxtSsscandSsctxtSsfgfgfgscandSfgcandSctxtsscandSsoftmax华南理工大学硕士学位论文  12 层进行分类筛选，选出得分最高的为目标句子的上下文相关句子。模型使用的分类器即得分函数为两个向量内积： 
 ， (2-1) 其中和表示不同的句子向量。
Quick-thoughts 模型在解码器端使用一个简单的分类器，其目的是将模型的工作重点放在编码器的训练上，尽可能地减少除了编码器训练以外的工作量，最终得到一个好的句子向量表示。 
对比2.1 小节提到的传统Skip-thought 模型，Quick-thoughts 模型在其基础上进行优化改进，在解码器模块引入一个分类器来判别选择正确的上下文句子，从而在模型性能上更加优越。因此本文选择Quick-thoughts 模型作为基准模型进行改进和分析。 


2.2.2 Transformer-XL 模型 
在当前的NLP 领域中，通常使用RNN 和Transformer 这两种架构来处理语言建模问题。其中RNN 是按照序列顺序来逐个学习输入的字符或词语之间的相互关系，而Transformer 则是接受一整段的序列，然后使用自注意力机制来学习它们之间的依赖关系。
虽然这两种架构都取得了不错的成绩，但是它们都在捕捉长距离依赖性上有所欠缺。
因此，卡内基梅隆大学联合Google Brain 在2019 年提出了Transformer-XL模型。该模型同时结合了RNN 序列建模和Transformer 自注意力机制的优点，在输入数据的每个段上均使用Transformer的注意力模块，并使用循环机制来学习连续段之间的依赖关系。 
对比传统Transformer 模型，Transformer-XL 由一个新的循环机制和一个新设计的相对位置编码器组成。首先简单介绍传统Transformer 模型的训练及预测流程，以长度为4 的片段为例，传统Transformer 模型的训练、预测流程如图2-2 所示： 
 a) 传统Transformer 模型的训练阶段 sctxtsc(),Tc u vu v=uv

第二章 相关理论和技术 
 13  b) 传统Transformer 模型的测试阶段 图2-2 传统Transformer 模型各阶段流程图 从图2-2 中可以看出，Transformer 模型在训练时，处理序列长度超过固定长度的句子，处理方式是将文本序列划分为多个Segments，训练时对每个Segment 进行单独处理，且Segment 之间没有联系，如图2-2(a)所示。但这样处理的缺点是按照固定长度来切割句子，并没有考虑句子的自然边界，因此分割出来的Segments 在语义上是不完整的。Transformer 模型在预测时，如图2-2(b)所示，对固定长度的Segment 做计算，取最后一个位置的隐向量作为输出。为了充分利用上下文之间的关系，每做完一次预测，模型将对整个序列向后移动一个位置，再做一次计算。但这样的方式往往会降低模型的计算效率，增加模型的计算复杂度，因此研究人员提出Transformer-XL 模型来优化改进传统Transformer 模型。同样以长度为4 的片段为例，Transformer-XL 模型的训练、预测流程如图2-3 所示： 
 a) Transformer-XL 模型的训练阶段 华南理工大学硕士学位论文  14  b) Transformer-XL 模型的测试阶段 图2-3 Transformer-XL 模型各阶段流程图 从图2-3 中可以看出，在Transformer-XL 模型的训练阶段，在处理新片段时，每个隐藏层都将会接收两种输入：一种是该段前面隐藏层的输出，如图2-3（a）灰色线部分所示，该部分与传统Transformer 模型相同；另一种是前面段的隐藏层的输出，如图2-3（a）绿色线部分所示，这一部分的加入可以使Transformer-XL 模型对文本创建长期依赖关系。在Transformer-XL 模型的测试阶段，传统Transformer 模型在测试阶段时，每一次只能前进一个片段，并且需要重新构建段，并全部从头开始计算；而Transformer-XL 模型在测试阶段时，每一次都能前进一整个段，并能利用之前的段的数据来预测当前段的输出，与传统Transformer 模型相比速度更快。 
另一个改进就是Transformer-XL 模型的位置编码的改进，在传统Transformer 中，模型考虑了序列的位置信息，使用的是绝对位置编码。
而Transformer-XL 模型使用的相对位置编码，其允许Transformer 网络在长度可变的内容中学习依赖关系，而不干扰时空的一致性。在分段的情况下，如果仅仅对于每个段仍直接使用传统Transformer 的绝对位置编码，即每个不同段在同一个位置上的表示使用相同的位置编码，就会出现问题。
比如，第i-2 段和第i-1 的第一个位置将具有相同的位置编码，但它们对于第i 段的建模重要性显然并不相同（例如第i-2 段中的第一个位置重要性可能要低一些）。因此，需要对这种位置进行区分，通过对计算公式的改进，引入相对编码。相对位置编码总体来看分为四个部分，一是基于内容的“寻址”，即没有添加原始位置编码的原始分数；二是基于内容的位置偏置，即相对于当前内容的位置偏差；三是全局的内容偏置，用于衡

第二章 相关理论和技术 
 15 量Key 值的重要性；四是全局的位置偏置，根据Query 值和Key 值之间的距离调整重要性。 
Transformer-XL 模型不仅可以有效解决传统RNN 神经网络模型对长距离文本依赖不足的问题，让模型捕捉到长期的依赖，同时也可以解决内容分块的问题，拓展单词关注不同位置的能力。比如“A 公司申报出口货物1800 件到B 公司，他实际出口2000件。”Attention 机制会将“他”对应“A 公司”。并且Transformer-XL 模型的扩展能力强，能够有效的扩展到其他需要该能力的深度学习领域，比如音频分析等。
Transformer-XL 模型的训练速度相比传统的Transformer 模型也有了很大的提高，对语言的理解过程十分有帮助，有效提高了训练效率。 


2.2.3 卷积神经网络 
卷积神经网络（Convolutional Neural Networks，CNN）是一种特殊的神经网络模型，它与普通神经网络模型相似，都由具有可以学习的权重和偏置常量的神经元组成。
不同之处在于CNN 的默认输入是图像，CNN 可以将输入信息编码成具有特定性质的网络结构，使得CNN 能够很好的利用输入数据的二维结构。这样大大地减少了模型的参数使用，提高模型的效率。
CNN 常常应用于语音识别和图像处理领域的任务。
近年来，CNN 在自然语言处理领域上也达到了很好的效果，比如自动问答和机器翻译等任务。 
下面将对CNN 模型的结构进行简单介绍，CNN 模型通常包含以下几种层： 
（1）卷积层（Convolutional Layer）。卷积神经网络包含多层卷积层，其中每层卷积层都由多个卷积单元组成。卷积层的卷积运算旨在对输入信息的不同特征进行提取，第一层卷积也许只能够提取一些简单的基本特征，但随着层数的不断增加，卷积神经网络能够从基础特征中迭代提取出更复杂的特征。
每个卷积单元的参数都是由反向传播算法优化得来的。 
（2）池化层（Pooling Layer）。池化层通常在卷积层之后。卷积层会得到维度很大的特征，计算量过于复杂庞大。因此池化层会把特征切成几个部分，通过取各个部分特征的最大值或平均值的方式，以此来获得新的维度更小的特征。 
（3）全连接层（Fully Connected Layer）。全连接层把所有的局部特征结合生成全局特征，用来做最后的计算。 
在常见的自然语言处理任务中，卷积神经网络将一个以矩阵形式表示的文本或句子作为网络模型的输入，其中文本或句子矩阵的每一行分别对应于文本中的一个句子或者句子中的一个词语，也就是说每一行对应一个句子或单词的向量表达形式；矩阵的列就华南理工大学硕士学位论文  16 表示这个句子或单词的向量表达形式的维度。文本矩阵可以由Word2Vec 或其他文本向量表达算法得到；接着卷积层对文本矩阵进行卷积处理。卷积核会覆盖文本的若干个句子或词语，卷积核的高度取决于上下文词语的数量，卷积核的宽度一般与词向量的维度相等；卷积层之后接着是池化层，池化层会依据一定的规则，例如取特征区域的平均值、最大值等等，进一步对卷积层的输出特征进行抽象处理；一般使用最大值池化对前一层中最主要的特征进行保留处理；接着是全连接层，但在全连接层之前一般会使用flatten函数将向量“拉直”处理；为了避免出现过拟合的现象，一般还会在全连接层对数据进行正则化处理，即采用Dropout 技术来“丢弃”部分隐藏层的节点；使用全连接层可以将CNN 应用于处理分类或回归任务；最后传递输出层输出最终任务结果。 
目前CNN 在自然语言处理领域的应用对自然语言处理技术的发展产生了十分积极的影响。如何有效利用CNN 技术对自然语言处理任务进行学习研究，提高相应模型的性能与泛化能力，解决一些实际有效的问题，将对自然语言处理领域的发展具有十分重要意义。 


2.2.4 双向长短期记忆网络 
双向的长短期记忆网络模型Bi-LSTM（Bi-directional Long Short-Term Memory）是由Graves 和Schmidhuber 提出的一种处理序列数据的神经网络模型。
该模型包含有一前、一后两个方向的LSTM（Long Short-Term Memory），通过叠加组合的方式构成。
在介绍Bi-LSTM 模型之前首先介绍LSTM 神经网络模型，该模型是RNN 的一种变体，其设计目的是为了解决一般RNN 存在的远距离依赖问题、长文本序列训练中的梯度消失问题和梯度爆炸问题。
虽然相比RNN，LSTM 模型解决了一系列问题，但其仍然只能根据先前时刻的隐藏状态与当前时刻的输入信息来对下一时刻的隐藏状态输出进行预测分析。而通过对自然语言的表达习惯进行分析发现，在预测文本中某一词的词意时，往往需要根据上下文的语义进行分析。因此，考虑到单向LSTM 神经网络模型的不足，Bi-LSTM 模型被提出，该模型的输入分别是输入序列的正向排序和反向排序，输出由两个方向的LSTM 的隐藏层状态共同决定。这种网络结构较单向LSTM 可以更好的获取文本序列上下文之间的关系。图2-4 为双向长短期记忆网络结构图。 


第二章 相关理论和技术 
 17  图2-4 双向长短期记忆网络结构图 Bi-LSTM 由层结构相同的前向LSTM 和反向LSTM 构成。其中前向LSTM 层的输入为序列从0 时刻起的正向输入，得到并保存每个时刻的前向隐藏层状态，数学表达形式如公式（2-2）所示： 
 ， (2-2) 其中，表示前向LSTM 的权重系数，表示当前时刻的文本输入，表示前一时刻的前向LSTM 层输出。反向LSTM 层的输入为序列从序列末N 时刻的反向输入，得到并保存每个时刻反向隐藏层的输出，数学表达形式如公式（2-3）所示： 
 , (2-3) 其中，表示反向LSTM 的权重系数，表示当前时刻的文本输入，表示后一时刻的反向LSTM 层输出。Bi-LSTM 的输出为前向LSTM 层和反向LSTM 层的相应时刻隐藏层的组合，数学表达形式如公式（2-4）所示： 
 ， (2-4) 其中，表示输出层的权重系数，和表示当前时刻两个方向的LSTM 层输出。 
双向长短期记忆网络，不仅可以学习到时序序列的历史信息，还可以关联序列时序序列的未来数据，因此双向长短期记忆网络在自然语言处理领域中应用广泛。本文在第三章设计罚款金额预测模型中也将采用Bi-LSTM 来学习文本上下文特征。 
111()tlstmftthfUxW h -=×+×1U1Wtx1th -()221tlstmbtthfxUhW+×=×+2U2Wtx1th +()12tttog V hVh+××="1V2Vthth华南理工大学硕士学位论文  18 

2.2.5 注意力机制 
虽然之前介绍的Bi-LSTM 模型在语义解析任务中有很好的性能表现，但仍然存在两方面的限制。一方面是计算能力的限制。需要记忆的信息越多，模型结构越复杂，需要的计算能力越高。然而目前计算机的计算能力达不到复杂模型的设计要求，计算能力的不足依然在限制神经网络的发展。另一方面是优化算法的限制。虽然局部连接，权重共享等优化操作可以简化神经网络模型结构，有效缓解模型复杂度和表达能力之间的矛盾，但是仍然存在循环神经网络中长距离依赖等问题，这大大限制了信息的“记忆”能力。 
为了解决上述问题，Jang Y等人研究并提出了注意力机制。注意力机制最早被人们应用于计算机视觉领域，称为视觉注意力机制。视觉注意力机制是人类视觉所特有的大脑信息处理机制，人们在观察一幅画的时候，并不是需要完全看清这幅画的每一个像素才能了解画的意思，而是只需要观看这幅画的关键部分就能够了解其所表达的含义。
从此注意力机制及其变形[44,45,46,47]越来越多地运用在各个领域。注意力机制的具体工作流程如图2-5 所示。 
图2-5 注意力机制工作流程图 注意机制的工作流程主要分为以下三个步骤： 
（1）将输入的信息向量与每个相关单元进行相似度计算获得注意力权重。假设输入一个n 维的信息向量，对于输入的每一个，都可以看作由一个<>数据对构成。函数表示相似度计算函数，通常使用点积，拼接和感知机等方法。
是一个与目标输出相关的目标向量，由先前步的输出12( ,,...,)nXx xx=()1,2,...,ix in=,iiKey Value()SimilarityQuery

第二章 相关理论和技术 
 19 压缩得到。第一步主要是根据和来计算两者的相似性或相关性。其数学表达如下： 
 ， (2-5)  ， (2-6)  ， (2-7)  ， (2-8) 其中式（2-5）是由Graves等人提出的余弦相似度函数方法；式（2-6）是由Chorowski等人提出的线性相加的方法；式（2-7）是由Luong等人提出的点积的方法，其中的，，是函数引入的外部矩阵；式（2-8）是由Vaswani等人提出的在点积函数的基础上加入缩放因子的方法。以上四种方法都是用来计算两者之间的相似度的，目的是为了避免下一步骤中的函数出现梯度极小，难以计算的情况。 
（2）对这些注意力分值进行归一化处理。计算注意力系数，使用像这类的函数，将原本计算的数值转换成为权重和为1 的概率分布。其数学表达形式如公式（2-9）所示： 
 ， (2-9) （3）计算!""#$"%&$值。将之前学习到的注意力系数与输入信息的值通过加权求和运算，获得!""#$"%&$值。其数学表达形式如公式（2-10）所示： 
 。 
(2-10) 

2.3 本章小结 
本章首先对句向量技术及其分类进行了详细的介绍，包括有句子的有监督表示学习、无监督表示学习和多任务表示学习；其次分别对几种常用于文本向量编码的深度学习模型进行了介绍与分析，包括Quick-thoughts 句向量模型和Transformer-XL 自注意力机制模型；接着对用于回归预测的CNN 模型以及用于学习文本上下文特征的Bi-LSTM 神经网络模型进行了介绍和分析；最后介绍了注意力机制的使用特点。 
  QueryKey(),iiiKey QuerySimilarity Key QueryKeyQuery=××()(),tanh,TiiSimilarity Key QueryvW Key U Query=××(),iiSimilarity Key QueryKey Query×=(),iiKey QuerySimilarity Key Queryn×=vWU()softmaxia()softmax1ijSimilarityinSimilarityjeae==åiaValue()1,niiiAttention X Querya Value=×=å华南理工大学硕士学位论文  20 

第三章 海关虚假贸易罚款金额预测模型研究 
针对海关虚假贸易案件罚款金额预测问题，本章提出了一种基于深度学习的海关虚假贸易罚款金额预测模型，本章节将对所提模型进行详细介绍。具体而言，本章首先介绍本文所用到的符号及定义，以及本文所要解决问题；随后，介绍如何引入Transformer-XL 来对句向量模型Quick-thoughts 进行优化及改进，以获取案件描述的向量表示；最后，介绍如何基于上述所得向量表示，使用Encoder-Decoder 框架来预测获取最终的罚款金额，并在此过程中，着重讨论了如何处理案件句子之间的语义差异对最终处罚金额影响不同的问题。 


3.1 符号定义与问题描述 


3.1.1 符号定义 
本节中，将首先简单介绍本文所涉及的术语、符号，然后对本文所研究的问题进行简单描述。为方便阅读，表3-1 展示了本文所涉及的主要符号及其说明。 
符号 符号说明  一个案件文本的一个句子序列  一个案件文本的句子个数  案件文本数据集中的案件文本序号  一个案件文本  一个案件文本的真实罚款金额  训练集案件文本个数  案件文本数据集  真实罚金数值集合  训练数据集  句向量表示文本序列  经过Bi-LSTM 网络得到的文本向量  经过注意力机制对应的特征向量  句向量维度  预测当前案情罚款金额数值 snix( )( )( )( )12(,,...,)iiiinxsss=yMX(1)(2)(){,,...,}MXxxx=Y(1)(2)(){,,...,}MYyyy=D(1)(1)(2)(2)()(){(,),(,),...,(,)}MMxyxyxyw( )( )( )( )12(,,...,)iiiinwwww=h( )( )( )( )12(,,...,)iiiinhhhh=c( )( )( )( )12(,,...,)iiiincccc=dˆy

第三章 海关虚假贸易罚款金额预测模型研究 
 21 在海关虚假贸易案件罚款金额预测任务中，假设有一个包含M 个简要案件描述文本的集合，同时，有与案件描述文本对应的真实罚款金额集合为，其中表示第i 个案件描述文本，表示第i 个案件描述文本对应的真实罚款金额。而对于第i 个案件描述文本而言，其由n 个句子序列组成，即，其中表示第i 个案件描述文本的第j 个句子序列。
因此，训练数据集具体定义为，其中，每个样本表示案件的真实罚款金额是。 


3.1.2 问题描述 
基于3.1.1 小节的概念与描述，本文研究的海关虚假贸易案件罚款金额问题可以定义为： 
输入：一个案件描述文本。 
输出：对于给定的一个海关虚假贸易案件，首先通过句向量编码模块的文本预处理部分进行分句、文本标准化处理等操作后，再传入其句向量编码部分生成对应的案件文本语义向量表达，其中为第i 个案件描述文本的第j 个句子对应的案件文本语义句向量表达；然后基于该文本语义向量表达，在罚款金额预测模块先通过Bi-LSTM Encoder 部分，生成能够捕捉双向语义的文本向量表达；为了更好地体现各个句子对罚款金额的不同影响，引入注意力机制生成特征语义向量表达；最后通过CNN Decoder 部分，输出对该案件文本最终的罚款金额预测值。 


3.2 整体模型概述 
图3-1 展示了本文提到的海关虚假贸易案件罚款金额预测模型的整体网络结构。 
海关虚假贸易案件罚款金额预测模型进行罚金预测的过程包含以下几个模块：
（1）句向量编码模块：将输入的案件描述文本经过文本预处理生成符合统一规范的句子序列，再将处理好的句子序列传入到基于Transformer-XL 改进的Quick-thoughts 句向量模型(Improved Quick-thoughts model based on Transformer-XL，TQT)中，生成案件文本的语义向量表达；（2）罚款金额预测模块：
采用Encoder-Decoder 的框架进行设计。 
( )( )( )()12{,,...
,...
,,}iMXxxxx=( )( )( )()12{,,...
,...
,,}iMYyyyy=( )ix( )iy( )( )( )( )( )12(,,...,,...,)iiiiijnxssss=( )ijs(1)(1)(2)(2)()(){(,),(,),...,(,)}MMDxyxyxy=( )( )(,)iixyDÎ( )ix( )iy( )( )( )( )12(,,...,)iiiinxsss=( )ix( )( )( )( )( )12(,,,,,)iiiiijnwwwww=…( )ijw(iw ）( )( )( )( )( )12(,,,,,)iiiiijnhhhhh=…( )( )( )( )( )12(,,,,,)iiiiijnccccc=…( )ˆ iy( )ix( )iw华南理工大学硕士学位论文  22  图3-1 基于海关虚假贸易案件的罚款金额预测模型框架图 为学习文本深层次语义信息，根据（1）得到的案件文本向量表达，在Encoder-Decoder框架的Encoder 端引入了可关注于句子语义级别文本建模的Bi-LSTM 模型；为重点关注重要的文本句子对最终罚款金额的影响，在Encoder 端和Decoder 端中间引入Attention机制；最后，使用一个CNN 预测模块作为Decoder 端，根据注意力机制输出的特征向量序列进行案件罚款金额预测，输出模型最终的罚款金额。
为了更好的理解模型的罚款金额预测过程，下面将详细地介绍每个模块的具体结构以及基本原理。 


3.3 句向量编码模块 
该模块的输入为海关虚假贸易的简要案件描述文本，输出为该案件文本的语义向量表。如图3-2 所示，该模块的主要分成两个部分，第一部分是文本预处理，对案件描述文本进行文本预处理，使文本在送入句向量模型前得到统一规范；第二部分是对案件描述文本的句向量生成，在该部分设计了一种基于Transformer-XL 改进的Quick-thoughts 模型来生成对案件描述文本句子的语义向量表达。下面将对这两部分的内容分别进行详细的介绍。 
 ( )iw( )ic( )ˆ iy( )ix( )( )( )( )12(,,...,)iiiinwwww=图3-2 案件文本句向量编码模块示意图 

第三章 海关虚假贸易罚款金额预测模型研究 
 23 

3.3.1 文本预处理 
文本预处理是为了剔除语料库中的一些杂质，且在文本送入改进的Quick-thoughts句向量模型前，对文本进行统一规范。预处理的流程主要包含以下几个方面： 
（1）分句。将输入的案件描述文本，通过分句工具分解成一个个句子序列，其中，n 表示案件描述文本中句子的数量。常用的中文分句工具有：
Jieba、SnowNLP、THULAC 和NLPIR 等等；本文模型使用Jieba 分句工具对案件描述文本进行分句的操作； （2）文本标准化处理。
使用正则表达式来对文本中的一些杂质，如脱敏处理后的乱码随机数、停用词等，进行剔除操作；此外，由于数据中包含了大量的时间、车牌号、手机号等信息，这些信息将对结果产生干扰作用，因此将其用统一代词进行表示，如表3-2 所示；最后，案件文本中不仅包含阿拉伯数字，还存在一些中文数字，因此将文本中的数字符号统一为阿拉伯数字。经过文本标准化处理的案件文本表示为。 
数字类型 示例 代词名称 时间数字 2008 年8 月31 号 TIME 车牌号 粤S18525 CAR 货物商品号 商品编码85466847 GOODS 产品型号 电脑型号1001PX-WHI029X MODEL 海关关口号 关口B2 PASS 为方便理解，图3-3 给出了一个文本预处理操作实例。
在该实例中，案件文本为“当事人于2004 年8 月31 日持******号报关单向海关申报出口******号一般贸易合同项下，由粤S18525 号车载运的工艺蜡烛2670 公斤。
经查，发现实际出口工艺蜡烛2670公斤。另有化纤针织绒布包290 公斤出口未申报，价值人民币8551.73 元。”，按照文本预处理的操作流程，先对进行分句处理，得到“当事人于2004 年8 月31 日持******号报关单向海关申报出口******号一般贸易合同项下，由粤S18525 号车载运的工艺蜡烛2670 公斤。”、“经查，发现实际出口工艺蜡烛2670 公斤。”、“另有化纤针织绒布包290 公斤出口未申报，价值人民币8551.73 元。”共3 条分句。为方便起见，本文( )ix( )( )( )12(,,,)iiinsss( )( )( )( )12ˆˆˆˆ(,,,)iiinixsss=( )ix( )ix华南理工大学硕士学位论文  24 取第一条分句作为文本标准化处理的演示。
该句中所包含有的时间“2004 年8 月31 日”、脱敏处理后的乱码“*******”以及车牌信息“粤S18525”等一些杂项，因此，文本标准化处理会去除掉句子序列中的上述乱码信息，并按照表3-2 将时间信息替换成“TIME”、车牌信息替换成“CAR”这类代词表示。再将文本标准化处理过后的句子序列传入到句向量编码部分。 
 图3-3 文本预处理操作示例图 3.3.2 基于Transformer-XL 改进的Quick-thoughts 模型 本文第二章对句向量技术进行了详细介绍，其中Quick-thoughts 句向量模型可以以无监督学习的方式生成句子文本向量表示。在Quick-thoughts 句向量模型中，编码器使用GRU 作为参数化函数，以一个文本句子作为模型的输入，将其训练编码成为一个固定长度的文本向量。但是该模型在处理较长文本时，因GRU 存在固有的长距离信息传播损失问题，会存在较大误差，降低了所得语义向量表示的准确性。因此，为得到更为准确的句子语义向量表达，本文在模型句向量编码部分使用改进的Quick-thoughts 句向量模型，即使用完全基于注意力网络的Transformer-XL 来替代Quick-thoughts 句向量模型的GRU 模块，生成更富含语义信息的句向量表达。具体而言，图3-4 展示了改进的Quick-thougths 句向量模型编码的流程图，输入第i 个案件描述文本的句子片段，首先通过Word2Vec 向量初始化，将案件描述文本句子转换成其各自的向量表达形式，然后通过多头注意力层，生成代表各个句子注意力的向量表达形式，最后通过前馈神经网络层生成输出其各自的语义向量表达。 
( )( )( )12ˆˆˆ(,,,)iiinsss( )( )( )( )12ˆˆˆˆ(,,...,)iiiinssss=( )( )( )( )12(,,...,)iiiinoooo=( )( )( )( )12(,,....,)iiiinzzzz=( )( )( )( )12(,,...,)iiiinwwww=

第三章 海关虚假贸易罚款金额预测模型研究 
 25  图3-4 基于Transformer-XL 改进的Quick-thoughts 模型编码流程图 l 多头注意力层 多头注意力层的输入是使用Word2Vec 进行向量初始化后的句向量，维度为d。
多头注意力层是由多个不同自注意力组合而成。在自注意力中，输入序列的每个句子都有三个不同的向量，分别是query 向量（简称q 向量）、key 向量（简称k 向量）和value向量（简称v 向量），它们的向量维度为，并且在计算q 向量和k 向量时，引入了相对位置编码。图3-5 展示了第t 个句子的多头注意力层的计算流程，以目标句子与案件描述文本中的另一个句子为例，介绍该多头注意力层的计算流程。
首先计算句子的q 向量、k 向量和v 向量，并且在计算q 向量和k 向量的过程中引入相对位置编码： 
 ， (3-1)  ， (3-2)  ， (3-3) ( )ioqkvdddd===( )ito( )ipo( )( )iqitqW or=+( )( )ikikptpkW oR=+( )( )ivitvW o=华南理工大学硕士学位论文  26  图3-5 多头注意力层流程图 其中、和为q、k 和v 向量的权重矩阵，为句子的q 向量对于句子的相对位置编码，因为句子相对于句子的相对位置是个固定值，因此用r 表示，为句子的k 向量对于句子的相对位置编码。接下来将计算句子对句子的打分（score），采用q 向量和k 向量相乘的方式得到，这个分数决定了在编码句子的过程中，对案件描述文本句子的重视程度： 
 ，  (3-4) 其中表示句子对句子的打分，为k 向量的维度，作为缩放因子能使梯度更加稳定。接下来使用softmax 函数对所有句子的得分进行分数归一化： 
 ， (3-5) 其中表示归一化后的分数。接着将每个归一化后的分数与其对应句子的v 向量加权求和得到这一层自注意力的输出结果： 
 。  (3-6) qddqW´ÎkddkW´ÎvddvW´ÎdrÎ( )ito( )ito( )ito( )itokdtpR Î( )ipo( )ito( )ipo( )ito( )ito( )ipo( )( )( )iiitpkqked×=( )itpe( )ipo( )itokd( )( )( )1exp()exp()itpitpnitpkeea==å( )itpa( )ipo( )itz( )( )1niittptpzva==×å

第三章 海关虚假贸易罚款金额预测模型研究 
 27 l 相对位置编码 模型将位置信息注入到求Attention 分数的过程中，即将相对位置信息编码加入到隐藏状态中去，引入相对位置编码来表示案件文本中的句子的顺序关系，考虑与之依赖句子的相对位置关系，这样模型就能确认不同位置的句子。使用正余弦函数来对案件描述文本中的不同句子的位置信息进行编码，并不是通过学习获得，因此在模型预测时可以使用比训练距离更长的位置向量。
具体而言，句子相对于句子的相对位置编码的计算方式如下： 
 ， (3-7)  ， (3-8) 其中是指句子相对于句子的位置距离，，其中n 表示案件描述文本的句子数；j 是句向量的某一维度，，其中d 是句向量的维度；是位置向量的维度，与句向量维度相同。 
相对位置编码的每一个维度对应于正弦曲线，波长构成了从到10000的等比数列。而对于句子的q 向量对于句子的相对位置编码，因为句子相对于句子的位置是不会变的，因此这里引入可训练的参数，使得所有句子相对于句子的q 向量都是相同的。 
l 求和归一化 如图3-4 所示，本文在多头注意力层和前馈神经网络之后都设计有一个求和归一化层（Layer Normalization，LN）。LN 是针对批量归一化（Batch Normalization，BN）的不足提出来的一种归一化数据的方式，与BN 不同的是，LN 将综合考虑一层所有维度的输入，将输入的信息转化成在某一范围内稳定的数据，以保证数据分布的稳定性，加速模型的优化速度。具体而言，本文求和归一化层操作如公式（3-7）所示： 
 ， (3-9) 其中是案件描述文本第t 个句子的向量初始化编码，是多头注意力的输出向量，的维度与一样；是平移参数，是缩放参数，b 是再平移参数，g( )ito( )ipotpR(,2 )2 /sin 10000rtptpdj dposRæö=ç÷èø(,21)2 /cos 10000rtptpdj dposR+æö=ç÷èøtppos( )ito( )ipo[0,1,...,n 1]tppos Î-[0,1,...,/ 2]jdÎrd2p2p´( )ito( )ito( )ito( )itodrÎ( )ipo( )ito( )( )( )ˆ(gb)iiitttozzfµs+-=×+( )idtoÎ( )idtzÎ( )ˆ itz( )itzµs华南理工大学硕士学位论文  28 是再缩放参数，这四个参数均为标量，因此所有输入共享一个规范化变化，最终得到的数值符合均值为b 方差为的分布。 
LN 主要针对单一训练样本进行，不依赖于其它数据，因此可以避免BN 中受到小批量样本数据分布影响的问题，即不需要保存小批量样本数据的均值和方差，节省了额外的存储空间。 
l 前馈神经网络 在归一化层之后，每个输出都接入了全连接的前馈神经网络（Feed-Forward network，FFN）。FNN 先对进行线性变换，即将案件描述文本每个句子位置的注意力结果映射到一个更大维度的特征空间，然后引入ReLU 函数进行非线性筛选，最后再通过线性变化将结果恢复到原始维度，具体计算过程见公式（3-11）： 
 ，  (3-10) 其中是经过求和归一化的多头注意力层的输出结果，维度为，n 代表案件描述文本的句子数，d 代表句向量的维度；参数矩阵、，偏置向量和，为前馈神经网络的维度，因此该前馈神经网络层的输出结果维度仍为。由图3-4 可知，FNN 层的输出结果就是句向量编码部分每个子层的输出结果，维度为。 
l 循环机制 在本文改进的Quick-thoughts 模型中，引入了句子与句子之间的循环机制，使得当前句子在建模的时候能够利用之前句子的信息来实现长期依赖性，即在处理后面的新句子时，每个隐藏层都会接收两个输入，第一个是本句的隐藏层的输出；第二个是前面句的隐藏层输出，这一部分可以使模型创建长期依赖关系。这两个输入会被拼接，然后用于计算当前句子的key 向量和value 向量。循环机制示例如图3-6 所示。 
在改进的Quick-thoughts 模型中，假设包含m 层的句向量编码，那么每个句子序列中就有m 组隐向量序列。将第t 个句子序列的第m 层隐向量表示为，其中d表示隐向量的维度，则对于模型的第t 个句子第m 层隐向量可以计算得出，首先对第t 个句子前一层m-1 的隐向量进行计算，如公式（3-12）所示： 
 ， (3-11) 2g( )ˆ iz( )1122ˆFNNReLU()iz Wb Wb=++( )ˆ izn d´1fnnd dW´Î2fnnddW´Î11fnndb´Î12fnndb´Îfnndn d´n d´( ),itmdwÎR( ),i mtw( ),1i mtw-()( ),1( ),11( ),1i mi mti mttwSG ww----éù= ëû"

第三章 海关虚假贸易罚款金额预测模型研究 
 29  图3-6 循环机制示例图 其中t 表示案件文本的第几个句子，m 表示第几层的句向量编码，表示当前句子前一隐藏层的输出，如图3-6 中蓝线部分所示；表示停止计算梯度，即不再对前一句子的隐向量做反向传播，该部分如图3-6 中绿线部分所示；表示对两个隐向量序列在长度维度上的拼接，拼接后的维度是2d。因此，对第t 个句子的第m 层的key和value 向量和的计算过程，如公式（3-13）和（3-14）所示： 
 ，  (3-12)  ， (3-13) 其中是和分别对应key 和value 的转化矩阵。 
因此，最终改进的Quick-thoughts 模型将图3-4 这样的结构向上重复叠加m 次，即进行m 次这样的运算，在这部分使用循环机制来得到句向量编码模块对案件描述文本的句向量表达形式，用表示，维度为。 


3.4 罚款金额预测模块 
当前海关虚假贸易案件罚款金额预测的相关研究工作主要采用将文本输入到CNN模型中，经过卷积池化全连接等操作，迭代训练，最终预测的方式来获得。然而这种使用CNN 模型进行预测罚款金额的方式存在两个缺点：一是简单的CNN 模型无法很好的对案件描述文本进行语义分析，因此并不能很好地学习得到案件的罚款金额，即模型( ),m 1itdw- Î()SG[]u v( ),i mtk( ),i mtv( ),( ),1i mi mktptktkW wR- +=(1( ),),vi mi mttvW w-=2kddW´Î2vddW´Î( )( )( )( )12(,,...,)iiiinwwww=n d´华南理工大学硕士学位论文  30 的准确率较低；二是单一的文本向量表达输入忽略了案件上下文之间的相互关系。针对上述问题二，简单地引入LSTM 进行罚款金额预测地方法，仍然存在对文本段落的表达不够充分的问题，进而影响罚款金额准确性。因此本文根据海关虚假贸易案件罚款金额任务的特点，采用Encoder-Decoder 模型结构设计罚款金额预测模块，以充分获取文本语义信息，最终实现对罚款金额的准确预测。 
在3.3.2 小节已经得到的维度为的案件描述文本句向量形式表达的基础上，本文结合任务的特点，设计了如下图3-7 所示的罚款金额预测模块，该模块包含Encoder、注意力机制和Decoder 三个结构。为更好的捕捉学习语义信息用于罚款金额预测任务，该模块采用Bi-LSTM 网络作为模块的Encoder，将Bi-LSTM 网络中低层用于从句向量表达中提取文本向量的语法信息，高层用于捕捉文本向量的语义特征；为使Encoder获取文本中不同句子对于罚款金额预测结果的不同权重，该模块在Encoder 与Decoder之间引入注意力机制；最后，使用CNN 作为Decoder 来预测具体的罚款金额数值。
下面将对各部分内容进行详细介绍。 
 图3-7 基于Encoder-Decoder 的罚款金额预测模块结构图 

3.4.1 Bi-LSTM Encoder 
将词的表示组合成句子的表示，可以采用相加的方法，即将所有词的表示进行加和，或者取平均等方法，但是这些方法并没有考虑到词语在句子中前后顺序。
使用LSTM 模型可以更好的捕捉到较长距离的依赖关系。
因为LSTM 通过训练过程可以学到记忆哪些信息和遗忘哪些信息。但是利用LSTM 对句子进行建模无法编码从后到前的信息，而Bi-LSTM 可以更好的捕捉双向的语义依赖。 
因此在本文所设计的Encoder-Decoder 框架中，Encoder 由一层Bi-LSTM 构成。Bi-LSTM 输入端直接与句向量编码模块的输出端连接，将句向量编码模块输出的文本句向n d´( )iw( )iw( )iy

第三章 海关虚假贸易罚款金额预测模型研究 
 31 量表示作为输入，通过Bi-LSTM Encoder 进行向量编码生成包含双向语义的序列，Encoder 模块结构如图3-8 所示。 
 图3-8 Bi-LSTM Encoder 结构示意图 第i 个案件描述文本的语境化句向量表达输入到Encoder 中，Encoder 的前向LSTM 层输入为从0 时刻起对案件描述文本向量的正向输入，得到并保存每一时刻的前向隐含层状态，即： 
 ， (3-14) 其中，表示前向LSTM 的权重系数，表示当前t 时刻的文本输入，表示前一时刻的前向LSTM 层输出，表示前向LSTM 层编码。 
反向LSTM 层输入为从末尾时刻到起始时刻对案件描述文本向量的反向输入，得到并保存每一时刻的反向隐含层状态，表示为： 
 ，  (3-15) 其中，表示反向LSTM 的权重系数，表示当前t 时刻的文本输入，表示后一时刻的反向LSTM 层输出，表示反向LSTM 层编码。 
Bi-LSTM Encoder 的最终输出为前后向LSTM 隐含层输出的拼接，维度为2d，表示为： 
 。  (3-16) ( )( )( )( )12(,,...,)iiiinwwww=( )( )( )( )12(,,...,)iiiinhhhh=( )( )( )12(,,...,)iiinwww( )ith()( )( )( )111iiitlstmftthfUwW h -=×+×1d dU´Î1d dW´Î( )itw( )1ith -()lstmff( )ith()( )( )( )221iiitlstmbtthfUwWh +=×+×2d dU´Î2d dW´Î( )itw( )1ith +()lsmbf( )ith( )( )( );itttiihhhéù= ëû"华南理工大学硕士学位论文  32 

3.4.2 注意力机制 
案件最终罚款金额取决于申报进出口情况、实际进出口情况和案件价值等众多因素，而不同因素对案件最终罚款金额的影响不同。因此，罚款金额预测建模任务面临的最大挑战是如何根据不同的影响为这些因素分配权重。
传统建模方法通常直接把案情文本的句向量拼接在一起送入CNN 模型，但是这种方式很难捕捉到目标语义，导致预测效果并不理想。同时案件文本的不同句子将对最后的罚款金额预测结果产生不同的影响，如相比于其它文本，涉及“案值”、“涉案金额”、“涉案数量”等内容的案件文本描述对最后的罚款金额影响更大。针对该问题，本文引入注意力机制以捕获案件描述文本中针对罚款金额的重要信息。 
为使用注意力机制捕获案件描述文本中得每个句子对最终罚款金额的不同影响，本文在罚款金额预测模型中引入加法注意力机制模块。
本模块给文本序列赋予不同的权重，最后聚合句子的隐藏表示产生一个特征向量。注意力机制模块的结构如图3-9 所示。 
 图3-9 注意力机制结构 输入为第i 个案件描述文本句向量通过Encoder Bi-LSTM 隐藏层后的拼接输出，通过注意力机制模块计算出每个句子向量对应的文本特征向量。其中为的加权和，表示为： 
 ，  (3-17) ( )( )( )12(,,...,)iiinccc( )( )( )12(,,...,)iiinhhh( )( )( )12(,,...,)iiinccc( )itc( )ith( )( )( )1ntittppiica h== å

第三章 海关虚假贸易罚款金额预测模型研究 
 33 其中表示为第t 个句子对应的文本特征在第p 个句子中的权重值的大小，其计算方式表示为： 
 ，  (3-18) 其中score 为concat 对齐函数，其计算方式表示为： 
 ，  (3-19) 其中表示与案件罚款金额有关内容的向量表达，为权重矩阵。 


3.4.3 CNN Decoder 
Decoder 端旨在预测案情最终的罚款金额，本文采用CNN 卷积神经网络来执行该预测过程，整体结构如图3-10 所示。 
 图3-10 Decoder 模块结构示意图 CNN 的第一层为输入层。输入层的输入为案件描述文本对应的每个句子的特征向量拼接成的文本矩阵。在该文本矩阵中每行是对应句子的句向量，维度为d；然后输入文本经过第二层卷积层，该卷积层的卷积核高度为h=（2，3，4，5），宽度与句向量维度保持一致；第三层为池化层，因为使用不同的卷积核所生成的句子向量也是不同长度的，这里将不同高度的句子向量变成固定的长度，本文选择使用最大池化层以能最大限度的捕获文本所表达最重要的语义信息；第四层为Flatten 层，作用是把多维的向量一维化；最后一层为全连接层，用来输出最终罚款金额的预测数值。
接下来将对CNN Decoder 模块进行详细介绍： 
输入层：
如图3-10 所示，为案件文本经过注意力机制的向量化表示。
将案件文本句子对应的句向量表示按文本顺序进行排列，可得到句向量文本矩阵。
假设某一案件文本句子数目为n，且句向量的维度为d，根据矩阵运算规则，( )itpa( )ith()()()()( )( )( )1expexpititippnscore hascore h==å()()( )T( )tanh Wiittscore hvh=×dvÎ2Wdd´Î( )( )( )12(,,...,)iiinccc( )ˆ iy( )( )( )12(,,...,)iiincccn dH´Î华南理工大学硕士学位论文  34 句向量矩阵的维度为n*d。紧接着模型将对矩阵进行padding 操作，目的是为了处理不同的案件文本描述的句子数量不完全相同的问题。具体而言，假定一个案件文本的最大句子数量为k，将矩阵的维度扩充为固定大小k*d，此时该案件描述文本表示为，句向量文本矩阵扩展为。当出现数据样本不够问题时，缺少的矩阵数值（n<k）可以选择使用全0 填充或者使用随机填充的方法。案件描述文本矩阵的结构如下图3-11 所示。 
 图3-11 案件描述文本矩阵图 卷积层：
卷积层的目的是对文本中的语义特征进行提取。
在本文的CNN Decoder 中，卷积层设置有m 种不同高度的卷积核，其中h 代表卷积核的高度，也就是卷积核所覆盖的纵向句子个数，d 代表句向量的维度。每种高度卷积核的数量为num，因此输入层的输出文本矩阵在卷积层通过这m 种卷积核进行卷积运算后，将生成个FeatureMap，每个FeatureMap 表示为，具体计算过程如下： 
 ，  (3-20)  ，  (3-21) 其中表示对中间的数值进行向下取整运算；和分别为经过卷积运算后输出FeatureMap 的高度和宽度；和分别代表输入的句向量文本矩阵H 的高度和宽度，也就是最大句子数k 和句向量维度d；和分别代表卷积核的高度h和宽度d；pad 代表填充值的大小，本文设置为0；stride 代表卷积操作滑动的步长，本( )( )( )( )12(,,...,)iiiikcccc=k dH´Îh dkernel´ÎHm num´height weightI´Î21khheightHkernelpadIstride-+êú=+êúëû21ddweightHkernelpadIstride-+êú=+êúëûê úë ûheightIweightIkHdHhkerneldkernelkernel

第三章 海关虚假贸易罚款金额预测模型研究 
 35 文设置为1。最后输入的句向量文本矩阵H 经过卷积层的卷积操作后的输出，维度为。 
池化层：本文在卷积层后设计了池化层，目的是为了防止模型过拟合，降低卷积层输出的特征向量维度，减轻模型的计算量，提高模型的学习训练效率。池化层的池化操作可分为最大池化（Max-Pooling）、全局最大池化（Global Max-Pooling）、平均池化（Mean-Pooling）、全局平均池化（Global-Average）和随机池化（Stachastic-Pooling）等几个类型。本文考虑到罚金任务应提取最重要的特征信息，并过滤掉无用的语义信息的特点，选择最大池化方法来对卷积层的输出向量中最重要的特征信号进行过滤筛选，输出其特征信息中的最大值，计算过程如下： 
 ，  (3-22) 其中为某一高度的一个卷积核得到的列向量经过最大池化运算后的输出值；表示最大池化函数。在本文中，共设置有4 种高度不同的卷积核，其中每一种高度的卷积核100 个。在卷积操作后，对这100 个卷积核得到的列向量进行最大池化操作，得到4 个100 维的特征向量 Flatten 层：
在池化层之后，将池化层输出的4 个100 维的特征向量“压缩”到一维，即把多维的输入一维化，得到一个特征向量来对整个案件描述文本的语义特征进行表达。该层是到全连接层前的过渡，Flatten 层的操作不影响batch 的大小。 
全连接层：连接特征向量计算得到罚款金额预测的具体数值。首先对Flatten 层输出得特征向量进行Dropout 处理，以减少过拟合、保证准确率并提高训练效率，然后通过全连接层输出罚款金额的预测值。具体过程如下： 
 ，  (3-23) 其中L 表示全连接层的层数，，和分别表示第L 层全连接层的权重矩阵、偏置向量和输出结果，和分别是输出层的权重矩阵和偏执向量。本文使用ReLU 函数来产生非线性决策边界增加非线性，使用Dropout 函数来改善过拟合的情况，提高模型的泛化能力。 
I(1) 1kh- +´ImaxI( )maxmaxIfI=maxII()maxf()25h££( )1 4000ig´Î( )ˆ iy( )ˆ iy( )( )1011( )( )2122( )( )L00=(() Wb )=(() Wb )...
ˆW +biiiiiigReLU DropOut ggReLU DropOut gygì×+ï×+ïíïï=×î400 400WL´ÎbL Î( )1 400iLg´Î400 10W´Î0b Î华南理工大学硕士学位论文  36 3.5 本章小节 本章从任务需求出发，针对海关虚假贸易罚款金额预测任务设计并建立罚款金额预测模型。首先介绍了该模型的整体结构；然后结合任务特点，详细介绍了本文的罚款金额预测模型：分为句向量生成模块和罚金预测模块两个部分。句向量生成模块介绍了采用的改进的Quick-thoughts 句向量语言模型，及对其进行预训练得到语境化句向量的过程；罚金预测模块采用Encoder-Decoder 框架进行设计，使用Bi-LSTM 作为Encoder 模块，用于学习文本深层语义信息；使用CNN 作为Decoder 模块，得到案情的罚款金额预测结果；在框架中引入注意力机制，更好地捕捉文本各个句子对案情罚款金额的影响。
 

第四章 实验设计与结果分析 
 37 

第四章 实验设计与结果分析 
本章将对海关虚假贸易罚款金额预测模型的实验部分进行分析介绍。具体而言，主要介绍实验所用到的数据集及其预处理方式、对比模型、评价指标以及实验环境等基础设置；然后详细介绍实验结果并对实验结果进行详尽分析。以下，将对各部分展开详细介绍。 


4.1 实验数据集 


4.1.1 数据描述 
本文实验所使用的数据集来自于某海关已查获的虚假贸易案件的真实数据集。
该数据集包含了2003 年6 月至2018 年6 月共计1 万6 千多条样本。每条样本包含三种信息：
（1）简要案情的基本描述，以及与之相关的基本信息如商品名称、商品数量等；（2）案件的审批相关单位信息；（3）案件的处罚决定内容。
本文将海关虚假贸易数据集的所有字段汇集起来，形成表4-1 所示的样本数据格式。 
序号 属性 描述 1 案件编号 1 2 案件名称 CP00000004 违规出口申报不实案 3 贸易方式 一般贸易 4 进出口代码 出口(境) 5 简要案情 当事人于2004 年8 月31 日持******号报关单向海关申报出口******号一般贸易合同项下，由粤S18525 号车载运的工艺蜡烛2670 公斤。
经查，发现实际出口工艺蜡烛2670 公斤。另有化纤针织绒布包290 公斤出口未申报，价值人民币8551.73 元。 
6 立案定性 违规 7 立案审批日期 2004/9/22 8 立案单位 海关缉私局查私一科 9 案件种类 一般案件 10 罚款金额 1000 11 执行完毕日期 2005/1/10  华南理工大学硕士学位论文  38 在真实数据集中，所有案件数据的的“贸易方式”和“案件种类”两个字段的内容都是单一的，分别为“一般贸易”和“一般案件”；在1 万6 千多条数据中，99% 的案件数据在“立案定性”字段是“违规”，“走私”性质的案件仅有3 例。由此可知，数据集案件类型单一，多数为违规案件。因此本文不考虑判断海关虚假贸易案件违规与否的分类任务。此处仅使用该数据集中“简要案情”和“罚款金额”两个字段的数据：在输入端输入“简要案情”文本信息，在输出端预测“罚款金额”具体数值。 


4.1.2 数据分析 
本文将将该数据集划分为三部分：训练集、验证集和测试集。其中数据集的80%用于模型训练，10%用于交叉验证，剩余10%用于最终测试。最终实验所用数据统计如表4-2 所示。这里将通过绘制表格和直方图的形式，对该数据集的情况进行详细介绍。 
数据集 样本数/千 训练集 13 验证集 2 测试集 2 通过对大量的海关违规案件进行整理与统计发现案情描述中出现的金钱信息、货物种类信息以及货物数量信息对罚款金额的影响最为重要。
真实罚款金额的数据分布见图4-1。由该图可以得知，海关虚假贸易案件的罚款金额主要集中在500-50000 元之间，且罚款小额与超大额的案件数目较少。 
 图4-1 罚款金额数据分布直方图 

第四章 实验设计与结果分析 
 39 

4.2 实验数据预处理 


4.2.1 预测值变化 
从海关虚假贸易案件数据的罚款金额数值分布中可以发现，案件罚款金额的真实数值呈现出跨度较大、分布不均且在某一区间数量较大的特点。如图4-1 所示，本文所用数据集中罚款金额的真实数值在0 元到100 万元之间，且主要在500-50000 元这个数据区间集中。因此，针对该数据集真实罚款金额的分布特点，如何设计加强此类数据的拟合和预测效果，提高最终模型的预测准确率，是一个亟待解决的问题。 
预测值变换是一种案件数据的重要预处理方法，可使原本密集的区间的值变得尽可能分散，使原本分散的区间的值变得尽可能聚合。 
因此在罚款金额预测模型预处理过程中，本文使用预测值变换中的对数（log）变换方法来对罚款金额真实数值进行处理。对数变换的数学表达形式如公式（4-1）所示。 
 ， (4-1) 其中为经过对数变换后的罚款金额，为虚假贸易案件的真实罚款金额。 
对真实罚款金额数据进行预测值变换能够缩小模型的的预测范围，同时光滑模型的预测曲线。 


4.2.2 数字离散化 
数字离散化是在本文罚款金额预测的另一个重要预处理方法。在本文中，数字离散化是指对于案情描述中部分数字进行离散化处理，例如将案情描述文本中的“9522.35 元”转变为“10000 元”。 
本文使用数字离散化进行预处理主要考虑到以下两个方面的问题。首先，对于句向量生成任务，由于数字文本大多庞大且细致，目前使用的模型并不能很好的处理数字文本之间的关系。其次，对于罚款金额预测任务，简要案件描述文本中所包含的数字对违规违法案件的处罚结果从客观上就存在着一定的影响，比如案件描述中出现的违规进出口货物的价值、重量、数量等等数字。 
因此，为了准确预测罚款金额，将案情描述文本中的重要数字进行离散化是至关重要的一步。基于以上分析，本文使用数字离散化方法对案件文本中的相关数字信息进行预处理：对案件文本进行正则化匹配操作，定位和替换案件文本中的数字信息，生成新的案件文本。 
logyy=yy华南理工大学硕士学位论文  40 在实际的案件数字离散化实验中，数字信息不仅仅出现在金钱、重量等本文意图离散化的信息中，在车牌、时间等不需要离散化的信息中也存在着数字信息。因此，本文实验设置只对以“*个”、“*公斤”、“*元”等形式出现的数字信息进行离散化处理，其中“*”代表具体的数值。
对于案件描述文本中所出现的时间信息，如“*年*月*日”，车牌信息，如“粤*****”，报关单号信息等等中出现的数字不予处理。
本章实验所使用的数字离散化区间信息如表4-3 所示。 
类别 离散化区间 数量 0-50 个，50-100 个，100-1000 个，1000-5000 个，5000-10000 个，10000-50000 个，50000 个以上 重量 0-10 公斤，10-50 公斤，50-200 公斤，200-1000 公斤，1000-10000 公斤，10000-50000 公斤，50000-100000 公斤，100000 公斤以上 金钱 0-1000 元，500-4000 元，4000-10000 元，10000-50000 元，50000-100000元，100000-250000 元，250000-500000 元，500000-1000000 元，1000000元以上 图4-2 对案情描述文本的数字离散化过程进行举例说明，其中图左侧为原本的简要案件描述文本，图右侧为数字离散化操作后的简要案件描述文本，高亮区域为具体离散化的数值内容。 
 图4-2 案情离散化示例 案情离散化将数字离散化的方法应用到实际案件描述文本中，是一项重要的文本数据预处理方法。因此在本文最终的实验中，将案件离散化方法与上节介绍的预测值变换的方法相结合，作为案件描述文本输入模型前的预处理方法。 


第四章 实验设计与结果分析 
 41 

4.3 实验设置 
本文的实验基于Python 语言进行编写，并使用PyTorch 深度学习框架进行实现。
表4-4 展示了本文实验所使用计算机的相关配置信息。 
名称 配置 CPU GPU Inter(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz NVIDIA TITAN Xp@1.6 GHz 12GB 内存 存储 128GB 16TB HDD，以RAID5 模式运行 操作系统 Ubuntu 18.04.1LTS 编程语言 Python 3.6 64 位 软件包 numpy 1.8.0，pandas 0.25.3，tensorflow 1.15.0，pytorch 0.2.0 在本文具体的实验中，采用Jieba 工具对案件描述文本进行分句，使用Word2Vec 模型对句子序列进行向量初始化，训练模式采用CBOW 模型，上下文窗口数设置为3，句向量维度为200，采样值大小设为0.001。
在句向量编码模块，案件描述文本的最大句子数设置为10，采用6 层的Transformer-XL，和8 头注意力机制，隐藏层节点数设置为1024。在罚款金额预测模块，长短期记忆网络的输出向量维度为400，注意力机制特征输出向量维度为200，卷积核高度共设置有4 种，分别是2，3，4，5，每种卷积核的数量设置为100，全连接网络隐藏层数设置为3 层，神经元个数设置为512，Dropout 率设置为0.5。模型在优化过程中使用小批量Adam 算法，设置训练周期调整为60，批大小调整为200，学习率大小调整为0.001，并且每过10 个周期，学习率减少至原本的0.1。
 

4.4 实验评估指标 
本文实验使用了以下三种评价指标，根均方误差（Root Mean Square Error, RMSE），衡量预测值与真实值之间的偏差，其计算方式如公式（4-2）所示；平均绝对比例误差（Mean Absolute Percentage Error, MAPE），其计算方式如公式（4-3）所示；一致率，表示罚金预测模型预测结果一致的比例，即当模型预测值在真实值的一定范围以内时，即在和这个范围内（的取值在0 到1 之间），认为模型的预测值与真实值是一致的，其计算方式如公式（4-4）所示。 
 ， (4-2) 1 a-1 a+a()21 ˆNiiiRMSEyyEN=-= å华南理工大学硕士学位论文  42  ， (4-3)  ， (4-4) 其中为海关虚假贸易案件语料库中的罚款金额的真实值，为模型根据海关虚假贸易案件文本预测的预测值，为测试的案件文本数，Boolean 函数表示当在这个范围内时，函数值为1 否则为0。在一致率的公式中，的取值是自主设定的，在本文实验中将其设置为0.2。 


4.5 实验结果分析 
本小节首先对比分析了本文提出的罚款金额预测模型与其它模型的评估结果；其次，通过注意力机制演示了案件文本中不同句子对罚款金额预测任务的影响程度；最后，对本文模型中的重要超参数的敏感实验结果进行了展示分析。 


4.5.1 性能对比实验 
为评估模型在罚款金额预测任务上的性能，将在实验数据集上与如下相关模型进行对比： 
（1）Word2Vec+CNN：采用Word2vec 模型作为句向量训练模块，并抽取文本一系列表层特征，词典特征和语法分析特征。以CNN 作为罚款金额预测模块。 
（2）Doc2Vec+CNN：采用Doc2Vec 作为句向量训练模块，并抽取文本一系列表层特征，词典特征和语法分析特征。以CNN 作为罚款金额预测模块。 
（3）Transomer-XL+CNN：采用Transomer-XL 模型作为句向量训练模块，并抽取文本一系列表层特征，词典特征和语法分析特征。以CNN 作为罚款金额预测模块。 
（4）QT+CNN：采用传统的Quick-toughts 模型作为句向量训练模块，并抽取文本一系列表层特征，词典特征和语法分析特征。以CNN 作为罚款金额预测模块。 
（5）TQT+CNN：采用基于Transformer-XL 改进的Quick-toughts 模型作为句向量训练模块，并抽取文本一系列表层特征，词典特征和语法分析特征。以CNN 作为罚款金额预测模块。 
（6）TQT+Bi-LSTM+Attention+CNN：本文第三章所介绍的模型，采用基于Transformer-XL 改进的Quick-toughts 模型作为句向量训练模块，并抽取文本一系列表层特征，词典特征和语法分析特征。以Bi-LSTM+CNN 作为罚款金额预测模块，使用双向1ˆ1NiiMAPEiiyyENy=-=å()()11ˆ(11)NaccuracyiiiiEBoolean yyyNaa==-££+´´åiyˆiyNˆiya

第四章 实验设计与结果分析 
 43 长短期记忆神经网络学习案情文本中各方面的描述信息。
使用注意力机制来表达不同句子对最终预测结果产生的不同影响，最终使用CNN 来对进行罚款金额预测。 
根均方误差、平均绝对比例误差、一致率。 
模型 RMSE MAPE 一致率 Word2Vec+ CNN 0.0389 50.25% 51.14% Doc2Vec+ CNN 0.0371 49.42% 53.27% Transformer-XL+ CNN 0.0345 41.26% 68.45% QT+ CNN 0.0356 45.33% 62.14% TQT+ CNN 0.0319 40.17% 70.16% TQT+Bi-LSTM&Attention+CNN 0.0251 34.09% 74.24% 通过分析实验结果发现，首先，Word2Vec+CNN 或Doc2Vec+CNN 模型的性能是最差的，其MAPE 指标接近50%，准确率较低。
但Doc2Vec+CNN 模型较Word2vec+CNN模型有一定的提升。这得益于Doc2Vec 对长文本信息的表达比Word2vec 要更加充分，在一定程度上解决未登陆词的问题。然而这种提升并不明显，相较于以上两种模型，Transformer-XL+CNN 模型和QT+CNN 模型在罚款金额预测任务中对预测准确率有更明显的提高。其次，TQT+CNN 模型的准确率在其他句向量生成模块+CNN 模型中是最好的，更适用于罚款金额预测任务。与传统的QT+CNN 模型相比，TQT+CNN 模型在MAPE 和一致率指标上分别有5.16%和8.02%的相对提升。这说明预训练的好坏对罚款金额预测结果有所影响，基于Transformer-XL 改进QT 模型能使模型性能得到进一步提升。最后，将CNN 罚款金额模块扩展成Bi-LSTM&Attention+CNN 的Encoder-Decoder结构，能将模型的MAPE 和一致率指标分别提升6.08%和4.08%。 
为了进一步说明注意力机制模块的作用，我们对注意力机制模块进行了分析。如图4-3 所示为注意力分布可视化结果，颜色深度表示句子语义的重要程度。 
 图4-3 对于罚款金额预测注意力分布可视化 从图4-3 中可以看出注意力机制主要关注了含有“价值”、“人民币”、“元”这些词汇的句子，对与“罚金”语义相关的货物数量和货物品种如“工艺蜡烛”、“化纤华南理工大学硕士学位论文  44 针织绒布包”和货物单位如“公斤”也有一定程度的关注。因此，引入注意力机制，可以有效的提取文本方面的信息，较为准确的将注意力关注到与罚金相关的句子上。 
由实验结果可知，本文工作虽然对罚金预测的根均方误差、平均绝对比例误差和一致率有稳定的提高，但仍然存在一定的误差。这是因为罚款金额预测任务的难度仍然较高，通过调研得知相同的海关违规违法案件由不同的海关法务人员处理，其处理结果仍然存在一定的差异性，这种客观因素在一定程度上提高了罚款金额预测任务的难度。 


4.5.2 模型部件验证 
由第三章对海关虚假贸易罚款金额预测模型的介绍可知，模型的预测模块由三个核心部件构成，分别是Bi-LSTM，Attention 和CNN，Bi-LSTM 用来捕捉句子向量的双向语义信息，Attention 用于学习案件文本各句子之间的语义相关性，CNN 用来预测案件的罚款金额。为了验证模型中各部件的有效性，本小节设计了如下几种模型部件组合方式进行验证实验。 
（1）CNN：仅使用CNN 来对案件罚款金额进行预测。 
（2）Bi-LSTM+CNN：加入Bi-LSTM 来捕捉句子向量的双向语义信息。 
（3）Attention+CNN：
引入Attention 机制直接对句向量编码生成的句向量进行语义相关性学习。 
（4）Bi-LSTM+Attention+ CNN：加入Bi-LSTM 来捕捉句子向量的双向语义信息，并引入Attention 机制对句向量表达进行语义相关性学习，最后以用CNN 来对案件罚款金额进行预测。 
验证实验结果如表4-6 所示： 
模型 RMSE MAPE 一致率 CNN 0.0319 40.17% 70.16% Bi-LSTM+CNN 0.0296 38.01% 71.98% Attention+CNN 0.0284 37.87% 72.45% Bi-LSTM+Attention+CNN 0.0251 34.09% 74.24% 由表4-6 的实验结果来看，Bi-LSTM+Attention+CNN 模型在RMSE 和MAPE 指标上均取得了最佳的效果。通过对比发现在CNN 的基础上引入Bi-LSTM 和Attention 机制在一致率上分别有1.82%和2.29%的提升，而通过Encoder-Decoder 框架将其组合起来的模型是所有组合中提升最大的，也是最优的。也就是说采用Bi-LSTM 来捕捉句子向

第四章 实验设计与结果分析 
 45 量的双向语义信息，能提升模型预测罚款金额的准确性。
同理引入Attention 机制对句向量表达进行语义相关性学习也能有效的提升模型预测罚款金额的准确性，证明了加入Bi-LSTM 和Attention 机制能够提升模型的性能。 


4.5.3 超参数敏感实验 
本小节对模型使用的超参数进行敏感实验测试分析。因为将罚金预测模块固定后，输入文本信息的句向量维度、Transformer-XL 的向量编码层数、隐层的节点数及多头注意力个数的设定都将影响模型最后的预测结果，其次将输入的文本信息固定后，CNN 预测模块中超参数值设定的大小也会影响模型最后的预测结果，比如全连接网络隐含层的层数、神经元的个数和节点丢弃（Dropout）等等因素。本小节的超参数敏感实验分为四组进行：第一组实验为关于句向量维度设置的实验；第二组实验为关于Transformer-XL的向量编码层数、隐层的节点数及多头注意力个数设置的实验；第三组实验为关于全连接网络中神经元个数和隐含层层数设置的实验；第四组实验为关于全连接网络中的节点丢弃设置的实验。本节实验将会采取固定其他参数数值的方式，来对需要测试的超参数进行对比实验，按照从小到大变化的模式调节测试超参数的大小，从中观察模型RMSE值和MAPE 值的变化过程。 
l 关于句向量维度设置的超参数敏感实验 句向量的维度设置是处理文本句子的关键因素之一，句向量维度设置得过小，则模型偏差大，表达的文本特征不够，损失了文本语义信息。句向量维度设置得过大，则模型方差大，计算任务消耗的时间过长，且模型极有可能学习到与模型任务无关的信息，加大模型训练难度。所以本文选择几种常使用的句向量维度值来对模型进行学习训练，实验分别对比了句向量维度为50、100、150、200、250、300、400 的罚款金额预测模型。在验证集上，本组句向量维度大小设置实验的性能对比结果如图4-4 所示。 
从图4-4 中展示可以看到，不论是RMSE 指标还是MAPE 指标，在句向量维度从50 到200 的增加过程中，指标数值都在不断减少，说明模型的性能在不断提升，当句向量维度增加到200 以后，RMSE 指标和MAPE 指标的数值均保持平稳。
这一系列得变化说明当句向量维度不足时，无法充分表达句子文本的语义信息，使得模型的预测准确率不高，因此在50 维度时，模型的准确率并不高；但随着句向量维度的增加，对句子文本的语义信息表达就越来越充分，因此模型的性能和效率也在慢慢变高；而当句向量维 华南理工大学硕士学位论文  46     a) 在RMSE 指标上的表现                  b) 在MAPE 指标上的表现 图4-4 不同词向量维度在两种评估指标上的表现 度增加到200 时，模型的性能和效率已达到最优，再继续增加句向量的维度模型的性能并不会增加，而只会增加模型的运算复杂度与时间成本。因此，本模型选取200 的句向量维度数值作为本文实验的向量维度参数。 
l 关于Transformer-XL 编码层数，隐层节点数和多头注意力个数的超参数敏感实验 在Transformer-XL 中，向量编码的层数，隐层的节点数，以及多头注意力的个数都会对实验结果产生一定程度的影响。为了节省模型实验成本，本文通过对这三个超参数进行组合对比的方式来选取模型性能的最佳值。本文对Transformer-XL 的向量编码层数、隐层的节点数及多头注意力个数的组合设置为（4，512，5）、（4，1024，8）、（4，1024，10）、（6，512，5）、（6，1024，8）、（6，1024，10）、（8，512，5）、（8，1024，8）、（8，1024，10）。本组关于Transformer-XL 的向量编码层数、隐层的节点数及多头注意力个数设置实验结果如表4-7 所示。 
参数组合 RMSE MAPE 一致率 （4，512，5） 0.0285 37.68% 64.85% （4，1024，8） 0.0276 36.17% 68.43% （4，1024，10） 0.0268 35.22% 70.46% （6，512，5） 0.0272 36.11% 69.34% （6，1024，8） 0.0251 34.09% 74.24% （6，1024，10） 0.0249 34.00% 74.19% （8，512，5） 0.0270 35.18% 71.12% （8，1024，8） 0.0250 34.12% 74.22% （8，1024，10） 0.0251 34.20% 74.24%  

第四章 实验设计与结果分析 
 47 从表4-7 中可以看出，在隐层节点数以及多头注意力的个数固定的情况下，向量编码层数由4 层增加到6 层，模型对应的三种评价指标都有明显提升，但继续增加至8 层时，模型的性能提升并不明显。同样，多头注意力的个数在设置为8 时，模型的效果最好并且性能最优。因此，本文选择关于Transformer-XL 的向量编码层数、隐层的节点数及多头注意力个数设置的参数组合为（6，1024，8）。 
l 关于全连接网络的神经元个数和隐含层层数设置的超参数敏感实验 模型CNN Decoder 部分的全连接网络设置对最后的罚款金额结果影响至关重要，主要包括神经元个数和隐含层层数。考虑到本文的模型构造较为复杂，如果将全部的参数组合考虑进来训练模型，将会大大增加模型的计算成本。所以本文将选择几种经常使用的超参数值来对模型进行学习训练，并通过实验对比，选取使用后模型性能最好的参数。
实验分别对比了全连接网络隐含层神经元个数为64、128、256、512、1024 以及2048的罚款预测模型，并使罚款金额预测模型的隐含层层数从1 层开始以1 步长增加。在验证集上，本组全连接网络中神经元个数和隐含层层数设置实验的性能对比结果如图4-5所示，左图a）为不同神经元个数与隐含层层数对RMSE 值的影响，右图b）为其对MAPE值的影响。 
  a) 在RMSE 指标上的表现                 b) 在MAPE 指标上的表现 图4-5 不同神经元个数与隐含层层数在两种评估指标上的表现 从图4-5 中展示的结果可以看到，全连接网络的神经元个数在256、512、1024 的时候，模型的RMSE 和MAPE 指标均表现较好；在神经元个数为512 时，表现最佳，且模型的训练参数与计算量都合适。因此，本文选择使用具有512 个神经元的全连接层网络结构，使模型的性能和效率都达到最优。当模型全连接网络神经元个数确定选取512后，可以从上述图中看到在隐藏层个数为1 和2 时，模型的RMSE 和MAPE 指标均表现较差。但随着网络的加深，模型指标都呈先减小后增大的趋势。这说明随着隐藏层层华南理工大学硕士学位论文  48 数的增加，模型的拟合能力将逐渐增加；而当隐藏层层数的增加过了最适层数继续时，模型可能出现过拟合情况，即对案件数据的拟合过于精细，导致模型的RMSE 和MAPE值表现都呈现一种先变好再变差的过程。综上所述，本文模型选取每层包含512 个神经元的3 层隐含层作为罚金预测模型中的全连接网络。 
l 关于全连接网络Dropout 设置的超参数敏感实验 最后进行第四组关于全连接网络Dropout 设置的超参数敏感实验。为了解决模型出现的过拟合问题，使用Dropout 在模型的学习训练过程中以某一概率随机丢弃节点。随着Dropout 的取值不断增大，模型网络中每个神经元被丢弃的可能性也不断增高。本文分别设置Dropout 的值为0.01、0.1、0.3、0.5、0.7 以及0.9，其在验证集上的性能表现如图4-6 所示。 
         a) Dropout 取值对RMSE 值的影响          b) Dropout 取值对MAPE 值的影响 图4-6 Dropout 的敏感实验结果 从实验结果可以看出，随着Dropout 取值的不断增大，模型的MAPE 指标值呈现先减小再增大的趋势，即模型的准确率先逐渐增加再慢慢见效。当模型最开始取值为0.01时，RMSE 和MAPE 指标数值偏高，说明模型的准确率较低，这是因为过低的Dropout值意味着在学习训练过程中所丢弃的神经元个数很少，模型仍然会存在过拟合的问题。
紧接着，随着Dropout 取值的不断增大，模型RMSE 和MAPE 指标值逐渐变好的原因是模型设置的Dropout 机制对罚款金额预测模型的结果可以带来一定的提升，从图中可以看出，当Dropout 的值为0.5 时，模型的RMSE 和MAPE 指标最小，模型预测罚款金额的准确率最高。而继续增大Dropout 值，RMSE 和MAPE 的性能指标开始变差，因为过高的Dropout 值意味着在学习训练过程中所丢弃的神经元个数过多，模型将会出现欠拟合问题。 


第四章 实验设计与结果分析 
 49 4.6 本章小结 本章首先对海关虚假贸易案件罚款金额预测任务的实验数据进行了简单的介绍与分析；然后使用两种预处理方法对实验数据进行数据预处理：
预测值变化和数字离散化；接着介绍了实验的评估指标；然后对基于文本向量化模型Word2Vec、Doc2Vec、Transformer-XL、Quick-thoughts 和改进的Quick-thoughts 句向量模型的罚款预测模型对比试验，实验结果证明使用Transformer-XL 替换GRU 模块的Quick-thoughts 句向量模型能够有效提升罚款预测模型的性能；此外，本章通过CNN、Bi-LSTM+CNN、Attention+CNN、Bi-LSTM+Attention+CNN 四组罚款金额预测模型部件验证试验，证明了长短期记忆网络以及注意力机制的有效性；最后进行了参数敏感实验，对罚金预测模型中的句向量维度设置、全连接网络神经元个数、隐藏层数量以及Dropout 这几项超参数数值进行了数组实验对比调整，实验结果验证了本文模型参数设置的有效性。 
华南理工大学硕士学位论文  50 

总结与展望 


本文工作总结 
本文针对海关虚假贸易案件的罚款金额预测展开研究：给定案情简要描述文本，通过深度学习相关方法，分析该案情文本所表达的语义，对案件最终罚款金额进行预测。
传统文本分析预测方法由于所使用的句向量模型无法捕获长文本向量语义信息、难以捕获各个句子所表达语义对案件最终罚款金额影响等因素，对罚款金额预测准确率不高。
针对以上问题，本文提出基于深度学习的海关虚假贸易案件罚款金额预测方法。
具体地，本文主要完成以下工作： 
（1）在对虚假贸易案件文本分析过程中，本文设计句向量编码模块对Quick-thoughts 句向量模型进行了改进，使用Transformer-XL 替换传统Quick-thoughts 句向量模型的GRU 模块来获得海关虚假贸易案件数据集的句向量表达，利用Transformer-XL的循环机制和相对位置编码，更准确的表达案件文本的向量化信息。 
（2）在对罚款金额预测任务的过程中，本文使用基于Encoder-Decoder 框架设计罚款金额预测模块。为更好学习案情文本向量中的语义信息，本文使用Bi-LSTM 模型作为Encoder；考虑到在句子上下文信息的影响下句义表达有所差异，本文在Encoder 与Decoder 之间引入注意力机制模块以实现各个句子的语义表达；最后在Decoder 部分使用CNN 模型预测罚款金额。 
（3）实验表明，本文设计的基于深度学习的海关虚假贸易案件罚款金额预测方法在本文提供的海关真实数据集上的性能表现总体优于现有的相关工作。 


未来工作展望 
本文的研究工作尽管在罚款金额预测任务上取得了很不错的效果，但是仍然存在不足之处。针对本文方法存在的一些局限，在此提出以下展望： 
（1）目前工作仅针对于罚款金额预测开展，后续工作希望基于现有工作提高模型的泛化能力，并将其应用于其他任务（如案情分类任务等），更进一步解放海关法务人员的劳动力。 
（2）针对句向量生成任务，由于本文的所使用的数据集为数量仍然有限，当数据量随着时间逐渐加大时，本文所提出的句向量编码模块仍有改进空间。如2019 年谷歌公司AI 团队新发布了一款基于自回归的XLNet 模型。该模型能够较好地处理大规模数据

总结与展望 
 51 集，通过在较大数据集上的训练，该模型在20 个任务上超越了现有的Transformer-XL模型，并且在18 个任务上取得当前最佳效果的表现。本文后续工作有望在有更大数据集、更多数据量的基础上，使用XLNet 替换本文所使用的Transformer-XL 实现句向量模型性能的进一步提升。 
（3）本文后续将着重研究海关虚假贸易案件法务处理平台的设计与开发，并将本文所提模型应用于该平台，以进一步检验、优化理论研究成果的有效性和效果。结合本文模型开发的海关虚假贸易案件法务处理平台的可以大大提升案件处理速度，提升海关工作效率，具有很强的应用前景。 
  



参考文献 
[1] 央广网.新闻和报纸摘要[EB/OL].http://china.cnr.cn/news,2019-12-9. 
[2] 蔡岩红. 虚假贸易骗取国家出口退税[N]. 法制日报,2016-05-09(006). 
[3] 刘朋承.虚假贸易的危害与治理[EB/OL].海关总署统计司网站,2016. 
[4] 宋江培,梅莹.新常态下海关管控虚假贸易工作的思考[EB/OL].海关总署统计司网站,2016. 
[5] 于少卿.关于新形势下如何通过海关统计工作打击虚假贸易的几点思考[EB/OL].海关总署统计司网站,2016. 
[6] 宁波海关统计处课题组.虚假贸易：监测与管控——论海关打击管控虚假贸易[EB/OL].海关总署统计司网站,2016. 
[7] 宋雅君. 黄埔海关绩效考核体系的研究与设计[D].广东工业大学,2013. 
[8] 张舫,李响.对证监会执法强度的实证分析[J].现代法学,2016,38(01):173-183. 
[9] Levy O, Goldberg Y. Neural word embedding as implicit matrix factorization[C]//  Advances in neural information processing systems. 2014: 2177-2185. 
[10] Kim Y . Convolutional Neural Networks for Sentence Classification[J]. Eprint Arxiv, 2014. 
[11] Hu M, Liu B. Mining and summarizing customer reviews[C]//Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. 2004: 
168-177. 
[12] Yang P, Ma S, Zhang Y, et al. A deep reinforced sequence-to-set model for multi-label text classification[J]. arXiv preprint arXiv:1809.03118, 2018. 
[13] Gabrilovich E, Markovitch S. Computing semantic relatedness using wikipedia-based explicit semantic analysis[C]//IJcAI. 2007, 7: 1606-1611. 
[14] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105. 
[15] Kombrink S, Mikolov T, Karafiát M, et al. Recurrent neural network based language modeling in meeting recognition[C]//Twelfth annual conference of the international speech communication association. 2011. 
[16] Mishra A K, Desai V R. Drought forecasting using feed-forward recursive neural network[J]. Ecological Modelling, 2006, 198(1): 127-138. 
[17] Goodfellow I,Pouget-Abadie J,Mirza M, et al. Generative adversarial nets[C]//Advances in neural information processing systems. 2014: 2672-2680. 
[18] Mnih V, Kavukcuoglu K, Silver D, et al. Playing atari with deep reinforcement learning[J]. 
arXiv preprint arXiv:1312.5602, 2013. 
[19] Lample G, Ballesteros M, Subramanian S, et al. Neural architectures for named entity recognition[J]. arXiv preprint arXiv:1603.01360, 2016. 
[20] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014. 
[21] Kumar A, Irsoy O, Ondruska P, et al. Ask me anything: Dynamic memory networks for natural language processing[C]//International Conference on Machine Learning. 2016: 
1378-1387. 
[22] Cui Y, Chen Z, Wei S, et al. Attention-over-attention neural networks for reading comprehension[J]. arXiv preprint arXiv:1607.04423, 2016. 
[23] Miwa M, Bansal M. End-to-end relation extraction using lstms on sequences and tree structures[J]. arXiv preprint arXiv:1601.00770, 2016. 
[24] Landauer T K, Dumais S T. A solution to plato’s problem: The latent semantic analysis theory of acquisition induction, and representation of knowledge [J]. Psychological Review, 1997, 104(2):211. 
[25] Chowdhury G G. Introduction to modern information retrieval[M]. Facet publishing, 2010. 
[26] Baroni M, Dinu G, Kruszewski G. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors [C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics(Volume 1:Long Papers). 
2014:238-247. 
[27] Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155. 
[28] Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013. 
[29] Le Q, Mikolov T. Distributed representations of sentences and documents[C]//International conference on machine learning. 2014: 1188-1196. 
[30] Kiros R, Zhu Y, Salakhutdinov R R, et al. Skip-thought vectors[C]//Advances in neural information processing systems. 2015: 3294-3302. 
[31] Logeswaran L, Lee H. An efficient framework for learning sentence representations[J]. 
arXiv preprint arXiv:1803.02893, 2018. 
[32] Iyyer M, Manjunatha V, Boyd-Graber J, et al. Deep unordered composition rivals syntactic methods for text classification[C]//Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers). 2015: 1681-1691. 
[33] Arora S, Liang Y, Ma T. A simple but tough-to-beat baseline for sentence embeddings[J]. 
2016. 
[34] Conneau A, Kiela D, Schwenk H, et al. Supervised learning of universal sentence representations from natural language inference data[J]. arXiv preprint arXiv:1705.02364, 2017. 
[35] Subramanian S, Trischler A, Bengio Y, et al. Learning general purpose distributed sentence representations via large scale multi-task learning[J]. arXiv preprint arXiv:1804.00079, 2018. 
[36] Cer D, Yang Y, Kong S, et al. Universal sentence encoder[J]. arXiv preprint arXiv:1803.11175, 2018. 
[37] LeCun Y, Bengio Y, Hinton G. Deep learning[J]. nature, 2015, 521(7553): 436-444. 
[38] Kim Y. Convolutional neural networks for sentence classification[J]. arXiv preprint arXiv:1408.5882, 2014. 
[39] Zaremba W, Sutskever I, Vinyals O. Recurrent neural network regularization[J]. arXiv preprint arXiv:1409.2329, 2014. 
[40] Dai Z, Yang Z, Yang Y, et al. Transformer-XL: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019. 
[41] Graves A, Schmidhuber J. Framewise phoneme classification with bidirectional LSTM and other neural network architectures[J]. Neural networks, 2005, 18(5-6): 602-610. 
[42] Hochreiter S, Schmidhuber J. LSTM can solve hard long time lag problems[C]//Advances in neural information processing systems. 1997: 473-479. 
[43] Jang Y, Song Y, Yu Y, et al. TGIF-QA: Toward spatio-temporal reasoning in visual question answering[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2758-2766. 
[44] Shen T, Zhou T, Long G, et al. Disan: Directional self-attention network for rnn/cnn-free language understanding[C]//Thirty-Second AAAI Conference on Artificial Intelligence. 
2018. 
[45] Im J, Cho S. Distance-based self-attention network for natural language inference[J]. arXiv preprint arXiv:1712.02047, 2017. 
[46] Verga P, Strubell E, McCallum A. Simultaneously self-attending to all mentions for full-abstract biological relation extraction[J]. arXiv preprint arXiv:1802.10569, 2018. 
[47] Tan Z, Wang M, Xie J, et al. Deep semantic role labeling with self-attention[C]//Thirty-Second AAAI Conference on Artificial Intelligence. 2018. 
[48] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105. 
[49] Chorowski J, Bahdanau D, Serdyuk D, et al. Attention-Based Models for Speech Recognition[C]//International Conference on Neural Information Processing Systems. 
ACM, 2015:577-585. 
[50] Luong M T, Pham H, Manning C D. Effective approaches to attention-based neural machine translation[J]. arXiv preprint arXiv:1508.04025, 2015. 
[51] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in neural information processing systems. 2017: 5998-6008. 
[52] Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016. 
[53] Cho K, Van Merriënboer B, Bahdanau D, et al. On the properties of neural machine translation: Encoder-decoder approaches[J]. arXiv preprint arXiv:1409.1259, 2014. 
[54] He H,Gimple K,Lin J. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks[C]// Conference on Empirical Methods in Natural Language Processing. 
2015:1576-1586. 
  

攻读硕士学位期间取得的研究成果 
一、已发表（包括已接受待发表）的论文，以及已投稿、或已成文打算投稿、或拟成文投稿的论文情况（只填写与学位论文内容相关的部分）： 
序号 作者（全体作者，按顺序排列） 题目 发表或投稿刊物名称、级别 发表的卷期、年月、页码 相当于学位论文的哪一部分（章、节） 被索引收录情况               注：在“发表的卷期、年月、页码”栏： 
1 如果论文已发表，请填写发表的卷期、年月、页码； 2 如果论文已被接受，填写将要发表的卷期、年月； 3 以上都不是，请据实填写“已投稿”，“拟投稿”。 
不够请另加页。 
 二、与学位内容相关的其它成果（包括专利、著作、获奖项目等） 序号 作者（全体作者，按顺序排列） 题目 成果类型 状态 受理/授权时间 1 胡鑫 彭新一  刘孜文 基于UserCF 的图书推荐系统V1.0 软件著作权 已授权，登记号：
2018SR982142 2018 年12 月6 号   

致 谢 
路漫漫其修远兮，吾将上下而求索，三年的研究生生活转眼就要结束了，感谢在成长的岁月中，老师、同学、家人以及朋友们的一路陪伴和鼓励，让我在我的人生篇章中，写下了浓墨重彩的一笔。 
学贵得师，亦贵得友。在这里我也结识了学识渊博又平易近人的老师，感谢每一位在我学习和生活中有过帮助的老师。
衷心感谢导师彭新一在本人硕士就读期间对我的悉心指导，他言传身教的敬业精神使我受益匪浅。衷心感谢刘孜文老师，在我的学业和生活上提供了很多的指导和帮助，使我不仅掌握了扎实的专业技能，还学到了许多待人接物与为人处世的道理。衷心感谢导师宋恒杰教授，在论文撰写的整个过程中给我提出了许多宝贵的意见与推荐。 
平生感知己，方寸岂悠悠。感谢唐小丽同学对我论文的指导与帮助。感谢我实验室的小伙伴们对我三年来的包容和关怀，帮助和支持。感谢我亲爱的舍友们，无论何时何处都给予了我很大的支持和鼓励。在我论文写作烦闷、焦躁时，你们的耐心开解和不断鼓励使我的心慢慢清静下来，安心完成论文的撰写。你们的陪伴让我的生活变得更加丰富多彩，每天都充满了快乐。 
借此机会，特别感谢我的家人。这么多年的求学之路，他们在背后默默的支持，无论我做出怎样的决定，他们都义无反顾的支持我，在我面对各种选择时，是他们给予了我力量，他们的无私奉献和爱是我前进路上坚实的后盾，也是无穷的动力，祝愿他们都身体健康，和睦美满。 
同时，非常感谢参加论文评审和答辩的专家和老师，在百忙之中抽出宝贵的时间给予我指导，非常感谢！ 最后，再次感谢研究生三年来陪伴过我的所有人，谢谢你们参与了我的人生，我的人生也因为有你们才有所不同。 
 （土要内界包括：
丨？对论文的综合评价；２．对论文主耍工作和创造性成３．对作者糊綱舰、．到纟＿雛、独立从賴研工碰４．存在的不足之处＿议；５．答辩翻会结论意见等）ＨＪ沖細＿介；ＷＡＮ笮的袖丨？论文“ 鳩丨深度，刃的洵关炮《贸砝案件Ｗ款金饅携测＂１４ｓｓｒｊ＾ｎｔｆ丨－ｕ：的ｊ＞ｉ■论？《！：义ＷＴｆｃｋｗ川价ｄ论文以沏关山们贸＾絮ｎ的Ｔ：丨议金Ｍｌ拘测；／Ｊ研宄Ｘ．丨免．分別ｌｉＨｉｆｉｔ．丨也ｆｉ贸文本分ｍ和对罚坎＆轴ＫｉＭ任务．Ｘ‘Ｍ〇丨：，丨Ｗ编５１！校块进彳丨改进．并《：ＨＩＥｎｃｏｄｅｒ－Ｄｅｃｏｄｅｒ＆金抑力ｉｉａｉＨｆｒ？实论文采ｍ的文献资料较为充挤．结构？合〗１丨！．丨ｉ；次较分明．内咨较文农ｉｉ？ｍ笵？衣叫作ｄ打一ＶＵ的砰论Ｗ础和咐决实的能力．－／？士学位论文水平的耍求＊＾爸Ｊ学位申ｍ人答Ｗ过Ｇｉ并述沾楚．对符辩委贝们提出的问路或防疑己＾＾＾！＾£ －的答！ｉ．回？？问踴正确？抒杳Ｗ会无记名投栗，ＭＸＳ该同学通过ＳＨ：论文？ｆ辩，同？授子硕士学位．论文答辩日期：
２以０年Ｕ月彡日答辩委员会委员玄人表决票数：
同意毕业及授予学位（＾）票同意毕业，但不同意授予学位（）票不同意毕业（）票表决结果（打“Ｖ”）：
同意毕业及授予学位（＼／）同意毕业，但不同意授予学位（）不同意毕业（）丨（墟九ｋ说後难答辩成员２签名ｍｈ。
＾３－答ｓｒ第１０页共１２页申请人姓名：
胡產