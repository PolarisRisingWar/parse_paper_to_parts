摘要
 近年来，中国对外进出口经济贸易呈飞速增长趋势，与此同时，海关进出口违规违法案件数量也随之不断增加。为保证进出口贸易健康有序发展，海关法务人员需要加大对虚假贸易案件的打击力度，并根据案件描述文本预测对应罚款金额。然而，海关相关法务人员目前主要依赖于人工方法来对案件文本进行分析，并据此预测罚款金额，工作量大且效率低下。 
针对以上分析，本文提出一种基于深度学习的海关虚假贸易案件罚款金额预测模型。具体而言，本文主要研究内容如下： 
（1）在虚假贸易案件文本分析过程中，本文采用传统文本分析模型 Quick-thoughs来对案件文本进行编码。而考虑到 Quick-thoughs 模型采用 GRU 来对文本进行编码，而案情文本通常较长，GRU 无法捕获案情文本中的长距离依赖问题，从而会影响到案件分析的准确性。针对这一问题，本文使用 Transformer-XL 模型替代 Quick-thoughts 模型中的 GRU 模块，来解决 GRU 无法对超过固定长度的依赖关系建模的问题，以生成准确性较高且语义表达较为充分的文本向量表示。 
（2）为从文本向量中充分学习语义信息从而准确预测罚款金额，本文基于 Encoder-Decoder 框架设计并建立罚款预测模块。具体而言，本文使用 Bi-LSTM 模型作为 Encoder，以更好地从案件文本向量中更准确地学习语义信息；在 Encoder 与 Decoder 之间，本文引入注意力机制模块，以区分不同句子对最终罚款金额的不同影响程度，从而更准确地对 Encoder 中所得的各个句子语义进行表示；最后，本文使用 CNN 模型作为 Decoder，以处理高维度的向量表示，从而实现最终的罚款金额预测。 
（3）基于真实海关贸易数据的多项对比实验表明，本文提出的模型在真实数据集上能有效提高罚款金额预测任务的 RSME、MAPE 和一致率指标。 
本文工作在海关司法领域有效提高了案件罚款金额预测的准确率，是海关虚假贸易案件的罚款金额处理办法的新思路和新方法，也是深度学习技术应用于海关司法领域的一次新的探索。 
   关键词：深度学习；罚款金额预测模型；文本向量；Quick-thoughts 模型；注意力机制 

Abstract
  The expeditious growth of China's import and export economy and trade has been witnessed recently. Simultaneously, the violations in imports and export are increased.  In order to ensure the healthy and orderly development of import and export trade, customs legal officers need to increase efforts to crack down on false trade cases and predict the corresponding fines based on the case description text. However, customs legal officer currently mainly rely on manual methods to analyze case texts and predict the number of fines based on this, which is heavy and inefficient. 
Because of the above analysis, this thesis proposes a deep learning-based model for predicting the number of fines in customs false trade cases. The main contents of the thesis as followed: 
(1) In the process of text analysis of false trade cases, this thesis uses the traditional text analysis model Quick-thoughts to encode the case text. Considering that the Quick-thoughts model uses GRU to encoder the text, and the case text is usually longer, GRU can not capture the long-distance dependence problem in the case text, which will affect the accuracy of the case analysis. In response to this problem, this thesis uses the Transformer-XL model to replace the GRU module in the Quick-thought model to solve the problem that GRU cannot model dependencies that exceed a fixed length, to generate text with high accuracy and sufficient semantic expression Vector Representation. 
(2) In order to fully learn semantic information from the text vector to accurately predict the number of fines, a suitable prediction model based on the Encoder-Decoder framework has been designed and built in this thesis. Specifically, this thesis uses the Bi-LSTM model as the Encoder to better learn semantic information from the case text vector; The attention mechanism module has been introduced between the Encoder and Decoder to achieve the semantic expression of each sentence, to classify the influential caused by the final forfeit. 
Finally, this thesis uses the CNN model as the Decoder to process high-dimensional vector representations, to achieve the final fine amount prediction.  (3) Multiple comparison experiments based on real customs trade data show that the model proposed in this thesis can effectively improve the RMSE, MAPE,  and consistency rate indicators of the fine amount prediction task on the real data set.  The work of this thesis has effectively improved the accuracy in customs false trade cases in the customs judicial field, which can be regarded as a novel idea and method for handling   III fines in customs false trade cases, also a new exploration of deep learning applied in the customs justice. 
 Keywords: Deep Learning; Penalty Amount Prediction Model; Text Vector; The Quick-thoughts Model; Attentional Mechanism 第一章 绪论 
 1 第一章 绪论 
1.1 研究背景及意义 
随着中国经济贸易的飞速发展，海关进出口贸易规模不断增加。根据海关总署发布的数据显示，2019 年我国前 11 个月外贸进出口总值 28.5 万亿元，同比增长 2.4%[1]。然而在进出口贸易高速增长的同时，由于市场环境、竞争规则、企业诉求、金融监管以及地方政府等多方利益因素的不断变化，不少企业在法律监管不足的现状下片面追求利益最大化，铤而走险。在这种环境下通过虚假贸易获取不当利益的情况日益严重。虚假贸易[2]是指买卖双方制作虚假购销合同，制造虚假发票、货运单据等资料，进而骗取出口退税、出口补贴或出口奖励的贸易行为。 
在我国对外贸易经济飞速发展的大环境大背景下，虚假贸易违法违规活动的大量存在，一方面严重影响海关贸易统计的数据质量，影响国家宏观调控政策的制定，影响经济市场的稳定发展和平稳运行，扰乱进出口秩序，抑制真实贸易的发展，并且对于国家经济政策的执行与实施，经济转型升级和产业结构调整有更深远的影响；另一方面影响海关的执法公信力，引发社会对口岸数据的质疑，基于不准数据而开展的监测预警无法准确判断出我国涉外经贸的具体情况。另外，由于本地企业进出口数据是地方政府政绩的一项重要指标，利用虚假贸易“刷数据”的方式容易引发权力寻租、腐败滋生等现象出现。因此对于虚假贸易违规违法案件的治理打击，已经成为国家海关严查和重点关注的方向。然而，由于全国各地海关每天都会查获大量的违规进出口案件，而案件的识别与处理一直以来都是一项十分繁琐的工作，这些与日递增的违法违规案件更是大大增加了海关执法人员的工作量。 
虚假贸易案件的传统识别是由法务人员根据相关法律法规进行手动判别并确定案件类型及处罚结果。但这种人工识别方法不仅费时费力，并且效率并不高——案件的处理结果主要由执法人员的主观因素来主导评判，虽然有相关法律法规作为执法依照，但不同的执法人员仍然可能产生不同的评判结果。针对该问题，越来越多的学者开始尝试利用自然语言技术对虚假贸易案件进行调查研究。 
自然语言处理（Nature Language Processing，NLP）是从自然语言中获取其中所包含的语义信息，使得机器能对自然语言所表达的真正含义进行理解，从而实现人与机器之间能够通过人类自然语言进行交流、沟通的技术。自然语言处理技术的本质是促使机器能够理解人类的语言，即把自然语言通过计算机转化成离散的数学形式，使得自然语言华南理工大学硕士学位论文  2 可以在机器上进行运行处理。结合海关执法现状，将自然语言处理技术应用于海关打击违法犯罪活动，设计出一套可供海关人员使用的模型，来解决不同案件发生情况下的海关虚假贸易案件执法问题，能够及时有效地打击海关违法犯罪行为，降低海关违规违法犯罪活动的发生。 
基于以上分析，海关虚假贸易案件的高效处理需要实时案件的准确描述、对案件的性质和处罚办法的正确判断、以及在抓获违法案件上人员的着重分配，从而提高海关工作人员的办案效率，进而提高对虚假贸易违规违法案件的打击力度、降低海关虚假贸易案件的发生率。 
因此，本文聚焦此类平台中的关键任务——“海关虚假贸易案件罚款金额预测”，系统性的分析了在复杂的案件文本语义下，根据简要的案件文本来预测该类虚假贸易案件罚款金额。进而，本文分别在文本向量生成、罚款金额预测两个阶段所涉及的核心任务与关键问题展开研究，设计并构建有效的模型以对案件文本进行准确的向量表示与金额预测。本文工作是深度学习技术应用于海关司法领域的一次新的探索，为海关进出口贸易执法智能化提供了有效的新方法与新思路，其研究成果具有一定的应用价值与研究价值。 
1.2 研究现状及分析 
与本文研究相关的工作主要分为两部分，一部分是现有的海关虚假贸易案件执法办法，另一部分是与本文研究相关的若干自然语言处理任务的研究现状，如文本向量表示，文本相似性等。本小节将分别对这两方面的研究内容进行简单的介绍与总结。 
1.2.1 海关虚假贸易案件执法研究现状 
在海关打击违法犯罪领域，海关虚假贸易案件执法的目的是保证精确打击虚假贸易、保证进出口贸易正常进行。通常情况下，海关通过对企业申报的进出口物资与实际进出口物资进行比对核实，针对进出口公司实际运营情况、物流单据真实性情况进行分析，依据分析情况参考海关进出口法律法规对虚假贸易进行打击。与此同时，海关依据执法情况，对执法效果进行评估，进而提高执法水平和工作效率。因此，打击虚假贸易的执法方法是海关能否准确识别并有力打击虚假贸易的关键问题，执法效果评估是保证执法质量的重要措施。下面将围绕这两个方向的研究现状展开介绍。 
在打击虚假贸易执法方法方面，众多研究者提出应建立海关贸易审查制度，以实现虚假贸易的准确打击。海关贸易核查制度可以认为是基于贸易数据的执法方法。其基本第一章 绪论 
 3 思路是：海关对贸易数据进行检测与分析，并依照法律规定在期限内对相关企事业单位的相关业务和相关货物的会计帐本和其他材料进行检查核实，来确定企业公司活动的合法性。通过对贸易数据的分析，针对不同的贸易数据，相应的海关贸易核查制度相继建立与提出。海关总署出台了暂缓纳入海关统计的制度——《中华人民共和国海关统计工作管理规定》（海关总署令第 242 号）——暂缓纳入统计的贸易无法获得国家补贴和退税优惠直至重新纳入统计数据为止。另外，为阻止没有真实付汇的虚假贸易申报，海关与金融管理局联合出台了外汇联网核查制度。刘鹏承[3]认为海关统计部门应该着手建设和完善出口货物的申报价格管控制度，加强防范企事业单位对出口货物价格进行虚假上报的行为。随后，宋江培、梅莹[4]提出，海关统计部门应当加强对虚假贸易数据的排查和监管力度，对于有问题的企事业单位进行摸底调查和重点关注。考虑到新一代数据技术的迅速发展，于少卿[5]提出，应通过引入“大数据”、“云计算”等新型技术的分析能力，随时调动海量数据，改变以往海关年度分析、月度分析的习惯，实现统计分析的即时性，及时发现风险，做好预警工作。为进一步完善海关贸易核查制度，宁波海关统计处课题组[6]提出，应通过“产品出口增速”、“出口关别和各类特殊监管区域出口增速”、“出口交货值增速”等七个我国监测虚假贸易的指标，结合微观数据审核和宏观数据监控，监测和管控双管齐下，充分发挥海关统计数据资源优势，开展虚假贸易统计分析和监测预警，定期通报外贸进出口整体数据及虚假贸易情况。 
在对执法效果评估方面，宋雅君[7]指出了海关在反走私、监管、征税、统计评估四大职能的一级绩效指标，并建立了相应的二级指标体系。为进一步评估执法强度和震慑力，张舫、李响[8]等人参考了 Gary Becker 提出的执法震慑不等式，提出了违法者得到有效处罚的概率以及执法人员关于处罚轻重的执法评价方法。 
因此，海关依据执法情况，通过对执法效果进行评估检验，能够很好的提高执法水平和工作效率。海关使用打击虚假贸易的有效执法方法，能够准确识别并有力打击虚假贸易违规违法行为，这是保证执法质量的重要措施。 
1.2.2 自然语言处理领域研究现状 
目前，自然语言处理领域研究主要包括文本向量化[9]、文本分类[10]、情感分析[11]、多标签分类[12]、文本相似度计算[13]等方向。在深度学习被人们广泛使用之前，自然语言处理研究较少，自然语言处理任务也都较为简单。大多自然语言处理任务通过文本预处理工具，如：语句分词、词性标记、命名实体识别、依存关系分析等进行文本预处理，华南理工大学硕士学位论文  4 然后通过特征抽取等传统方法，结合支持向量机、朴素贝叶斯等机器学习方法来进行学习训练处理相关任务。之后伴随着深度学习的不断发展，自然语言处理技术开始逐渐在各个领域“大显身手”。配合着各种神经网络模型的不断提出与完善，如卷积神经网络[14]、循环神经网络[15]、递归神经网络[16]、生成式对抗网络[17]、强化学习[18]等等，这些模型应用于自然语言处理的众多任务，如命名实体识别[19]，机器翻译[20]，问答系统[21]，阅读理解[22]，关系抽取[23]等，均对其结果有着很大程度的改善与提升。 
海关虚假贸易案件处罚判断生成的核心问题是如何将复杂的案件描述转化为可供计算机处理的段落文本向量化表示，即获取一个适合海关处理违规案件的段落嵌入模型。文本向量化是将文本表示成低维、稠密的实数向量的一种方法，可以对文本所蕴含的语义信息进行表达，是文本表示中的一种重要方式。文本表示作为自然语言处理技术的一项基础工作，对整个自然语言处理系统的性能产生直接的影响。目前而言，对文本向量化的研究工作主要包括词的向量化和句子向量化，本文将从这两个方向的研究现状分别进行介绍。 
词向量又称为词嵌入（Word Embedding），在自然语言处理领域中词向量研究一直都是人们的研究热门与重点。词嵌入就是有效的将词语映射到一个向量空间，使得后续模型快速获取词语的语义信息加以运用。词嵌入最开始是在潜在语义学（Latent Semantic Analysis，LSA）[24]中以一种统计信息的表示方法出现的，之后才应用于自然语言处理的各个方向。一个好的词嵌入向量不仅能很好地反映出语义和句法的复杂特征，还能很好地适应不同上下文之间的变换。最早用来表示词语的表达模型是词袋模型（Bag of Words，BOW），其假定对于一个文本而言，忽略其词序，语法和句法，仅仅将其看作一个个词的集合或组合，文本中的每个词都相互独立且不依赖于其他词是否存在。最典型的词袋模型为独热编码（One Hot Vector），该模型对于句子中的每一个单词都用长度为 n 的向量表示，n 为词典的长度，并且该向量中单词在词典中的索引序列号位置为 1，其余位置均为 0。虽然该模型非常易于理解实施且灵活性高，但缺点也十分明显：这种向量是一种稀疏向量，向量的维度随着词典的增大而增大，当文本增大时向量的计算复杂度也随之增大；该向量丢弃了词序，忽略了上下文之间的关系，无法衡量相似度，影响在文档中词语的语义。 
为获得更进一步的词向量表示方法，学者们研究并提出了 TF-IDF[25]。TF-IDF 是一种应用于信息检索和文本挖掘的常用加权技术。其主要思想是根据一个单词或者短语在不同的文章中出现频次不同来进行判断，若这个词在本篇文章中出现的频率越高越重要，第一章 绪论 
 5 在其他文章中出现的越少越不重要，则说明这个词越有区分度，区分度明显则认为该单词或短语具有很好的类别区分能力，适合用来做分类任务，反之则不适合。由于既考虑了词语在文本中出现的频率也强调了单词在文本与文本之间的重要性，TF-IDF 方法作为一种统计学习方法被广泛使用。但是这种方法简单地认定文本出现频率小的单词就越有用，文本出现频率大的单词就越无用，这显然是很片面的。TF-IDF 方法也无法准确地给出单词的重要程度与特征词语的分布情况，无法有效地完成对权值的调整。 
为了获得更加充分的词的语义相关性表达，Hinton[26]等人提出了词的分布式特征表示（Distributed Representation)。分布式表达的基本思路是，在对一个词语状态进行表达时，状态寄存器和状态不再是一一对应映射存储的，而是多对多的关系。具体而言，在词的分布式特征表达中，一个词语状态可以用多个状态寄存器共同定义表达，同时一个状态寄存器也可以参与多个不同状态的表达。比如“大红灯笼”这个词，如果用分布式特征来表示，那么可能是一个状态寄存器代表大小，一个状态寄存器代表颜色，一个状态寄存器代表物品种类。当且仅当这三个状态寄存器同时激活时，要表达的物体才能得到比较准确的描述。最早将分布式表示应用处理到词向量上来的是 Bengio[27]等人，他们提出的神经网络语言模型（Neural Probabilistic Language Model，NNLM）将每个词转换表示为一个稠密的实数向量。该模型可以自由定义词向量的维度，且维度并不会因为词典的扩展而发生改变，并且该模型生成的词向量能够很好的根据特征距离度量词语之间的相关性。但该模型的缺点也十分明显，计算复杂度过大且参数较多。 
真正让分布式词向量得以广泛运用且性能效果较好的是由 Tomas Mikolov[28]提出的Word2Vec 模型。它的本质就是简单化的神经网络模型，能够将不可计算的非结构化的词语转化为可计算且结构化的向量表示。Word2Vec 共有两种不同的表示形式，跳字模型（Skip-gram）和连续词袋模型（Continuous Bag of Words，CBOW）。CBOW 是一种统计语言模型，即根据某个词前面的几个词或者前后出现的几个词，来计算预测该词出现的概率。而与 CBOW 模型相反，Skip-gram 根据当前词分别计算预测该词出现上下文词的概率。尽管两者的目标优化函数并不一致，但是其目的都是为了获得单词与相邻单词之间的语义相关性。以“我喜欢吃蔬菜水果”为例，假设此时的关键词为“吃”，训练好的 CBOW 模型就是将“我”“喜欢”“蔬菜”“水果”的 one hot 表示方式作为输入，来计算预测“吃”的分布式表示。Skip-gram 模型与其相反，它们的模型流程如图 1-1 所示： 
华南理工大学硕士学位论文  6  图 1-1 CBOW 利用上下文预测中心词“吃”及 Skip-gram 利用中心词“吃”来预测词上下文 Word2Vec 模型的优点在于考虑了词语和词语之间的语义关系以及词序关系，同时能够应用于不同场景的上下文语句。除此之外，该模型还能够在其他大型语料上进行模型预训练，获取训练好的词向量的表达方法，随后迁移到其他任务中。但是其仍然存在一些不可忽视的缺点，由于词语和向量是一一对应的关系，一些词语只考虑了当前语料库中所表达的词义，而没法更多的考虑整句话或者词语所处的其他上下文环境，比如“苹果”到底是指日常生活中所吃的水果“苹果”，还是指美国的电子商务公司的“苹果”，这对于词向量来说是很难进行辨别的，一个词的词向量不应该仅仅只在语料中进行学习，而是应该考虑该词语实际具体所处的位置，才能得到有效语义更丰富以及更为准确的表征。 
为了更加充分地考虑句子中词语与词语之间的关系，以及当前词语所处的语境，研究人员们通过研究提出了词语在句子级别中的特征表示，进行句子表示学习，从而获得语义表达较好的句子向量（Sentence Embedding）。传统获取句向量的方法是对一个句子中所有词的词向量进行组合，但这种方法虽然足够简单高效但，并没有考虑词序信息。例如平均句向量模型就是将句子中所有词的词向量表达进行相加取平均运算，将计算得到的向量当作最终的句子向量。但在该模型中，句子中所有单词对于表达句子含义的重要性是相同的，这很显然是不准确的。更典型的句子向量表达就是 14 年由 Tomas Mikolov[29]提出的 Doc2vec 模型，用于学习句子和文档的分布式表示。该模型在训练过程中引入了段落特征（Paragraph Vector），即将每一个段落表示为一个向量，其中不同的段落具有不同的段落向量。该段落向量能够让模型更加充分考虑上下文信息，对全局的文本信息起到一个概括作用，即该段落特征可以作为该段落或句子的一个主旨，对该段落或句子的主要信息进行概括表达。Doc2vec 模型的每一次训练不仅仅得到了词向量，而且每一次输入都会共享该段落特征，并对段落特征不断更新使得其表达更加准确。并且，Doc2vec 模型通过该段落向量可以构建语言模型来预测下一个词，且段落向量在获第一章 绪论 
 7 取的过程中，能够对无标记的数据进行训练。此处仍然以“我喜欢吃蔬菜”为例，输入“我”“喜欢”“吃”的 one hot 表示方式以及段落特征作为输入，计算预测出下一个单词“蔬菜”的分布式表示，模型流程如图 1-2 所示。但是该方法仍然存在着一些不足之处，例如对段落文本级别特征的考虑过于粗糙，语义表征的精确度准确性不够高等。 
 图 1-2 Doc2vec 添加段落特征预测下一个词 除了 Dov2vec 之外，还有在 NIPS-2015 上 Kiros R[30]等人提出的 Skip-thought 模型，该模型使用的是传统的端对端（End-to-End）方法，该方法使用的编码器以及解码器都是 RNN 模型，并根据上下文关系进行预测，以获得更好特征编码的语义表征。以及在2018 年的 ICLR 上 Logeswaran[31]等人提出的 Quick-thoughts 模型，该模型是在 Skip-thought 模型的基础上进行的改进优化。这个模型将在本论文第二章进行详细描述。 
1.3 论文研究内容 
本文以海关进出口虚假贸易打击执法为主要应用场景，围绕海关虚假贸易案件的罚款金额预测问题展开。下面将对本文的主要研究内容和工作进行展开介绍： 
（1）本文通过对句向量生成模型的相关工作进行分析与总结，设计句向量编码模块，提出基于 Transformer-XL 改进的 Quick-thoughts 句向量编码模块，使用 Transformer-XL 替代传统 Quick-thoughts 模型中的 GRU 模块来对案件文本描述进行编码，生成更加准确并且语义表达更为充分的文本向量表示。 
（2）在对罚款金额预测模块的设计建立过程中，传统方法大多直接使用 CNN 卷积网络模型学习预测罚款金额，但简单的 CNN 模型无法有效的学习案件文本复杂的语义信息，因而罚款金额预测的准确率往往不高。针对该问题，本文采用 Encoder-Decoder 框架来建立罚款金额预测模型，并引入句子层级的注意力机制来体现案件文本中不同句子代表的不同含义。 
（3）本文在包含 1 万 5 千条海关虚假贸易案件的真实数据集上进行了多组实验进华南理工大学硕士学位论文  8 行对比研究。实验结果显示，本文提出的罚款金额预测模型相对于目前主流的模型，在海关虚假贸易违规违法案件领域能有效提高案件罚款金额预测的准确率，达到了预期的效果。 
1.4 论文组织结构 
本文的研究目的是根据海关虚假贸易案件的案情描述帮助海关执法人员准确地判断并预测出该描述案件的罚款金额情况，研究重点在于如何准确生成该案件描述的文本特征表达以及建立预测准确的罚款预测模型。其中，各个章节的具体安排如下。 
第一章，绪论。首先介绍了海关虚假贸易案件罚款金额预测课题的研究背景、研究目和研究意义；然后对海关虚假贸易案件执法研究情况以及相关自然语言处理技术中的向量模型的发展过程进行介绍，并对相关句向量模型进行了阐述；最后简单梳理了本文的主要研究内容与组织结构。 
第二章，相关理论和技术。首先从句子的有监督表示学习、无监督表示学习和多任务表示学习三个方向简单地介绍了句向量技术的研究现状；其次对本文涉及到的深度学习模型进行介绍，主要包括无监督的句向量模型（Qucik-thoughts）和基于自注意力机制的 Transformer-XL 模型；最后对本章的内容进行总结。 
第三章，模型整体框架的设计。首先介绍了模型整体架构，介绍了模型的两个主要模块——句向量生成模块和罚款金额预测模块；然后对本文的两个模块进行详细介绍，在句向量生成模块中，利用基于 Transformer-XL 改进的 Quick-thoughts 语言模型，训练海关虚假贸易相关句向量；在罚款金额预测模块中，使用 Encoder-Decoder 框架设计罚款金额预测模型，采用 Bi-LSTM 编码器对案情文本进行编码，引入 Attention 机制并使用 CNN 解码器生成最终案件的罚款金额，不断提升预测的准确性；最后对本章内容进行总结。 
第四章，实验设计与结果分析。首先对本文所使用的实验数据集进行了介绍与分析；其次对数据进行预处理的两种方法进行了简单介绍；接着对实验所使用的评估指标与实验流程进行了介绍；最后通过对比不同模型在海关虚假贸易案件数据集上的表现并对实验结果进行分析，证明了本文所提模型的可行性以及稳定性。 
第五章，总结与展望。首先分析总结了本文的主要工作内容以及研究结果，然后对
基于深度学习的海关虚假贸易案件数据的罚款金额预测方法的未来工作进行展望。 
 第二章 相关理论和技术 
 9 第二章 相关理论和技术 
本章将对本文研究工作所涉及的相关理论和技术进行介绍，着重阐述与本课题研究相关的基础知识和对应的算法模型。首先，简单介绍了句向量技术及其分类；其次，分别对几种常用于文本向量编码与用于回归预测的深度学习模型进行了详细介绍与分析。本章内容为第三章罚款金额预测模型设计提供理论基础。 
2.1 句向量技术 
在自然语言处理任务中，句子的向量表达常常都是通过词向量的简单相加或加权求和等方式堆叠而来。现阶段常用的句向量方法通常可分为三类，即对句子的有监督表示学习、无监督表示学习和多任务表示学习。 
对于句子的有监督表示学习方法，2015 年由 Iyyer[32]等人提出了一种深度平均网络模型 DAN（Deep Averaging Networks），该网络模型是在传统词袋模型的基础上，通过添加多个隐藏层的方式来增加整体网络的深度，实现在句法上的信息提取，并提高训练准确度。结果表明 DAN 模型在句子和文档级别的情感分析以及问答任务上的准确性都有所提升，训练时间也比其他复杂神经网络方法要少得多。为了进一步提高句向量表示的准确率，2016 年由 Sanjeev[33]等人提出了一种加权词袋模型 SIF（Smooth Inverse Frequency），该方法使用当前主流的词向量方法来计算句子中每个词语的词向量表示，接着通过计算词向量的加权平均值来表示该句子，然后通过主成分分析或奇异值分解的方法对它们进行调整。这种加权获得句向量的方法可以将文本相似性任务的性能提高约10%到 30%，并且优于包括 RNN 和 LSTM 在内的复杂的监督表示学习方法。为了提高句向量模型的泛化能力，2017 年由 Conneau[34]等人提出了一种基于 SNLI 语料库训练出来的通用的句向量表达。作者认为从 SNLI 语料库中训练得到的句向量也适合迁移到其它的 NLP 任务当中，该方法通过使用多种模型在多种数据集上反复训练得到一个通用的句向量模型，再将训练得到的句向量模型应用到其它 NLP 任务当中，通过对比任务效果得到最终的句向量模型。这种方法在自然语言推理任务中训练得到的模型性能要优于其他条件下学习得到的模型并具备可迁移性。 
对于句子的无监督表示学习方法，2014 年由 Tomas Mikolov[29]提出了一种用于学习句子和文档的分布式表示的 Doc2vec 模型。该模型增加了一个新的句子向量，将其作为句子的主旨用于对之前的信息进行记忆。虽然该模型能够处理不同长度的句子文本，但其准确率往往偏低，2015 年由 Kiros R[30]等人提出了一种利用中心句子来预测上下文句华南理工大学硕士学位论文  10 子的 Skip-thought 模型。该模型采用 Encoder-Decoder[53]结构作为基本框架，使用循环神经网络作为模型的编解码器，目的是通过训练得到一个性能良好的编码器，并将这个编码器作为句向量的通用模型。结果表明虽然 Skip-thought 模型生成的句向量质量不是最优的，但是普遍表现都很好，说明该模型具有优秀的通用性。随后在 2018 年 Logeswaran[31]等人提出了 Quick-thoughts 模型，该模型在 Skip-thought 模型的基础上进行了改进和优化，在模型的 Decoder 端仅使用一个分类器来学习句子的向量表示，在降低训练时间的同时也成功地在一定程度上忽略了句子形式，学习到了真正的语义表示。 
对于句子的多任务表示学习方法，2018 年由 Subramanian[35]等人提出了一种一对多的多任务学习框架，通过在不同的任务间进行切换来联合学习一个通用的句子表征。结果证明了在这几个分类任务上，使用联合训练学习的句子表征在分类效果上要比单独训练学习的要好得多，并且作者还指出联合训练中不同的任务对句子表征中的不同方面有着不同的贡献。同样在 2018 年，由谷歌的 Cer[36]等人提出一种类似的多任务学习框架来对句子的向量表示进行学习，不过该作者使用了 Transformer 和 DAN 两种框架来作为句子的 Encoder，在十个分类任务上进行迁移学习的评估，效果也都很不错。 
2.2 深度学习模型介绍 
深度学习在 2006 年由 Hinton[37]等人提出，又称为深度神经网络，旨在学习样本数据的内在规律和表示层次，是机器学习领域的重要研究方向之一。深层的神经网络是典型的深度学习模型，比较常见的如用于计算机视觉方面的卷积神经网络（Convolutional Neural Networks，CNN）[38]，在对图片的处理方面表现的十分出色；用于自然语言处理等方面的循环神经网络（Recurrent Neural Networks，RNN）[39]，在对文本语义表达方面表现十分出色；以及用于搜索、推广、广告等其他需要大量抽象特征功能的场景的深度神经网络（Deep Neural Networks，DNN）。在实践中，深度学习能够利用大量数据不断学习训练，更为全面的了解数据特征，模拟预测任务出现的各种情况，对预测任务进行更为准确判断。本文在 2.2.1、2.2.2 小节介绍涉及句向量生成任务中文本向量研究的深度学习方法，在 2.2.3、2.2.4、2.2.5 小节介绍预测任务中所涉及的深度学习方法。 
2.2.1 快速思维向量模型 
快速思维句向量[31]（Quick-thoughts vector，QT）模型是一种可用于从未标记的数据中学习句子表示的句向量模型。Quick-thoughts 模型采用一种分类器的方式学习句子表示，使用 Encoder-Decoder 框架作为模型的基础框架。在编码器部分，Quick-thoughts 模第二章 相关理论和技术 
 11 型使用某个函数（如循环神经网络）用来压缩所选择的中心句子信息。而在解码器部分，Quick-thoughts 模型使用一个分类器来选择的正确上下文语句。即给定一个输入句子，Encoder 部分对其进行编码，生成该句子的编码表示。以此为基础，Decoder 从一组候选句子中选择正确的目标句子。简单来说，Decoder 使用一个分类器从所有的候选句子中选择一个最可能与目标句子形成上下文关系的句子，这里可以近似看作是一种判别，判别候选句子中的哪一个句子是正确的句子。具体模型的结构如图 2-1 所示： 
 图 2-1 Quick-thoughts 模型结构图 编码器：如图 2-1 中的/所示，作用是将文本句子表示为分布式语义向量。假设表示所有训练数据的集合， 表示输入的目标句子，表示输入句子 的上下文相关句子集合（也就是 的前一个句子或后一个句子），表示除输入句子之外的候选句子集合，它包括上下文句子以及其它不是目标句子 的上下文句子。定义、作为参数化函数，、以一个句子作为输入，将句子编码成一个固定长度的向量。编码器通过参数化函数、对目标句子 以及候选句子集合中的每个句子进行编码，然后输入到解码器中。循环神经网络模型（RNN）中的门控循环单元（GRU）在近年句子表示学习方法中得到了广泛的应用，Quick-thoughts 模型将 GRU 作为参数化函数和。在 Quick-thoughts 模型的编码器部分，句子中的单词被顺序输入 GRU 中进行句子向量编码，GRU 最后的隐藏状态作为句子的向量表示输出。 
解码器：如图 2-1 所示该模型的解码器是一个分类选择器。在给定一个句子及其上下文的情况下，分类器根据上下文句子的向量表示，对编码器输出的向量进行比较选择，将目标句子与其他句子区分开来，并判断选出中的。具体而言，解码器对 的向量表示和中的所有句子向量表示计算内积得分，然后将计算结果输入到( )Enc f( )Enc gSsSctxtssScandsSctxtsfgfgfgscandSfgScandsctxtsScandsoftmax华南理工大学硕士学位论文  12 层进行分类筛选，选出得分最高的为目标句子 的上下文相关句子。模型使用的分类器即得分函数 为两个向量内积： 
 ， (2-1) 其中和 表示不同的句子向量。Quick-thoughts 模型在解码器端使用一个简单的分类器，其目的是将模型的工作重点放在编码器的训练上，尽可能地减少除了编码器训练以外的工作量，最终得到一个好的句子向量表示。 
对比 2.1 小节提到的传统 Skip-thought 模型，Quick-thoughts 模型在其基础上进行优化改进，在解码器模块引入一个分类器来判别选择正确的上下文句子，从而在模型性能上更加优越。因此本文选择 Quick-thoughts 模型作为基准模型进行改进和分析。 
2.2.2 Transformer-XL 模型 
在当前的 NLP 领域中，通常使用 RNN 和 Transformer 这两种架构来处理语言建模问题。其中 RNN 是按照序列顺序来逐个学习输入的字符或词语之间的相互关系，而Transformer 则是接受一整段的序列，然后使用自注意力机制来学习它们之间的依赖关系。虽然这两种架构都取得了不错的成绩，但是它们都在捕捉长距离依赖性上有所欠缺。因此，卡内基梅隆大学联合 Google Brain 在 2019 年提出了 Transformer-XL[40]模型。该模型同时结合了 RNN 序列建模和 Transformer 自注意力机制的优点，在输入数据的每个段上均使用Transformer的注意力模块，并使用循环机制来学习连续段之间的依赖关系。 
对比传统 Transformer 模型，Transformer-XL 由一个新的循环机制和一个新设计的相对位置编码器组成。首先简单介绍传统 Transformer 模型的训练及预测流程，以长度为 4 的片段为例，传统 Transformer 模型的训练、预测流程如图 2-2 所示： 
 a) 传统 Transformer 模型的训练阶段 sctxtsc(,)Tc u vu v=uv第二章 相关理论和技术 
 13  b) 传统 Transformer 模型的测试阶段 图 2-2 传统 Transformer 模型各阶段流程图 从图 2-2 中可以看出，Transformer 模型在训练时，处理序列长度超过固定长度的句子，处理方式是将文本序列划分为多个 Segments，训练时对每个 Segment 进行单独处理，且 Segment 之间没有联系，如图 2-2(a)所示。但这样处理的缺点是按照固定长度来切割句子，并没有考虑句子的自然边界，因此分割出来的 Segments 在语义上是不完整的。Transformer 模型在预测时，如图 2-2(b)所示，对固定长度的 Segment 做计算，取最后一个位置的隐向量作为输出。为了充分利用上下文之间的关系，每做完一次预测，模型将对整个序列向后移动一个位置，再做一次计算。但这样的方式往往会降低模型的计算效率，增加模型的计算复杂度，因此研究人员提出 Transformer-XL 模型来优化改进传统 Transformer 模型。同样以长度为 4 的片段为例，Transformer-XL 模型的训练、预测流程如图 2-3 所示： 
 a) Transformer-XL 模型的训练阶段 华南理工大学硕士学位论文  14  b) Transformer-XL 模型的测试阶段 图 2-3 Transformer-XL 模型各阶段流程图 从图 2-3 中可以看出，在 Transformer-XL 模型的训练阶段，在处理新片段时，每个隐藏层都将会接收两种输入：一种是该段前面隐藏层的输出，如图 2-3（a）灰色线部分所示，该部分与传统 Transformer 模型相同；另一种是前面段的隐藏层的输出，如图 2-3（a）绿色线部分所示，这一部分的加入可以使 Transformer-XL 模型对文本创建长期依赖关系。在 Transformer-XL 模型的测试阶段，传统 Transformer 模型在测试阶段时，每一次只能前进一个片段，并且需要重新构建段，并全部从头开始计算；而 Transformer-XL 模型在测试阶段时，每一次都能前进一整个段，并能利用之前的段的数据来预测当前段的输出，与传统 Transformer 模型相比速度更快。 
另一个改进就是 Transformer-XL 模型的位置编码的改进，在传统 Transformer 中，模型考虑了序列的位置信息，使用的是绝对位置编码。而 Transformer-XL 模型使用的相对位置编码，其允许 Transformer 网络在长度可变的内容中学习依赖关系，而不干扰时空的一致性。在分段的情况下，如果仅仅对于每个段仍直接使用传统 Transformer 的绝对位置编码，即每个不同段在同一个位置上的表示使用相同的位置编码，就会出现问题。比如，第 i-2 段和第 i-1 的第一个位置将具有相同的位置编码，但它们对于第 i 段的建模重要性显然并不相同（例如第 i-2 段中的第一个位置重要性可能要低一些）。因此，需要对这种位置进行区分，通过对计算公式的改进，引入相对编码。相对位置编码总体来看分为四个部分，一是基于内容的“寻址”，即没有添加原始位置编码的原始分数；二是基于内容的位置偏置，即相对于当前内容的位置偏差；三是全局的内容偏置，用于衡第二章 相关理论和技术 
 15 量 Key 值的重要性；四是全局的位置偏置，根据 Query 值和 Key 值之间的距离调整重要性。 
Transformer-XL 模型不仅可以有效解决传统 RNN 神经网络模型对长距离文本依赖不足的问题，让模型捕捉到长期的依赖，同时也可以解决内容分块的问题，拓展单词关注不同位置的能力。比如“A 公司申报出口货物 1800 件到 B 公司，他实际出口 2000件。”Attention 机制会将“他”对应“A 公司”。并且 Transformer-XL 模型的扩展能力强，能够有效的扩展到其他需要该能力的深度学习领域，比如音频分析等。Transformer-XL 模型的训练速度相比传统的 Transformer 模型也有了很大的提高，对语言的理解过程十分有帮助，有效提高了训练效率。 
2.2.3 卷积神经网络 
卷积神经网络（Convolutional Neural Networks，CNN）[33]是一种特殊的神经网络模型，它与普通神经网络模型相似，都由具有可以学习的权重和偏置常量的神经元组成。不同之处在于 CNN 的默认输入是图像，CNN 可以将输入信息编码成具有特定性质的网络结构，使得 CNN 能够很好的利用输入数据的二维结构。这样大大地减少了模型的参数使用，提高模型的效率。CNN 常常应用于语音识别和图像处理领域的任务。近年来，CNN 在自然语言处理领域上也达到了很好的效果，比如自动问答和机器翻译等任务。 
下面将对 CNN 模型的结构进行简单介绍，CNN 模型通常包含以下几种层： 
（1）卷积层（Convolutional Layer）。卷积神经网络包含多层卷积层，其中每层卷积层都由多个卷积单元组成。卷积层的卷积运算旨在对输入信息的不同特征进行提取，第一层卷积也许只能够提取一些简单的基本特征，但随着层数的不断增加，卷积神经网络能够从基础特征中迭代提取出更复杂的特征。每个卷积单元的参数都是由反向传播算法优化得来的。 
（2）池化层（Pooling Layer）。池化层通常在卷积层之后。卷积层会得到维度很大的特征，计算量过于复杂庞大。因此池化层会把特征切成几个部分，通过取各个部分特征的最大值或平均值的方式，以此来获得新的维度更小的特征。 
（3）全连接层（Fully Connected Layer）。全连接层把所有的局部特征结合生成全局特征，用来做最后的计算。 
在常见的自然语言处理任务中，卷积神经网络将一个以矩阵形式表示的文本或句子作为网络模型的输入，其中文本或句子矩阵的每一行分别对应于文本中的一个句子或者句子中的一个词语，也就是说每一行对应一个句子或单词的向量表达形式；矩阵的列就华南理工大学硕士学位论文  16 表示这个句子或单词的向量表达形式的维度。文本矩阵可以由 Word2Vec 或其他文本向量表达算法得到；接着卷积层对文本矩阵进行卷积处理。卷积核会覆盖文本的若干个句子或词语，卷积核的高度取决于上下文词语的数量，卷积核的宽度一般与词向量的维度相等；卷积层之后接着是池化层，池化层会依据一定的规则，例如取特征区域的平均值、最大值等等，进一步对卷积层的输出特征进行抽象处理；一般使用最大值池化对前一层中最主要的特征进行保留处理；接着是全连接层，但在全连接层之前一般会使用 flatten函数将向量“拉直”处理；为了避免出现过拟合的现象，一般还会在全连接层对数据进行正则化处理，即采用 Dropout 技术来“丢弃”部分隐藏层的节点；使用全连接层可以将 CNN 应用于处理分类或回归任务；最后传递输出层输出最终任务结果。 
目前 CNN 在自然语言处理领域的应用对自然语言处理技术的发展产生了十分积极的影响。如何有效利用 CNN 技术对自然语言处理任务进行学习研究，提高相应模型的性能与泛化能力，解决一些实际有效的问题，将对自然语言处理领域的发展具有十分重要意义。 
2.2.4 双向长短期记忆网络 
双向的长短期记忆网络模型 Bi-LSTM（Bi-directional Long Short-Term Memory）[41]是由 Graves 和 Schmidhuber 提出的一种处理序列数据的神经网络模型。该模型包含有一前、一后两个方向的 LSTM[42]（Long Short-Term Memory），通过叠加组合的方式构成。在介绍 Bi-LSTM 模型之前首先介绍 LSTM 神经网络模型，该模型是 RNN 的一种变体，其设计目的是为了解决一般 RNN 存在的远距离依赖问题、长文本序列训练中的梯度消失问题和梯度爆炸问题。虽然相比 RNN，LSTM 模型解决了一系列问题，但其仍然只能根据先前时刻的隐藏状态与当前时刻的输入信息来对下一时刻的隐藏状态输出进行预测分析。而通过对自然语言的表达习惯进行分析发现，在预测文本中某一词的词意时，往往需要根据上下文的语义进行分析。因此，考虑到单向 LSTM 神经网络模型的不足，Bi-LSTM 模型被提出，该模型的输入分别是输入序列的正向排序和反向排序，输出由两个方向的 LSTM 的隐藏层状态共同决定。这种网络结构较单向 LSTM 可以更好的获取文本序列上下文之间的关系。图 2-4 为双向长短期记忆网络结构图。 
第二章 相关理论和技术 
 17  图 2-4 双向长短期记忆网络结构图 Bi-LSTM 由层结构相同的前向 LSTM 和反向 LSTM 构成。其中前向 LSTM 层的输入为序列从 0 时刻起的正向输入，得到并保存每个时刻的前向隐藏层状态，数学表达形式如公式（2-2）所示： 
 ， (2-2) 其中，表示前向 LSTM 的权重系数，表示当前时刻的文本输入，表示前一时刻的前向 LSTM 层输出。反向 LSTM 层的输入为序列从序列末 N 时刻的反向输入，得到并保存每个时刻反向隐藏层的输出，数学表达形式如公式（2-3）所示： 
 , (2-3) 其中，表示反向 LSTM 的权重系数，表示当前时刻的文本输入，表示后一时刻的反向 LSTM 层输出。Bi-LSTM 的输出为前向 LSTM 层和反向 LSTM 层的相应时刻隐藏层的组合，数学表达形式如公式（2-4）所示： 
 ， (2-4) 其中，表示输出层的权重系数，和表示当前时刻两个方向的 LSTM 层输出。 
双向长短期记忆网络，不仅可以学习到时序序列的历史信息，还可以关联序列时序序列的未来数据，因此双向长短期记忆网络在自然语言处理领域中应用广泛。本文在第三章设计罚款金额预测模型中也将采用 Bi-LSTM 来学习文本上下文特征。 
111()tlstmftthfUxW h -=×+×!!U1W1txth -1!()221tlstmbtthfxUhW+×=×+!!U2W2txth +1!()12tttog V hVh+××=!"V1V2th!!th华南理工大学硕士学位论文  18 2.2.5 注意力机制 
虽然之前介绍的 Bi-LSTM 模型在语义解析任务中有很好的性能表现，但仍然存在两方面的限制。一方面是计算能力的限制。需要记忆的信息越多，模型结构越复杂，需要的计算能力越高。然而目前计算机的计算能力达不到复杂模型的设计要求，计算能力的不足依然在限制神经网络的发展。另一方面是优化算法的限制。虽然局部连接，权重共享等优化操作可以简化神经网络模型结构，有效缓解模型复杂度和表达能力之间的矛盾，但是仍然存在循环神经网络中长距离依赖等问题，这大大限制了信息的“记忆”能力。 
为了解决上述问题，Jang Y[43]等人研究并提出了注意力机制。注意力机制最早被人们应用于计算机视觉领域，称为视觉注意力机制。视觉注意力机制是人类视觉所特有的大脑信息处理机制，人们在观察一幅画的时候，并不是需要完全看清这幅画的每一个像素才能了解画的意思，而是只需要观看这幅画的关键部分就能够了解其所表达的含义。从此注意力机制及其变形[44,45,46,47]越来越多地运用在各个领域。注意力机制的具体工作流程如图 2-5 所示。 
�图 2-5 注意力机制工作流程图 注意机制的工作流程主要分为以下三个步骤： 
（1）将输入的信息向量与每个相关单元进行相似度计算获得注意力权重。假设输入一个 n 维的信息向量，对于输入的每一个，都可以看作由一个<>数据对构成。函数表示相似度计算函数，通常使用点积，拼接和感知机等方法。是一个与目标输出相关的目标向量，由先前步的输出1( ,2,...,n)Xx xx=(1,2,...,)ix in=i,Key Valuei()SimilarityQuery第二章 相关理论和技术 
 19 压缩得到。第一步主要是根据和来计算两者的相似性或相关性。其数学表达如下： 
 ， (2-5)  ， (2-6)  ， (2-7)  ， (2-8) 其中式（2-5）是由 Graves[48]等人提出的余弦相似度函数方法；式（2-6）是由 Chorowski[49]等人提出的线性相加的方法；式（2-7）是由 Luong[50]等人提出的点积的方法，其中的 ，，是函数引入的外部矩阵；式（2-8）是由 Vaswani[51]等人提出的在点积函数的基础上加入缩放因子的方法。以上四种方法都是用来计算两者之间的相似度的，目的是为了避免下一步骤中的函数出现梯度极小，难以计算的情况。 
（2）对这些注意力分值进行归一化处理。计算注意力系数，使用像这类的函数，将原本计算的数值转换成为权重和为 1 的概率分布。其数学表达形式如公式（2-9）所示： 
 ， (2-9) （3）计算!""#$"%&$值。将之前学习到的注意力系数与输入信息的值通过加权求和运算，获得!""#$"%&$值。其数学表达形式如公式（2-10）所示： 
 。 
(2-10) 2.3 本章小结 
本章首先对句向量技术及其分类进行了详细的介绍，包括有句子的有监督表示学习、无监督表示学习和多任务表示学习；其次分别对几种常用于文本向量编码的深度学习模型进行了介绍与分析，包括 Quick-thoughts 句向量模型和 Transformer-XL 自注意力机制模型；接着对用于回归预测的 CNN 模型以及用于学习文本上下文特征的 Bi-LSTM 神经网络模型进行了介绍和分析；最后介绍了注意力机制的使用特点。 
  QueryKey(),iiiKey QuerySimilarity Key QueryKeyQuery=××()(),tanh,TiiSimilarity Key QueryvW Key U Query=××()i,iSimilarity Key QueryKey Query×=(),iiKey QuerySimilarity Key Queryn×=vWU()softmaxia()softmax1ijSimilarityinSimilarityjeae==åiaValue()1,niiiAttention X Querya Value==å×华南理工大学硕士学位论文  20 第三章 海关虚假贸易罚款金额预测模型研究 
针对海关虚假贸易案件罚款金额预测问题，本章提出了一种基于深度学习的海关虚假贸易罚款金额预测模型，本章节将对所提模型进行详细介绍。具体而言，本章首先介绍本文所用到的符号及定义，以及本文所要解决问题；随后，介绍如何引入 Transformer-XL 来对句向量模型 Quick-thoughts 进行优化及改进，以获取案件描述的向量表示；最后，介绍如何基于上述所得向量表示，使用 Encoder-Decoder 框架来预测获取最终的罚款金额，并在此过程中，着重讨论了如何处理案件句子之间的语义差异对最终处罚金额影响不同的问题。 
3.1 符号定义与问题描述 
3.1.1 符号定义 
本节中，将首先简单介绍本文所涉及的术语、符号，然后对本文所研究的问题进行简单描述。为方便阅读，表 3-1 展示了本文所涉及的主要符号及其说明。 
表 3-1 主要符号对照表 符号 符号说明  一个案件文本的一个句子序列  一个案件文本的句子个数  案件文本数据集中的案件文本序号  一个案件文本  一个案件文本的真实罚款金额  训练集案件文本个数  案件文本数据集  真实罚金数值集合  训练数据集  句向量表示文本序列  经过 Bi-LSTM 网络得到的文本向量  经过注意力机制对应的特征向量  句向量维度  预测当前案情罚款金额数值 snix( )( )( )( )12(,,...,)iiinixsss=yMX(1)(2)(){,,...,M}Xxxx=Y(1)(2)(){,,...,M}Yyyy=D(1)(1)(2)(2)()(){(,),(,),...,(,)}MMxyxyxyw( )( )( )( )12(,,...,)iiiniwwww=h( )( )( )( )12(,,...,)iiinihhhh=c( )( )( )( )12(,,...,)iiinicccc=dˆy第三章 海关虚假贸易罚款金额预测模型研究 
 21 在海关虚假贸易案件罚款金额预测任务中，假设有一个包含 M 个简要案件描述文本的集合，同时，有与案件描述文本对应的真实罚款金额集合为，其中表示第 i 个案件描述文本，表示第 i 个案件描述文本对应的真实罚款金额。而对于第 i 个案件描述文本而言，其由 n 个句子序列组成，即，其中表示第 i 个案件描述文本的第 j 个句子序列。因此，训练数据集具体定义为，其中，每个样本表示案件的真实罚款金额是。 
3.1.2 问题描述 
基于 3.1.1 小节的概念与描述，本文研究的海关虚假贸易案件罚款金额问题可以定义为： 
输入：一个案件描述文本。 
输出：对于给定的一个海关虚假贸易案件，首先通过句向量编码模块的文本预处理部分进行分句、文本标准化处理等操作后，再传入其句向量编码部分生成对应的案件文本语义向量表达，其中为第 i 个案件描述文本的第 j 个句子对应的案件文本语义句向量表达；然后基于该文本语义向量表达，在罚款金额预测模块先通过 Bi-LSTM Encoder 部分，生成能够捕捉双向语义的文本向量表达；为了更好地体现各个句子对罚款金额的不同影响，引入注意力机制生成特征语义向量表达；最后通过 CNN Decoder 部分，输出对该案件文本最终的罚款金额预测值。 
3.2 整体模型概述 
图 3-1 展示了本文提到的海关虚假贸易案件罚款金额预测模型的整体网络结构。 
海关虚假贸易案件罚款金额预测模型进行罚金预测的过程包含以下几个模块：（1）句向量编码模块：将输入的案件描述文本经过文本预处理生成符合统一规范的句子序列，再将处理好的句子序列传入到基于 Transformer-XL 改进的 Quick-thoughts 句向量模型(Improved Quick-thoughts model based on Transformer-XL，TQT)中，生成案件文本的语义向量表达；（2）罚款金额预测模块：采用 Encoder-Decoder 的框架进行设计。 
( )( )( )()12{,,...,...,,}iMXxxxx=( )( )( )()12{,,...,...,,}iMYyyyy=x( )i( )iy( )( )( )( )( )12(,,...,,...,)iiiiijnxssss=( )ijs(1)(1)(2)(2)()(){(,),(,),...,(,)}MMDxyxyxy=( )( )(,)iixyÎD( )ix( )iy( )( )( )( )12(,,...,)iiinixsss=( )ix( )( )( )( )( )12(,,,,,)iiiiijnwwwww=…!j( )iw(iw ）( )( )( )( )( )12(,,,,,)iiiiijnhhhhh=…!( )( )( )( )( )12(,,,,,)iiiiijnccccc=…!ˆ i( )y( )ixw( )i华南理工大学硕士学位论文  22  图 3-1 基于海关虚假贸易案件的罚款金额预测模型框架图 为学习文本深层次语义信息，根据（1）得到的案件文本向量表达，在 Encoder-Decoder框架的 Encoder 端引入了可关注于句子语义级别文本建模的 Bi-LSTM 模型；为重点关注重要的文本句子对最终罚款金额的影响，在 Encoder 端和 Decoder 端中间引入 Attention机制；最后，使用一个 CNN 预测模块作为 Decoder 端，根据注意力机制输出的特征向量序列进行案件罚款金额预测，输出模型最终的罚款金额。为了更好的理解模型的罚款金额预测过程，下面将详细地介绍每个模块的具体结构以及基本原理。 
3.3 句向量编码模块 
该模块的输入为海关虚假贸易的简要案件描述文本，输出为该案件文本的语义向量表。如图 3-2 所示，该模块的主要分成两个部分，第一部分是文本预处理，对案件描述文本进行文本预处理，使文本在送入句向量模型前得到统一规范；第二部分是对案件描述文本的句向量生成，在该部分设计了一种基于 Transformer-XL 改进的 Quick-thoughts 模型来生成对案件描述文本句子的语义向量表达。下面将对这两部分的内容分别进行详细的介绍。 
 ( )iwc( )iˆ i( )y( )ix( )( )( )( )12(,,...,)iiiniwwww=图3-2 案件文本句向量编码模块示意图 第三章 海关虚假贸易罚款金额预测模型研究 
 23 3.3.1 文本预处理 
文本预处理是为了剔除语料库中的一些杂质，且在文本送入改进的 Quick-thoughts句向量模型前，对文本进行统一规范。预处理的流程主要包含以下几个方面： 
（1）分句。将输入的案件描述文本，通过分句工具分解成一个个句子序列，其中，n 表示案件描述文本中句子的数量。常用的中文分句工具有：Jieba、SnowNLP、THULAC 和 NLPIR 等等；本文模型使用 Jieba 分句工具对案件描述文本进行分句的操作； （2）文本标准化处理。使用正则表达式来对文本中的一些杂质，如脱敏处理后的乱码随机数、停用词等，进行剔除操作；此外，由于数据中包含了大量的时间、车牌号、手机号等信息，这些信息将对结果产生干扰作用，因此将其用统一代词进行表示，如表3-2 所示；最后，案件文本中不仅包含阿拉伯数字，还存在一些中文数字，因此将文本中 的 数 字 符 号 统 一 为 阿 拉 伯 数 字 。 经 过 文 本 标 准 化 处 理 的 案 件 文 本 表 示 为。 
表 3-2 文本标准化统一代词表 数字类型 示例 代词名称 时间数字 2008 年 8 月 31 号 TIME 车牌号 粤 S18525 CAR 货物商品号 商品编码 85466847 GOODS 产品型号 电脑型号 1001PX-WHI029X MODEL 海关关口号 关口 B2 PASS 为方便理解，图 3-3 给出了一个文本预处理操作实例。在该实例中，案件文本为“当事人于 2004 年 8 月 31 日持******号报关单向海关申报出口******号一般贸易合同项下，由粤 S18525 号车载运的工艺蜡烛 2670 公斤。经查，发现实际出口工艺蜡烛 2670公斤。另有化纤针织绒布包 290 公斤出口未申报，价值人民币 8551.73 元。”，按照文本预处理的操作流程，先对进行分句处理，得到“当事人于 2004 年 8 月 31 日持******号报关单向海关申报出口******号一般贸易合同项下，由粤 S18525 号车载运的工艺蜡烛 2670 公斤。”、“经查，发现实际出口工艺蜡烛 2670 公斤。”、“另有化纤针织绒布包 290 公斤出口未申报，价值人民币 8551.73 元。”共 3 条分句。为方便起见，本文( )ix( )( )( )12(,,,)iinisss!( )( )( )( )12ˆˆˆˆ(,,,)iiiinxsss=!( )ixx( )i华南理工大学硕士学位论文  24 取第一条分句作为文本标准化处理的演示。该句中所包含有的时间“2004 年 8 月 31 日”、脱敏处理后的乱码“*******”以及车牌信息“粤 S18525”等一些杂项，因此，文本标准化处理会去除掉句子序列中的上述乱码信息，并按照表 3-2 将时间信息替换成“TIME”、车牌信息替换成“CAR”这类代词表示。再将文本标准化处理过后的句子序列传入到句向量编码部分。 
 图 3-3 文本预处理操作示例图 3.3.2 基于 Transformer-XL 改进的 Quick-thoughts 模型 本文第二章对句向量技术进行了详细介绍，其中 Quick-thoughts 句向量模型可以以无监督学习的方式生成句子文本向量表示。在 Quick-thoughts 句向量模型中，编码器使用 GRU 作为参数化函数，以一个文本句子作为模型的输入，将其训练编码成为一个固定长度的文本向量。但是该模型在处理较长文本时，因 GRU 存在固有的长距离信息传播损失问题，会存在较大误差，降低了所得语义向量表示的准确性。因此，为得到更为准确的句子语义向量表达，本文在模型句向量编码部分使用改进的 Quick-thoughts 句向量模型，即使用完全基于注意力网络的 Transformer-XL 来替代 Quick-thoughts 句向量模型的 GRU 模块，生成更富含语义信息的句向量表达。具体而言，图 3-4 展示了改进的Quick-thougths 句向量模型编码的流程图，输入第 i 个案件描述文本的句子片段，首先通过 Word2Vec 向量初始化，将案件描述文本句子转换成其各自的向量表达形式，然后通过多头注意力层，生成代表各个句子注意力的向量表达形式，最后通过前馈神经网络层生成输出其各自的语义向量表达。 
( )( )( )12ˆˆˆ(,,,)iinisss!( )( )( )( )12ˆˆˆˆ(,,...,)iiinissss=( )( )( )( )12(,,...,)iiinioooo=( )( )( )( )12(,,....,)iiinizzzz=( )( )( )( )12(,,...,)iiiniwwww=